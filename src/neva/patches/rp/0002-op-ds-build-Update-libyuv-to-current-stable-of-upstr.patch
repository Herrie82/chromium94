From e9644e73369ba0073f91e963a1ff6942a85290cb Mon Sep 17 00:00:00 2001
From: Youngsoo Choi <youngsoo.choi@lge.com>
Date: Mon, 1 Nov 2021 11:44:12 +0900
Subject: [PATCH 2/2] [op][ds][build] Update libyuv to current stable of
 upstream to support NEON

:Release Notes:
Update libyuv to current stable of upstream to support NEON

:Detailed Notes:
Current version of libyuv has build error with NEON support when
compilng to 64 bit machine.
However, it seems the issue is fixed on the lastest upstream.

This CL updates libyuv to current stable of upstream.

    commit eb6e7bb63738e29efd82ea3cf2a115238a89fa51 (origin/stable)
    Author: Mirko Bonadei <mbonadei@chromium.org>
    Date:   Wed Apr 28 17:32:14 2021 +0200

        Fix autoroller main branch.

And adds 2 fixes above the stable version which fixes the build error.

    commit 40fb4b47c8fd11590053d551612dba2e873907ae
    Author: Stephan Hartmann <stha09@googlemail.com>
    Date:   Thu May 27 15:25:31 2021 +0000

        GCC: replace mov .8h with mov .16b

    commit 30d5fcc4d08504c8e80869ba47279861d65b0c99
    Author: Stephan Hartmann <stha09@googlemail.com>
    Date:   Thu May 27 15:18:36 2021 +0000

        GCC: force memory address without offset on aarch64

:Testing Performed:
Build succeeds

:QA Notes:
Test-cases: N/A

:Issues Addressed:
[WRN-8291] Enable NEON for libyuv on OSE

Change-Id: I9dc8b90580510f74878e8d2b0af84316d196c0fd
Reviewed-on: http://gpro.lge.com/c/webosose/chromium91/+/318436
Reviewed-by: Commit Msg Checker <commit_msg@lge.com>
Reviewed-by: Youngsoo Choi <youngsoo.choi@lge.com>
Reviewed-by: Wanchang Ryu <wanchang.ryu@lge.com>
Reviewed-by: Jose Dapena Paz <j.dapena@lgepartner.com>
Reviewed-by: Donghyun Kim <donghyun11.kim@lge.com>
Tested-by: Youngsoo Choi <youngsoo.choi@lge.com>
---
 DEPS                                          |   11 +-
 PRESUBMIT.py                                  |   23 -
 README.chromium                               |    2 +-
 codereview.settings                           |    3 +-
 docs/formats.md                               |   12 +-
 download_vs_toolchain.py                      |    2 +-
 include/libyuv/compare_row.h                  |   13 +-
 include/libyuv/convert.h                      |   13 +
 include/libyuv/convert_argb.h                 |   78 +-
 include/libyuv/convert_from_argb.h            |   24 +
 include/libyuv/planar_functions.h             |   69 +-
 include/libyuv/rotate_row.h                   |    7 +-
 include/libyuv/row.h                          |  586 +++++--
 include/libyuv/scale_row.h                    |    8 +-
 include/libyuv/version.h                      |    2 +-
 include/libyuv/video_common.h                 |    6 +-
 source/compare_common.cc                      |   30 -
 source/compare_gcc.cc                         |    2 +-
 source/compare_win.cc                         |   14 +-
 source/convert.cc                             |  272 ++-
 source/convert_argb.cc                        |  178 +-
 source/convert_from_argb.cc                   |  120 +-
 source/planar_functions.cc                    |  900 +++++++---
 source/rotate_gcc.cc                          |    2 +-
 source/rotate_win.cc                          |    5 +-
 source/row_any.cc                             |  254 ++-
 source/row_common.cc                          | 1062 ++++++------
 source/row_gcc.cc                             | 1060 +++++++++---
 source/row_mmi.cc                             |  436 ++---
 source/row_msa.cc                             |  186 +-
 source/row_neon.cc                            | 1081 +++++++-----
 source/row_neon64.cc                          | 1529 ++++++++++-------
 source/row_win.cc                             |  323 ++--
 source/scale.cc                               |    3 +-
 source/scale_gcc.cc                           |    2 +-
 source/scale_neon64.cc                        |   26 +-
 source/scale_uv.cc                            |    6 +-
 source/scale_win.cc                           |    5 +-
 tools_libyuv/autoroller/roll_deps.py          |   12 +-
 .../autoroller/unittests/roll_deps_test.py    |    5 +-
 tools_libyuv/valgrind/chrome_tests.bat        |   53 -
 tools_libyuv/valgrind/chrome_tests.py         |  869 ----------
 tools_libyuv/valgrind/chrome_tests.sh         |   94 -
 tools_libyuv/valgrind/common.py               |  256 ---
 tools_libyuv/valgrind/gdb_helper.py           |   91 -
 tools_libyuv/valgrind/libyuv_tests.bat        |   79 -
 tools_libyuv/valgrind/libyuv_tests.py         |  139 --
 tools_libyuv/valgrind/libyuv_tests.sh         |  101 --
 tools_libyuv/valgrind/locate_valgrind.sh      |   73 -
 tools_libyuv/valgrind/memcheck/OWNERS         |    1 -
 tools_libyuv/valgrind/memcheck/PRESUBMIT.py   |   99 --
 .../valgrind/memcheck/suppressions.txt        |   21 -
 .../valgrind/memcheck/suppressions_mac.txt    |    5 -
 .../valgrind/memcheck/suppressions_win32.txt  |    5 -
 tools_libyuv/valgrind/memcheck_analyze.py     |  644 -------
 tools_libyuv/valgrind/valgrind.sh             |  110 --
 tools_libyuv/valgrind/valgrind_test.py        |  517 ------
 unit_test/color_test.cc                       |   30 +-
 unit_test/compare_test.cc                     |    2 +-
 unit_test/convert_test.cc                     |  405 +++--
 unit_test/planar_test.cc                      |  399 ++++-
 unit_test/rotate_argb_test.cc                 |   16 +-
 unit_test/rotate_test.cc                      |   48 +-
 unit_test/scale_argb_test.cc                  |   14 +-
 unit_test/unit_test.h                         |    2 +-
 unit_test/video_common_test.cc                |    4 +-
 66 files changed, 5994 insertions(+), 6455 deletions(-)
 delete mode 100755 tools_libyuv/valgrind/chrome_tests.bat
 delete mode 100755 tools_libyuv/valgrind/chrome_tests.py
 delete mode 100755 tools_libyuv/valgrind/chrome_tests.sh
 delete mode 100644 tools_libyuv/valgrind/common.py
 delete mode 100644 tools_libyuv/valgrind/gdb_helper.py
 delete mode 100644 tools_libyuv/valgrind/libyuv_tests.bat
 delete mode 100755 tools_libyuv/valgrind/libyuv_tests.py
 delete mode 100755 tools_libyuv/valgrind/libyuv_tests.sh
 delete mode 100755 tools_libyuv/valgrind/locate_valgrind.sh
 delete mode 100644 tools_libyuv/valgrind/memcheck/OWNERS
 delete mode 100644 tools_libyuv/valgrind/memcheck/PRESUBMIT.py
 delete mode 100644 tools_libyuv/valgrind/memcheck/suppressions.txt
 delete mode 100644 tools_libyuv/valgrind/memcheck/suppressions_mac.txt
 delete mode 100644 tools_libyuv/valgrind/memcheck/suppressions_win32.txt
 delete mode 100755 tools_libyuv/valgrind/memcheck_analyze.py
 delete mode 100755 tools_libyuv/valgrind/valgrind.sh
 delete mode 100755 tools_libyuv/valgrind/valgrind_test.py

diff --git a/DEPS b/DEPS
index 603a9176..b3226764 100644
--- a/DEPS
+++ b/DEPS
@@ -48,7 +48,7 @@ deps = {
   },
 
   'src/buildtools/clang_format/script':
-    Var('chromium_git') + '/chromium/llvm-project/cfe/tools/clang-format.git' + '@' + '96636aa0e9f047f17447f2d45a094d0b59ed7917',
+    Var('chromium_git') + '/external/github.com/llvm/llvm-project/clang/tools/clang-format.git' + '@' + '99803d74e35962f63a775f29477882afd4d57d94',
   'src/buildtools/third_party/libc++/trunk':
     Var('chromium_git') + '/external/github.com/llvm/llvm-project/libcxx.git' + '@' + 'd9040c75cfea5928c804ab7c235fed06a63f743a',
   'src/buildtools/third_party/libc++abi/trunk':
@@ -72,8 +72,6 @@ deps = {
     Var('chromium_git') + '/chromium/deps/libjpeg_turbo.git' + '@' + 'd5148db386ceb4a608058320071cbed890bd6ad2',
   'src/third_party/nasm':
     Var('chromium_git') + '/chromium/deps/nasm.git' + '@' + '19f3fad68da99277b2882939d3b2fa4c4b8d51d9',
-  'src/third_party/yasm/source/patched-yasm':
-    Var('chromium_git') + '/chromium/deps/yasm/patched-yasm.git' + '@' + '720b70524a4424b15fc57e82263568c8ba0496ad',
   'src/tools':
     Var('chromium_git') + '/chromium/src/tools' + '@' + '1bb7c085e67a0fc8c63511af83299d1632f5a3f3',
   'src/tools/swarming_client':
@@ -410,13 +408,6 @@ deps = {
     'condition': 'checkout_ios'
   },
 
-  # Win deps:
-  # Dependencies used by libjpeg-turbo
-  'src/third_party/yasm/binaries': {
-    'url': Var('chromium_git') + '/chromium/deps/yasm/binaries.git' + '@' + '52f9b3f4b0aa06da24ef8b123058bb61ee468881',
-    'condition': 'checkout_win',
-  },
-
   # === ANDROID_DEPS Generated Code Start ===
   # Generated by //third_party/android_deps/fetch_all.py
   'src/third_party/android_deps/libs/android_arch_core_common': {
diff --git a/PRESUBMIT.py b/PRESUBMIT.py
index cc91dceb..b867239a 100644
--- a/PRESUBMIT.py
+++ b/PRESUBMIT.py
@@ -6,28 +6,6 @@
 # in the file PATENTS. All contributing project authors may
 # be found in the AUTHORS file in the root of the source tree.
 
-import os
-
-
-def _RunPythonTests(input_api, output_api):
-  def join(*args):
-    return input_api.os_path.join(input_api.PresubmitLocalPath(), *args)
-
-  test_directories = [
-      root for root, _, files in os.walk(join('tools_libyuv'))
-      if any(f.endswith('_test.py') for f in files)
-  ]
-
-  tests = []
-  for directory in test_directories:
-    tests.extend(
-      input_api.canned_checks.GetUnitTestsInDirectory(
-          input_api,
-          output_api,
-          directory,
-          allowlist=[r'.+_test\.py$']))
-  return input_api.RunTests(tests, parallel=True)
-
 
 def _CommonChecks(input_api, output_api):
   """Checks common to both upload and commit."""
@@ -49,7 +27,6 @@ def _CommonChecks(input_api, output_api):
                          'W0232',  # Class has no __init__ method
                         ],
       pylintrc='pylintrc'))
-  results.extend(_RunPythonTests(input_api, output_api))
   return results
 
 
diff --git a/README.chromium b/README.chromium
index 460d8ecc..7b25d686 100644
--- a/README.chromium
+++ b/README.chromium
@@ -1,6 +1,6 @@
 Name: libyuv
 URL: http://code.google.com/p/libyuv/
-Version: 1780
+Version: 1796
 License: BSD
 License File: LICENSE
 
diff --git a/codereview.settings b/codereview.settings
index 00ba1d37..b226fae5 100644
--- a/codereview.settings
+++ b/codereview.settings
@@ -1,6 +1,5 @@
-# This file is used by git cl to get repository specific information.
+# This file is used by `git cl` to get repository specific information.
 CODE_REVIEW_SERVER: codereview.chromium.org
 GERRIT_HOST: True
 PROJECT: libyuv
-TRY_ON_UPLOAD: False
 VIEW_VC: https://chromium.googlesource.com/libyuv/libyuv/+/
diff --git a/docs/formats.md b/docs/formats.md
index 5fc19d45..d628f7f9 100644
--- a/docs/formats.md
+++ b/docs/formats.md
@@ -54,12 +54,14 @@ The following is extracted from video_common.h as a complete list of formats sup
       // 1 Secondary YUV format: row biplanar.
       FOURCC_M420 = FOURCC('M', '4', '2', '0'),  // deprecated.
 
-      // 11 Primary RGB formats: 4 32 bpp, 2 24 bpp, 3 16 bpp, 1 10 bpc
+      // 13 Primary RGB formats: 4 32 bpp, 2 24 bpp, 3 16 bpp, 1 10 bpc, 2 64 bpp
       FOURCC_ARGB = FOURCC('A', 'R', 'G', 'B'),
       FOURCC_BGRA = FOURCC('B', 'G', 'R', 'A'),
       FOURCC_ABGR = FOURCC('A', 'B', 'G', 'R'),
       FOURCC_AR30 = FOURCC('A', 'R', '3', '0'),  // 10 bit per channel. 2101010.
       FOURCC_AB30 = FOURCC('A', 'B', '3', '0'),  // ABGR version of 10 bit
+      FOURCC_AR64 = FOURCC('A', 'R', '6', '4'),  // 16 bit per channel.
+      FOURCC_AB64 = FOURCC('A', 'B', '6', '4'),  // ABGR version of 16 bit
       FOURCC_24BG = FOURCC('2', '4', 'B', 'G'),
       FOURCC_RAW = FOURCC('r', 'a', 'w', ' '),
       FOURCC_RGBA = FOURCC('R', 'G', 'B', 'A'),
@@ -180,6 +182,14 @@ The 2 bit alpha has 4 values.  Here are the comparable 8 bit alpha values.
 The 10 bit RGB values range from 0 to 1023.
 XR30 is the same as AR30 but with no alpha channel.
 
+# AB64 and AR64
+
+AB64 is similar to ABGR, with 16 bit (2 bytes) per channel. Each channel stores an unsigned short.
+In memory R is the lowest and A is the highest.
+Each channel has value ranges from 0 to 65535.
+AR64 is similar to ARGB.
+
+
 # NV12 and NV21
 
 NV12 is a biplanar format with a full sized Y plane followed by a single
diff --git a/download_vs_toolchain.py b/download_vs_toolchain.py
index 4b345789..49d06930 100644
--- a/download_vs_toolchain.py
+++ b/download_vs_toolchain.py
@@ -22,7 +22,7 @@ sys.path.insert(0, os.path.join(checkout_root, 'build'))
 sys.path.insert(0, os.path.join(checkout_root, 'tools', 'find_depot_tools'))
 
 
-import vs_toolchain
+import vs_toolchain  # pylint: disable=wrong-import-position
 
 
 if __name__ == '__main__':
diff --git a/include/libyuv/compare_row.h b/include/libyuv/compare_row.h
index e95b9d93..18c5fa4e 100644
--- a/include/libyuv/compare_row.h
+++ b/include/libyuv/compare_row.h
@@ -18,7 +18,7 @@ namespace libyuv {
 extern "C" {
 #endif
 
-#if defined(__pnacl__) || defined(__CLR_VER) ||            \
+#if defined(__pnacl__) || defined(__CLR_VER) || \
     (defined(__native_client__) && defined(__x86_64__)) || \
     (defined(__i386__) && !defined(__SSE__) && !defined(__clang__))
 #define LIBYUV_DISABLE_X86
@@ -54,21 +54,22 @@ extern "C" {
 #endif
 
 // The following are available for Visual C and clangcl 32 bit:
-#if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && defined(_MSC_VER) && \
+#if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && \
+     defined(_MSC_VER) && !defined(__clang__) && \
     (defined(VISUALC_HAS_AVX2) || defined(CLANG_HAS_AVX2))
 #define HAS_HASHDJB2_AVX2
 #define HAS_SUMSQUAREERROR_AVX2
 #endif
 
-// The following are available for GCC and clangcl 64 bit:
+// The following are available for GCC and clangcl:
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 #define HAS_HAMMINGDISTANCE_SSSE3
 #endif
 
-// The following are available for GCC and clangcl 64 bit:
+// The following are available for GCC and clangcl:
 #if !defined(LIBYUV_DISABLE_X86) && defined(CLANG_HAS_AVX2) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 #define HAS_HAMMINGDISTANCE_AVX2
 #endif
 
diff --git a/include/libyuv/convert.h b/include/libyuv/convert.h
index 40869ef2..93e7550b 100644
--- a/include/libyuv/convert.h
+++ b/include/libyuv/convert.h
@@ -693,6 +693,19 @@ int RAWToI420(const uint8_t* src_raw,
               int width,
               int height);
 
+// RGB big endian (rgb in memory) to J420.
+LIBYUV_API
+int RAWToJ420(const uint8_t* src_raw,
+              int src_stride_raw,
+              uint8_t* dst_y,
+              int dst_stride_y,
+              uint8_t* dst_u,
+              int dst_stride_u,
+              uint8_t* dst_v,
+              int dst_stride_v,
+              int width,
+              int height);
+
 // RGB16 (RGBP fourcc) little endian to I420.
 LIBYUV_API
 int RGB565ToI420(const uint8_t* src_rgb565,
diff --git a/include/libyuv/convert_argb.h b/include/libyuv/convert_argb.h
index 474a8214..eb4ebd54 100644
--- a/include/libyuv/convert_argb.h
+++ b/include/libyuv/convert_argb.h
@@ -806,29 +806,29 @@ int I010ToAR30(const uint16_t* src_y,
                int width,
                int height);
 
-// Convert I010 to AB30.
+// Convert H010 to AR30.
 LIBYUV_API
-int I010ToAB30(const uint16_t* src_y,
+int H010ToAR30(const uint16_t* src_y,
                int src_stride_y,
                const uint16_t* src_u,
                int src_stride_u,
                const uint16_t* src_v,
                int src_stride_v,
-               uint8_t* dst_ab30,
-               int dst_stride_ab30,
+               uint8_t* dst_ar30,
+               int dst_stride_ar30,
                int width,
                int height);
 
-// Convert H010 to AR30.
+// Convert I010 to AB30.
 LIBYUV_API
-int H010ToAR30(const uint16_t* src_y,
+int I010ToAB30(const uint16_t* src_y,
                int src_stride_y,
                const uint16_t* src_u,
                int src_stride_u,
                const uint16_t* src_v,
                int src_stride_v,
-               uint8_t* dst_ar30,
-               int dst_stride_ar30,
+               uint8_t* dst_ab30,
+               int dst_stride_ab30,
                int width,
                int height);
 
@@ -1065,6 +1065,42 @@ int AR30ToAB30(const uint8_t* src_ar30,
                int width,
                int height);
 
+// Convert AR64 to ARGB.
+LIBYUV_API
+int AR64ToARGB(const uint16_t* src_ar64,
+               int src_stride_ar64,
+               uint8_t* dst_argb,
+               int dst_stride_argb,
+               int width,
+               int height);
+
+// Convert AB64 to ABGR.
+#define AB64ToABGR AR64ToARGB
+
+// Convert AB64 to ARGB.
+LIBYUV_API
+int AB64ToARGB(const uint16_t* src_ab64,
+               int src_stride_ab64,
+               uint8_t* dst_argb,
+               int dst_stride_argb,
+               int width,
+               int height);
+
+// Convert AR64 to ABGR.
+#define AR64ToABGR AB64ToARGB
+
+// Convert AR64 To AB64.
+LIBYUV_API
+int AR64ToAB64(const uint16_t* src_ar64,
+               int src_stride_ar64,
+               uint16_t* dst_ab64,
+               int dst_stride_ab64,
+               int width,
+               int height);
+
+// Convert AB64 To AR64.
+#define AB64ToAR64 AR64ToAB64
+
 // src_width/height provided by capture
 // dst_width/height for clipping determine final size.
 LIBYUV_API
@@ -1377,6 +1413,19 @@ int I420ToAR30(const uint8_t* src_y,
                int width,
                int height);
 
+// Convert I420 to AB30.
+LIBYUV_API
+int I420ToAB30(const uint8_t* src_y,
+               int src_stride_y,
+               const uint8_t* src_u,
+               int src_stride_u,
+               const uint8_t* src_v,
+               int src_stride_v,
+               uint8_t* dst_ab30,
+               int dst_stride_ab30,
+               int width,
+               int height);
+
 // Convert H420 to AR30.
 LIBYUV_API
 int H420ToAR30(const uint8_t* src_y,
@@ -1390,6 +1439,19 @@ int H420ToAR30(const uint8_t* src_y,
                int width,
                int height);
 
+// Convert H420 to AB30.
+LIBYUV_API
+int H420ToAB30(const uint8_t* src_y,
+               int src_stride_y,
+               const uint8_t* src_u,
+               int src_stride_u,
+               const uint8_t* src_v,
+               int src_stride_v,
+               uint8_t* dst_ab30,
+               int dst_stride_ab30,
+               int width,
+               int height);
+
 // Convert I420 to ARGB with matrix.
 LIBYUV_API
 int I420ToARGBMatrix(const uint8_t* src_y,
diff --git a/include/libyuv/convert_from_argb.h b/include/libyuv/convert_from_argb.h
index d992363c..bf487860 100644
--- a/include/libyuv/convert_from_argb.h
+++ b/include/libyuv/convert_from_argb.h
@@ -153,6 +153,30 @@ int ARGBToI444(const uint8_t* src_argb,
                int width,
                int height);
 
+// Convert ARGB to AR64.
+LIBYUV_API
+int ARGBToAR64(const uint8_t* src_argb,
+               int src_stride_argb,
+               uint16_t* dst_ar64,
+               int dst_stride_ar64,
+               int width,
+               int height);
+
+// Convert ABGR to AB64.
+#define ABGRToAB64 ARGBToAR64
+
+// Convert ARGB to AB64.
+LIBYUV_API
+int ARGBToAB64(const uint8_t* src_argb,
+               int src_stride_argb,
+               uint16_t* dst_ab64,
+               int dst_stride_ab64,
+               int width,
+               int height);
+
+// Convert ABGR to AR64.
+#define ABGRToAR64 ARGBToAB64
+
 // Convert ARGB To I422.
 LIBYUV_API
 int ARGBToI422(const uint8_t* src_argb,
diff --git a/include/libyuv/planar_functions.h b/include/libyuv/planar_functions.h
index ebefb568..fdecdee3 100644
--- a/include/libyuv/planar_functions.h
+++ b/include/libyuv/planar_functions.h
@@ -23,7 +23,7 @@ extern "C" {
 #endif
 
 // TODO(fbarchard): Move cpu macros to row.h
-#if defined(__pnacl__) || defined(__CLR_VER) ||            \
+#if defined(__pnacl__) || defined(__CLR_VER) || \
     (defined(__native_client__) && defined(__x86_64__)) || \
     (defined(__i386__) && !defined(__SSE__) && !defined(__clang__))
 #define LIBYUV_DISABLE_X86
@@ -229,6 +229,60 @@ void MergeARGBPlane(const uint8_t* src_r,
                     int width,
                     int height);
 
+// Merge separate 'depth' bit R, G and B planes stored in lsb
+// into one interleaved XR30 plane.
+// depth should in range [10, 16]
+LIBYUV_API
+void MergeXR30Plane(const uint16_t* src_r,
+                    int src_stride_r,
+                    const uint16_t* src_g,
+                    int src_stride_g,
+                    const uint16_t* src_b,
+                    int src_stride_b,
+                    uint8_t* dst_ar30,
+                    int dst_stride_ar30,
+                    int width,
+                    int height,
+                    int depth);
+
+// Merge separate 'depth' bit R, G, B and A planes stored in lsb
+// into one interleaved AR64 plane.
+// src_a can be NULL to fill opaque value to alpha.
+// depth should in range [1, 16]
+LIBYUV_API
+void MergeAR64Plane(const uint16_t* src_r,
+                    int src_stride_r,
+                    const uint16_t* src_g,
+                    int src_stride_g,
+                    const uint16_t* src_b,
+                    int src_stride_b,
+                    const uint16_t* src_a,
+                    int src_stride_a,
+                    uint16_t* dst_ar64,
+                    int dst_stride_ar64,
+                    int width,
+                    int height,
+                    int depth);
+
+// Merge separate 'depth' bit R, G, B and A planes stored in lsb
+// into one interleaved ARGB plane.
+// src_a can be NULL to fill opaque value to alpha.
+// depth should in range [8, 16]
+LIBYUV_API
+void MergeARGB16To8Plane(const uint16_t* src_r,
+                         int src_stride_r,
+                         const uint16_t* src_g,
+                         int src_stride_g,
+                         const uint16_t* src_b,
+                         int src_stride_b,
+                         const uint16_t* src_a,
+                         int src_stride_a,
+                         uint8_t* dst_argb,
+                         int dst_stride_argb,
+                         int width,
+                         int height,
+                         int depth);
+
 // Copy I400.  Supports inverting.
 LIBYUV_API
 int I400ToI400(const uint8_t* src_y,
@@ -945,7 +999,7 @@ void ARGBAffineRow_SSE2(const uint8_t* src_argb,
                         int width);
 
 // Shuffle ARGB channel order.  e.g. BGRA to ARGB.
-// shuffler is 16 bytes and must be aligned.
+// shuffler is 16 bytes.
 LIBYUV_API
 int ARGBShuffle(const uint8_t* src_bgra,
                 int src_stride_bgra,
@@ -955,6 +1009,17 @@ int ARGBShuffle(const uint8_t* src_bgra,
                 int width,
                 int height);
 
+// Shuffle AR64 channel order.  e.g. AR64 to AB64.
+// shuffler is 16 bytes.
+LIBYUV_API
+int AR64Shuffle(const uint16_t* src_ar64,
+                int src_stride_ar64,
+                uint16_t* dst_ar64,
+                int dst_stride_ar64,
+                const uint8_t* shuffler,
+                int width,
+                int height);
+
 // Sobel ARGB effect with planar output.
 LIBYUV_API
 int ARGBSobelToPlane(const uint8_t* src_argb,
diff --git a/include/libyuv/rotate_row.h b/include/libyuv/rotate_row.h
index 022293ee..5a9cf933 100644
--- a/include/libyuv/rotate_row.h
+++ b/include/libyuv/rotate_row.h
@@ -18,7 +18,7 @@ namespace libyuv {
 extern "C" {
 #endif
 
-#if defined(__pnacl__) || defined(__CLR_VER) ||            \
+#if defined(__pnacl__) || defined(__CLR_VER) || \
     (defined(__native_client__) && defined(__x86_64__)) || \
     (defined(__i386__) && !defined(__SSE__) && !defined(__clang__))
 #define LIBYUV_DISABLE_X86
@@ -32,8 +32,9 @@ extern "C" {
 #define LIBYUV_DISABLE_X86
 #endif
 #endif
-// The following are available for Visual C and clangcl 32 bit:
-#if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && defined(_MSC_VER)
+// The following are available for Visual C 32 bit:
+#if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && defined(_MSC_VER) && \
+    !defined(__clang__)
 #define HAS_TRANSPOSEWX8_SSSE3
 #define HAS_TRANSPOSEUVWX8_SSE2
 #endif
diff --git a/include/libyuv/row.h b/include/libyuv/row.h
index 7af36c74..6c3f81e6 100644
--- a/include/libyuv/row.h
+++ b/include/libyuv/row.h
@@ -20,7 +20,7 @@ namespace libyuv {
 extern "C" {
 #endif
 
-#if defined(__pnacl__) || defined(__CLR_VER) ||            \
+#if defined(__pnacl__) || defined(__CLR_VER) || \
     (defined(__native_client__) && defined(__x86_64__)) || \
     (defined(__i386__) && !defined(__SSE__) && !defined(__clang__))
 #define LIBYUV_DISABLE_X86
@@ -245,10 +245,10 @@ extern "C" {
 #endif
 #endif
 
-// The following are available for AVX2 Visual C and clangcl 32 bit:
+// The following are available for AVX2 Visual C 32 bit:
 // TODO(fbarchard): Port to gcc.
 #if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && defined(_MSC_VER) && \
-    (defined(VISUALC_HAS_AVX2) || defined(CLANG_HAS_AVX2))
+    !defined(__clang__) && defined(VISUALC_HAS_AVX2)
 #define HAS_ARGB1555TOARGBROW_AVX2
 #define HAS_ARGB4444TOARGBROW_AVX2
 #define HAS_ARGBTOARGB1555ROW_AVX2
@@ -270,9 +270,13 @@ extern "C" {
 // The following are available for gcc/clang x86 platforms:
 // TODO(fbarchard): Port to Visual C
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 #define HAS_ABGRTOAR30ROW_SSSE3
 #define HAS_ARGBTOAR30ROW_SSSE3
+#define HAS_ARGBTOAR64ROW_SSSE3
+#define HAS_ARGBTOAB64ROW_SSSE3
+#define HAS_AR64TOARGBROW_SSSE3
+#define HAS_AB64TOARGBROW_SSSE3
 #define HAS_CONVERT16TO8ROW_SSSE3
 #define HAS_CONVERT8TO16ROW_SSE2
 #define HAS_HALFMERGEUVROW_SSSE3
@@ -285,6 +289,7 @@ extern "C" {
 #define HAS_I410TOAR30ROW_SSSE3
 #define HAS_I410TOARGBROW_SSSE3
 #define HAS_MERGEARGBROW_SSE2
+#define HAS_MERGEXRGBROW_SSE2
 #define HAS_MERGERGBROW_SSSE3
 #define HAS_MIRRORUVROW_SSSE3
 #define HAS_P210TOAR30ROW_SSSE3
@@ -296,6 +301,8 @@ extern "C" {
 #define HAS_RGBATOYJROW_SSSE3
 #define HAS_SPLITARGBROW_SSE2
 #define HAS_SPLITARGBROW_SSSE3
+#define HAS_SPLITXRGBROW_SSE2
+#define HAS_SPLITXRGBROW_SSSE3
 #define HAS_SPLITRGBROW_SSSE3
 #define HAS_SWAPUVROW_SSSE3
 
@@ -310,7 +317,7 @@ extern "C" {
 // The following are available for AVX2 gcc/clang x86 platforms:
 // TODO(fbarchard): Port to Visual C
 #if !defined(LIBYUV_DISABLE_X86) &&                                       \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER))) && \
+    (defined(__x86_64__) || defined(__i386__)) && \
     (defined(CLANG_HAS_AVX2) || defined(GCC_HAS_AVX2))
 #define HAS_ABGRTOAR30ROW_AVX2
 #define HAS_ABGRTOUVROW_AVX2
@@ -318,11 +325,21 @@ extern "C" {
 #define HAS_ARGBTOAR30ROW_AVX2
 #define HAS_ARGBTORAWROW_AVX2
 #define HAS_ARGBTORGB24ROW_AVX2
+#define HAS_ARGBTOAR64ROW_AVX2
+#define HAS_ARGBTOAB64ROW_AVX2
+#define HAS_AR64TOARGBROW_AVX2
+#define HAS_AB64TOARGBROW_AVX2
 #define HAS_CONVERT16TO8ROW_AVX2
 #define HAS_CONVERT8TO16ROW_AVX2
 #define HAS_DIVIDEROW_16_AVX2
 #define HAS_HALFMERGEUVROW_AVX2
+#define HAS_MERGEAR64ROW_AVX2
+#define HAS_MERGEARGB16TO8ROW_AVX2
 #define HAS_MERGEARGBROW_AVX2
+#define HAS_MERGEXR30ROW_AVX2
+#define HAS_MERGEXR64ROW_AVX2
+#define HAS_MERGEXRGB16TO8ROW_AVX2
+#define HAS_MERGEXRGBROW_AVX2
 #define HAS_I210TOAR30ROW_AVX2
 #define HAS_I210TOARGBROW_AVX2
 #define HAS_I212TOAR30ROW_AVX2
@@ -342,6 +359,7 @@ extern "C" {
 #define HAS_MULTIPLYROW_16_AVX2
 #define HAS_RGBATOYJROW_AVX2
 #define HAS_SPLITARGBROW_AVX2
+#define HAS_SPLITXRGBROW_AVX2
 #define HAS_SPLITUVROW_16_AVX2
 #define HAS_SWAPUVROW_AVX2
 // TODO(fbarchard): Fix AVX2 version of YUV24
@@ -359,7 +377,7 @@ extern "C" {
 // TODO(fbarchard): Port to GCC and Visual C
 // TODO(fbarchard): re-enable HAS_ARGBTORGB24ROW_AVX512VBMI. Issue libyuv:789
 #if !defined(LIBYUV_DISABLE_X86) &&                                       \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER))) && \
+    (defined(__x86_64__) || defined(__i386__)) && \
     (defined(CLANG_HAS_AVX512))
 #define HAS_ARGBTORGB24ROW_AVX512VBMI
 #endif
@@ -383,6 +401,10 @@ extern "C" {
 #define HAS_ARGBTORGB24ROW_NEON
 #define HAS_ARGBTORGB565DITHERROW_NEON
 #define HAS_ARGBTORGB565ROW_NEON
+#define HAS_ARGBTOAR64ROW_NEON
+#define HAS_ARGBTOAB64ROW_NEON
+#define HAS_AR64TOARGBROW_NEON
+#define HAS_AB64TOARGBROW_NEON
 #define HAS_ARGBTOUV444ROW_NEON
 #define HAS_ARGBTOUVJROW_NEON
 #define HAS_ARGBTOUVROW_NEON
@@ -411,7 +433,13 @@ extern "C" {
 #define HAS_I422TOYUY2ROW_NEON
 #define HAS_I444TOARGBROW_NEON
 #define HAS_J400TOARGBROW_NEON
+#define HAS_MERGEAR64ROW_NEON
+#define HAS_MERGEARGB16TO8ROW_NEON
 #define HAS_MERGEARGBROW_NEON
+#define HAS_MERGEXR30ROW_NEON
+#define HAS_MERGEXR64ROW_NEON
+#define HAS_MERGEXRGB16TO8ROW_NEON
+#define HAS_MERGEXRGBROW_NEON
 #define HAS_MERGEUVROW_NEON
 #define HAS_MERGEUVROW_16_NEON
 #define HAS_MIRRORROW_NEON
@@ -442,6 +470,7 @@ extern "C" {
 #define HAS_RGBATOYROW_NEON
 #define HAS_SETROW_NEON
 #define HAS_SPLITARGBROW_NEON
+#define HAS_SPLITXRGBROW_NEON
 #define HAS_SPLITRGBROW_NEON
 #define HAS_SPLITUVROW_NEON
 #define HAS_SPLITUVROW_16_NEON
@@ -520,24 +549,14 @@ extern "C" {
 #define HAS_BGRATOYROW_MSA
 #define HAS_HALFFLOATROW_MSA
 #define HAS_I400TOARGBROW_MSA
-#define HAS_I422ALPHATOARGBROW_MSA
-#define HAS_I422TOARGBROW_MSA
-#define HAS_I422TORGB24ROW_MSA
-#define HAS_I422TORGBAROW_MSA
 #define HAS_I422TOUYVYROW_MSA
 #define HAS_I422TOYUY2ROW_MSA
-#define HAS_I444TOARGBROW_MSA
-#define HAS_I422TOARGB1555ROW_MSA
-#define HAS_I422TORGB565ROW_MSA
 #define HAS_INTERPOLATEROW_MSA
 #define HAS_J400TOARGBROW_MSA
 #define HAS_MERGEUVROW_MSA
 #define HAS_MIRRORROW_MSA
 #define HAS_MIRRORUVROW_MSA
 #define HAS_MIRRORSPLITUVROW_MSA
-#define HAS_NV12TOARGBROW_MSA
-#define HAS_NV12TORGB565ROW_MSA
-#define HAS_NV21TOARGBROW_MSA
 #define HAS_RAWTOARGBROW_MSA
 #define HAS_RAWTORGB24ROW_MSA
 #define HAS_RAWTOUVROW_MSA
@@ -557,10 +576,8 @@ extern "C" {
 #define HAS_SOBELXYROW_MSA
 #define HAS_SOBELYROW_MSA
 #define HAS_SPLITUVROW_MSA
-#define HAS_UYVYTOARGBROW_MSA
 #define HAS_UYVYTOUVROW_MSA
 #define HAS_UYVYTOYROW_MSA
-#define HAS_YUY2TOARGBROW_MSA
 #define HAS_YUY2TOUV422ROW_MSA
 #define HAS_YUY2TOUVROW_MSA
 #define HAS_YUY2TOYROW_MSA
@@ -610,8 +627,6 @@ extern "C" {
 #define HAS_I400TOARGBROW_MMI
 #define HAS_I422TOUYVYROW_MMI
 #define HAS_I422TOYUY2ROW_MMI
-#define HAS_I422TOARGBROW_MMI
-#define HAS_I444TOARGBROW_MMI
 #define HAS_INTERPOLATEROW_MMI
 #define HAS_J400TOARGBROW_MMI
 #define HAS_MERGERGBROW_MMI
@@ -642,20 +657,6 @@ extern "C" {
 #define HAS_YUY2TOUV422ROW_MMI
 #define HAS_YUY2TOUVROW_MMI
 #define HAS_YUY2TOYROW_MMI
-#define HAS_I210TOARGBROW_MMI
-#define HAS_I422TOARGB4444ROW_MMI
-#define HAS_I422TOARGB1555ROW_MMI
-#define HAS_I422TORGB565ROW_MMI
-#define HAS_NV21TORGB24ROW_MMI
-#define HAS_NV12TORGB24ROW_MMI
-#define HAS_I422ALPHATOARGBROW_MMI
-#define HAS_I422TORGB24ROW_MMI
-#define HAS_NV12TOARGBROW_MMI
-#define HAS_NV21TOARGBROW_MMI
-#define HAS_NV12TORGB565ROW_MMI
-#define HAS_YUY2TOARGBROW_MMI
-#define HAS_UYVYTOARGBROW_MMI
-#define HAS_I422TORGBAROW_MMI
 #endif
 
 #if defined(_MSC_VER) && !defined(__CLR_VER) && !defined(__clang__)
@@ -664,6 +665,7 @@ extern "C" {
 #else
 #define SIMD_ALIGNED(var) __declspec(align(16)) var
 #endif
+#define LIBYUV_NOINLINE __declspec(noinline)
 typedef __declspec(align(16)) int16_t vec16[8];
 typedef __declspec(align(16)) int32_t vec32[4];
 typedef __declspec(align(16)) float vecf32[4];
@@ -684,6 +686,7 @@ typedef __declspec(align(32)) uint8_t ulvec8[32];
 #else
 #define SIMD_ALIGNED(var) var __attribute__((aligned(16)))
 #endif
+#define LIBYUV_NOINLINE __attribute__((noinline))
 typedef int16_t __attribute__((vector_size(16))) vec16;
 typedef int32_t __attribute__((vector_size(16))) vec32;
 typedef float __attribute__((vector_size(16))) vecf32;
@@ -699,6 +702,7 @@ typedef uint32_t __attribute__((vector_size(32))) ulvec32;
 typedef uint8_t __attribute__((vector_size(32))) ulvec8;
 #else
 #define SIMD_ALIGNED(var) var
+#define LIBYUV_NOINLINE
 typedef int16_t vec16[8];
 typedef int32_t vec32[4];
 typedef float vecf32[4];
@@ -714,39 +718,18 @@ typedef uint32_t ulvec32[8];
 typedef uint8_t ulvec8[32];
 #endif
 
-#if defined(__aarch64__)
-// This struct is for Arm64 color conversion.
+#if defined(__aarch64__) || defined(__arm__)
+// This struct is for ARM color conversion.
 struct YuvConstants {
-  uvec16 kUVToRB;
-  uvec16 kUVToRB2;
-  uvec16 kUVToG;
-  uvec16 kUVToG2;
-  vec16 kUVBiasBGR;
-  vec32 kYToRgb;
-};
-#elif defined(__arm__)
-// This struct is for ArmV7 color conversion.
-struct YuvConstants {
-  uvec8 kUVToRB;
-  uvec8 kUVToG;
-  vec16 kUVBiasBGR;
-  vec32 kYToRgb;
+  uvec8 kUVCoeff;
+  vec16 kRGBCoeffBias;
 };
 #else
 // This struct is for Intel color conversion.
 struct YuvConstants {
-#if LIBYUV_UNLIMITED_DATA
   uint8_t kUVToB[32];
   uint8_t kUVToG[32];
   uint8_t kUVToR[32];
-#else
-  int8_t kUVToB[32];
-  int8_t kUVToG[32];
-  int8_t kUVToR[32];
-#endif
-  int16_t kUVBiasB[16];
-  int16_t kUVBiasG[16];
-  int16_t kUVBiasR[16];
   int16_t kYToRgb[16];
   int16_t kYBiasToRgb[16];
 };
@@ -755,11 +738,8 @@ struct YuvConstants {
 #define KUVTOB 0
 #define KUVTOG 32
 #define KUVTOR 64
-#define KUVBIASB 96
-#define KUVBIASG 128
-#define KUVBIASR 160
-#define KYTORGB 192
-#define KYBIASTORGB 224
+#define KYTORGB 96
+#define KYBIASTORGB 128
 
 #endif
 
@@ -1031,11 +1011,11 @@ void BGRAToYRow_SSSE3(const uint8_t* src_bgra, uint8_t* dst_y, int width);
 void ABGRToYRow_SSSE3(const uint8_t* src_abgr, uint8_t* dst_y, int width);
 void RGBAToYRow_SSSE3(const uint8_t* src_rgba, uint8_t* dst_y, int width);
 void RGB24ToYRow_SSSE3(const uint8_t* src_rgb24, uint8_t* dst_y, int width);
-void RGB24ToYJRow_SSSE3(const uint8_t* src_rgb24, uint8_t* dst_y, int width);
+void RGB24ToYJRow_SSSE3(const uint8_t* src_rgb24, uint8_t* dst_yj, int width);
 void RAWToYRow_SSSE3(const uint8_t* src_raw, uint8_t* dst_y, int width);
-void RAWToYJRow_SSSE3(const uint8_t* src_raw, uint8_t* dst_y, int width);
-void RGB24ToYJRow_AVX2(const uint8_t* src_rgb24, uint8_t* dst_y, int width);
-void RAWToYJRow_AVX2(const uint8_t* src_raw, uint8_t* dst_y, int width);
+void RAWToYJRow_SSSE3(const uint8_t* src_raw, uint8_t* dst_yj, int width);
+void RGB24ToYJRow_AVX2(const uint8_t* src_rgb24, uint8_t* dst_yj, int width);
+void RAWToYJRow_AVX2(const uint8_t* src_raw, uint8_t* dst_yj, int width);
 void ARGBToYRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width);
 void ARGBToYJRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width);
 void RGBAToYJRow_NEON(const uint8_t* src_rgba, uint8_t* dst_y, int width);
@@ -1230,16 +1210,16 @@ void RGB565ToYRow_MMI(const uint8_t* src_rgb565, uint8_t* dst_y, int width);
 void ARGB1555ToYRow_MMI(const uint8_t* src_argb1555, uint8_t* dst_y, int width);
 void ARGB4444ToYRow_MMI(const uint8_t* src_argb4444, uint8_t* dst_y, int width);
 
-void ARGBToYRow_C(const uint8_t* src_argb, uint8_t* dst_y, int width);
-void ARGBToYJRow_C(const uint8_t* src_argb, uint8_t* dst_y, int width);
-void RGBAToYJRow_C(const uint8_t* src_argb0, uint8_t* dst_y, int width);
-void BGRAToYRow_C(const uint8_t* src_argb, uint8_t* dst_y, int width);
-void ABGRToYRow_C(const uint8_t* src_argb, uint8_t* dst_y, int width);
-void RGBAToYRow_C(const uint8_t* src_argb, uint8_t* dst_y, int width);
-void RGB24ToYRow_C(const uint8_t* src_argb, uint8_t* dst_y, int width);
-void RGB24ToYJRow_C(const uint8_t* src_argb, uint8_t* dst_yj, int width);
-void RAWToYRow_C(const uint8_t* src_argb, uint8_t* dst_y, int width);
-void RAWToYJRow_C(const uint8_t* src_argb, uint8_t* dst_yj, int width);
+void ARGBToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void ARGBToYJRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void RGBAToYJRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void BGRAToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void ABGRToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void RGBAToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void RGB24ToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void RGB24ToYJRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void RAWToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
+void RAWToYJRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width);
 void RGB565ToYRow_C(const uint8_t* src_rgb565, uint8_t* dst_y, int width);
 void ARGB1555ToYRow_C(const uint8_t* src_argb1555, uint8_t* dst_y, int width);
 void ARGB4444ToYRow_C(const uint8_t* src_argb4444, uint8_t* dst_y, int width);
@@ -1341,42 +1321,42 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba,
                        uint8_t* dst_v,
                        int width);
 void ARGBToUVRow_Any_AVX2(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
 void ABGRToUVRow_Any_AVX2(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
 void ARGBToUVJRow_Any_AVX2(const uint8_t* src_ptr,
-                           int src_stride_ptr,
+                           int src_stride,
                            uint8_t* dst_u,
                            uint8_t* dst_v,
                            int width);
 void ARGBToUVRow_Any_SSSE3(const uint8_t* src_ptr,
-                           int src_stride_ptr,
+                           int src_stride,
                            uint8_t* dst_u,
                            uint8_t* dst_v,
                            int width);
 void ARGBToUVJRow_Any_SSSE3(const uint8_t* src_ptr,
-                            int src_stride_ptr,
+                            int src_stride,
                             uint8_t* dst_u,
                             uint8_t* dst_v,
                             int width);
 void BGRAToUVRow_Any_SSSE3(const uint8_t* src_ptr,
-                           int src_stride_ptr,
+                           int src_stride,
                            uint8_t* dst_u,
                            uint8_t* dst_v,
                            int width);
 void ABGRToUVRow_Any_SSSE3(const uint8_t* src_ptr,
-                           int src_stride_ptr,
+                           int src_stride,
                            uint8_t* dst_u,
                            uint8_t* dst_v,
                            int width);
 void RGBAToUVRow_Any_SSSE3(const uint8_t* src_ptr,
-                           int src_stride_ptr,
+                           int src_stride,
                            uint8_t* dst_u,
                            uint8_t* dst_v,
                            int width);
@@ -1385,7 +1365,7 @@ void ARGBToUV444Row_Any_NEON(const uint8_t* src_ptr,
                              uint8_t* dst_v,
                              int width);
 void ARGBToUVRow_Any_NEON(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
@@ -1408,47 +1388,47 @@ void ARGBToUVRow_Any_MMI(const uint8_t* src_ptr,
                          uint8_t* dst_v,
                          int width);
 void ARGBToUVJRow_Any_NEON(const uint8_t* src_ptr,
-                           int src_stride_ptr,
+                           int src_stride,
                            uint8_t* dst_u,
                            uint8_t* dst_v,
                            int width);
 void BGRAToUVRow_Any_NEON(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
 void ABGRToUVRow_Any_NEON(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
 void RGBAToUVRow_Any_NEON(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
 void RGB24ToUVRow_Any_NEON(const uint8_t* src_ptr,
-                           int src_stride_ptr,
+                           int src_stride,
                            uint8_t* dst_u,
                            uint8_t* dst_v,
                            int width);
 void RAWToUVRow_Any_NEON(const uint8_t* src_ptr,
-                         int src_stride_ptr,
+                         int src_stride,
                          uint8_t* dst_u,
                          uint8_t* dst_v,
                          int width);
 void RGB565ToUVRow_Any_NEON(const uint8_t* src_ptr,
-                            int src_stride_ptr,
+                            int src_stride,
                             uint8_t* dst_u,
                             uint8_t* dst_v,
                             int width);
 void ARGB1555ToUVRow_Any_NEON(const uint8_t* src_ptr,
-                              int src_stride_ptr,
+                              int src_stride,
                               uint8_t* dst_u,
                               uint8_t* dst_v,
                               int width);
 void ARGB4444ToUVRow_Any_NEON(const uint8_t* src_ptr,
-                              int src_stride_ptr,
+                              int src_stride,
                               uint8_t* dst_u,
                               uint8_t* dst_v,
                               int width);
@@ -1657,7 +1637,7 @@ void MirrorSplitUVRow_C(const uint8_t* src_uv,
 
 void ARGBMirrorRow_AVX2(const uint8_t* src, uint8_t* dst, int width);
 void ARGBMirrorRow_SSE2(const uint8_t* src, uint8_t* dst, int width);
-void ARGBMirrorRow_NEON(const uint8_t* src, uint8_t* dst, int width);
+void ARGBMirrorRow_NEON(const uint8_t* src_argb, uint8_t* dst_argb, int width);
 void ARGBMirrorRow_MSA(const uint8_t* src, uint8_t* dst, int width);
 void ARGBMirrorRow_MMI(const uint8_t* src, uint8_t* dst, int width);
 void ARGBMirrorRow_C(const uint8_t* src, uint8_t* dst, int width);
@@ -1673,9 +1653,13 @@ void ARGBMirrorRow_Any_NEON(const uint8_t* src_ptr,
 void ARGBMirrorRow_Any_MSA(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void ARGBMirrorRow_Any_MMI(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 
-void RGB24MirrorRow_SSSE3(const uint8_t* src, uint8_t* dst, int width);
-void RGB24MirrorRow_NEON(const uint8_t* src, uint8_t* dst, int width);
-void RGB24MirrorRow_C(const uint8_t* src, uint8_t* dst, int width);
+void RGB24MirrorRow_SSSE3(const uint8_t* src_rgb24,
+                          uint8_t* dst_rgb24,
+                          int width);
+void RGB24MirrorRow_NEON(const uint8_t* src_rgb24,
+                         uint8_t* dst_rgb24,
+                         int width);
+void RGB24MirrorRow_C(const uint8_t* src_rgb24, uint8_t* dst_rgb24, int width);
 void RGB24MirrorRow_Any_SSSE3(const uint8_t* src_ptr,
                               uint8_t* dst_ptr,
                               int width);
@@ -1896,23 +1880,23 @@ void MergeARGBRow_NEON(const uint8_t* src_r,
                        const uint8_t* src_a,
                        uint8_t* dst_argb,
                        int width);
-void MergeARGBRow_Any_SSE2(const uint8_t* src_r,
-                           const uint8_t* src_g,
-                           const uint8_t* src_b,
-                           const uint8_t* src_a,
-                           uint8_t* dst_argb,
+void MergeARGBRow_Any_SSE2(const uint8_t* y_buf,
+                           const uint8_t* u_buf,
+                           const uint8_t* v_buf,
+                           const uint8_t* a_buf,
+                           uint8_t* dst_ptr,
                            int width);
-void MergeARGBRow_Any_AVX2(const uint8_t* src_r,
-                           const uint8_t* src_g,
-                           const uint8_t* src_b,
-                           const uint8_t* src_a,
-                           uint8_t* dst_argb,
+void MergeARGBRow_Any_AVX2(const uint8_t* y_buf,
+                           const uint8_t* u_buf,
+                           const uint8_t* v_buf,
+                           const uint8_t* a_buf,
+                           uint8_t* dst_ptr,
                            int width);
-void MergeARGBRow_Any_NEON(const uint8_t* src_r,
-                           const uint8_t* src_g,
-                           const uint8_t* src_b,
-                           const uint8_t* src_a,
-                           uint8_t* dst_argb,
+void MergeARGBRow_Any_NEON(const uint8_t* y_buf,
+                           const uint8_t* u_buf,
+                           const uint8_t* v_buf,
+                           const uint8_t* a_buf,
+                           uint8_t* dst_ptr,
                            int width);
 void SplitARGBRow_C(const uint8_t* src_argb,
                     uint8_t* dst_r,
@@ -1938,31 +1922,31 @@ void SplitARGBRow_AVX2(const uint8_t* src_argb,
                        uint8_t* dst_b,
                        uint8_t* dst_a,
                        int width);
-void SplitARGBRow_NEON(const uint8_t* src_argb,
+void SplitARGBRow_NEON(const uint8_t* src_rgba,
                        uint8_t* dst_r,
                        uint8_t* dst_g,
                        uint8_t* dst_b,
                        uint8_t* dst_a,
                        int width);
-void SplitARGBRow_Any_SSE2(const uint8_t* src_argb,
+void SplitARGBRow_Any_SSE2(const uint8_t* src_ptr,
                            uint8_t* dst_r,
                            uint8_t* dst_g,
                            uint8_t* dst_b,
                            uint8_t* dst_a,
                            int width);
-void SplitARGBRow_Any_SSSE3(const uint8_t* src_argb,
+void SplitARGBRow_Any_SSSE3(const uint8_t* src_ptr,
                             uint8_t* dst_r,
                             uint8_t* dst_g,
                             uint8_t* dst_b,
                             uint8_t* dst_a,
                             int width);
-void SplitARGBRow_Any_AVX2(const uint8_t* src_argb,
+void SplitARGBRow_Any_AVX2(const uint8_t* src_ptr,
                            uint8_t* dst_r,
                            uint8_t* dst_g,
                            uint8_t* dst_b,
                            uint8_t* dst_a,
                            int width);
-void SplitARGBRow_Any_NEON(const uint8_t* src_argb,
+void SplitARGBRow_Any_NEON(const uint8_t* src_ptr,
                            uint8_t* dst_r,
                            uint8_t* dst_g,
                            uint8_t* dst_b,
@@ -1988,20 +1972,20 @@ void MergeXRGBRow_NEON(const uint8_t* src_r,
                        const uint8_t* src_b,
                        uint8_t* dst_argb,
                        int width);
-void MergeXRGBRow_Any_SSE2(const uint8_t* src_r,
-                           const uint8_t* src_g,
-                           const uint8_t* src_b,
-                           uint8_t* dst_argb,
+void MergeXRGBRow_Any_SSE2(const uint8_t* y_buf,
+                           const uint8_t* u_buf,
+                           const uint8_t* v_buf,
+                           uint8_t* dst_ptr,
                            int width);
-void MergeXRGBRow_Any_AVX2(const uint8_t* src_r,
-                           const uint8_t* src_g,
-                           const uint8_t* src_b,
-                           uint8_t* dst_argb,
+void MergeXRGBRow_Any_AVX2(const uint8_t* y_buf,
+                           const uint8_t* u_buf,
+                           const uint8_t* v_buf,
+                           uint8_t* dst_ptr,
                            int width);
-void MergeXRGBRow_Any_NEON(const uint8_t* src_r,
-                           const uint8_t* src_g,
-                           const uint8_t* src_b,
-                           uint8_t* dst_argb,
+void MergeXRGBRow_Any_NEON(const uint8_t* y_buf,
+                           const uint8_t* u_buf,
+                           const uint8_t* v_buf,
+                           uint8_t* dst_ptr,
                            int width);
 void SplitXRGBRow_C(const uint8_t* src_argb,
                     uint8_t* dst_r,
@@ -2023,32 +2007,205 @@ void SplitXRGBRow_AVX2(const uint8_t* src_argb,
                        uint8_t* dst_g,
                        uint8_t* dst_b,
                        int width);
-void SplitXRGBRow_NEON(const uint8_t* src_argb,
+void SplitXRGBRow_NEON(const uint8_t* src_rgba,
                        uint8_t* dst_r,
                        uint8_t* dst_g,
                        uint8_t* dst_b,
                        int width);
-void SplitXRGBRow_Any_SSE2(const uint8_t* src_argb,
+void SplitXRGBRow_Any_SSE2(const uint8_t* src_ptr,
                            uint8_t* dst_r,
                            uint8_t* dst_g,
                            uint8_t* dst_b,
                            int width);
-void SplitXRGBRow_Any_SSSE3(const uint8_t* src_argb,
+void SplitXRGBRow_Any_SSSE3(const uint8_t* src_ptr,
                             uint8_t* dst_r,
                             uint8_t* dst_g,
                             uint8_t* dst_b,
                             int width);
-void SplitXRGBRow_Any_AVX2(const uint8_t* src_argb,
+void SplitXRGBRow_Any_AVX2(const uint8_t* src_ptr,
                            uint8_t* dst_r,
                            uint8_t* dst_g,
                            uint8_t* dst_b,
                            int width);
-void SplitXRGBRow_Any_NEON(const uint8_t* src_argb,
+void SplitXRGBRow_Any_NEON(const uint8_t* src_ptr,
                            uint8_t* dst_r,
                            uint8_t* dst_g,
                            uint8_t* dst_b,
                            int width);
 
+void MergeXR30Row_C(const uint16_t* src_r,
+                    const uint16_t* src_g,
+                    const uint16_t* src_b,
+                    uint8_t* dst_ar30,
+                    int depth,
+                    int width);
+void MergeAR64Row_C(const uint16_t* src_r,
+                    const uint16_t* src_g,
+                    const uint16_t* src_b,
+                    const uint16_t* src_a,
+                    uint16_t* dst_ar64,
+                    int depth,
+                    int width);
+void MergeARGB16To8Row_C(const uint16_t* src_r,
+                         const uint16_t* src_g,
+                         const uint16_t* src_b,
+                         const uint16_t* src_a,
+                         uint8_t* dst_argb,
+                         int depth,
+                         int width);
+void MergeXR64Row_C(const uint16_t* src_r,
+                    const uint16_t* src_g,
+                    const uint16_t* src_b,
+                    uint16_t* dst_ar64,
+                    int depth,
+                    int width);
+void MergeXRGB16To8Row_C(const uint16_t* src_r,
+                         const uint16_t* src_g,
+                         const uint16_t* src_b,
+                         uint8_t* dst_argb,
+                         int depth,
+                         int width);
+void MergeXR30Row_AVX2(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint8_t* dst_ar30,
+                       int depth,
+                       int width);
+void MergeAR64Row_AVX2(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       const uint16_t* src_a,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width);
+void MergeARGB16To8Row_AVX2(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            const uint16_t* src_a,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width);
+void MergeXR64Row_AVX2(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width);
+void MergeXRGB16To8Row_AVX2(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width);
+void MergeXR30Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint8_t* dst_ar30,
+                       int depth,
+                       int width);
+void MergeXR30Row_10_NEON(const uint16_t* src_r,
+                          const uint16_t* src_g,
+                          const uint16_t* src_b,
+                          uint8_t* dst_ar30,
+                          int /* depth */,
+                          int width);
+void MergeAR64Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       const uint16_t* src_a,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width);
+void MergeARGB16To8Row_NEON(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            const uint16_t* src_a,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width);
+void MergeXR64Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width);
+void MergeXRGB16To8Row_NEON(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width);
+void MergeXR30Row_Any_AVX2(const uint16_t* r_buf,
+                           const uint16_t* g_buf,
+                           const uint16_t* b_buf,
+                           uint8_t* dst_ptr,
+                           int depth,
+                           int width);
+void MergeAR64Row_Any_AVX2(const uint16_t* r_buf,
+                           const uint16_t* g_buf,
+                           const uint16_t* b_buf,
+                           const uint16_t* a_buf,
+                           uint16_t* dst_ptr,
+                           int depth,
+                           int width);
+void MergeXR64Row_Any_AVX2(const uint16_t* r_buf,
+                           const uint16_t* g_buf,
+                           const uint16_t* b_buf,
+                           uint16_t* dst_ptr,
+                           int depth,
+                           int width);
+void MergeARGB16To8Row_Any_AVX2(const uint16_t* r_buf,
+                                const uint16_t* g_buf,
+                                const uint16_t* b_buf,
+                                const uint16_t* a_buf,
+                                uint8_t* dst_ptr,
+                                int depth,
+                                int width);
+void MergeXRGB16To8Row_Any_AVX2(const uint16_t* r_buf,
+                                const uint16_t* g_buf,
+                                const uint16_t* b_buf,
+                                uint8_t* dst_ptr,
+                                int depth,
+                                int width);
+void MergeXR30Row_Any_NEON(const uint16_t* r_buf,
+                           const uint16_t* g_buf,
+                           const uint16_t* b_buf,
+                           uint8_t* dst_ptr,
+                           int depth,
+                           int width);
+void MergeXR30Row_10_Any_NEON(const uint16_t* r_buf,
+                              const uint16_t* g_buf,
+                              const uint16_t* b_buf,
+                              uint8_t* dst_ptr,
+                              int depth,
+                              int width);
+void MergeAR64Row_Any_NEON(const uint16_t* r_buf,
+                           const uint16_t* g_buf,
+                           const uint16_t* b_buf,
+                           const uint16_t* a_buf,
+                           uint16_t* dst_ptr,
+                           int depth,
+                           int width);
+void MergeARGB16To8Row_Any_NEON(const uint16_t* r_buf,
+                                const uint16_t* g_buf,
+                                const uint16_t* b_buf,
+                                const uint16_t* a_buf,
+                                uint8_t* dst_ptr,
+                                int depth,
+                                int width);
+void MergeXR64Row_Any_NEON(const uint16_t* r_buf,
+                           const uint16_t* g_buf,
+                           const uint16_t* b_buf,
+                           uint16_t* dst_ptr,
+                           int depth,
+                           int width);
+void MergeXRGB16To8Row_Any_NEON(const uint16_t* r_buf,
+                                const uint16_t* g_buf,
+                                const uint16_t* b_buf,
+                                uint8_t* dst_ptr,
+                                int depth,
+                                int width);
+
 void MergeUVRow_16_C(const uint16_t* src_u,
                      const uint16_t* src_v,
                      uint16_t* dst_uv,
@@ -2109,16 +2266,16 @@ void MultiplyRow_16_AVX2(const uint16_t* src_y,
                          uint16_t* dst_y,
                          int scale,
                          int width);
-void MultiplyRow_16_Any_AVX2(const uint16_t* src_y,
-                             uint16_t* dst_y,
+void MultiplyRow_16_Any_AVX2(const uint16_t* src_ptr,
+                             uint16_t* dst_ptr,
                              int scale,
                              int width);
 void MultiplyRow_16_NEON(const uint16_t* src_y,
                          uint16_t* dst_y,
                          int scale,
                          int width);
-void MultiplyRow_16_Any_NEON(const uint16_t* src_y,
-                             uint16_t* dst_y,
+void MultiplyRow_16_Any_NEON(const uint16_t* src_ptr,
+                             uint16_t* dst_ptr,
                              int scale,
                              int width);
 
@@ -2130,16 +2287,16 @@ void DivideRow_16_AVX2(const uint16_t* src_y,
                        uint16_t* dst_y,
                        int scale,
                        int width);
-void DivideRow_16_Any_AVX2(const uint16_t* src_y,
-                           uint16_t* dst_y,
+void DivideRow_16_Any_AVX2(const uint16_t* src_ptr,
+                           uint16_t* dst_ptr,
                            int scale,
                            int width);
 void DivideRow_16_NEON(const uint16_t* src_y,
                        uint16_t* dst_y,
                        int scale,
                        int width);
-void DivideRow_16_Any_NEON(const uint16_t* src_y,
-                           uint16_t* dst_y,
+void DivideRow_16_Any_NEON(const uint16_t* src_ptr,
+                           uint16_t* dst_ptr,
                            int scale,
                            int width);
 
@@ -2563,6 +2720,71 @@ void ARGBToARGB4444Row_C(const uint8_t* src_argb, uint8_t* dst_rgb, int width);
 void ABGRToAR30Row_C(const uint8_t* src_abgr, uint8_t* dst_ar30, int width);
 void ARGBToAR30Row_C(const uint8_t* src_argb, uint8_t* dst_ar30, int width);
 
+void ARGBToAR64Row_C(const uint8_t* src_argb, uint16_t* dst_ar64, int width);
+void ARGBToAB64Row_C(const uint8_t* src_argb, uint16_t* dst_ab64, int width);
+void AR64ToARGBRow_C(const uint16_t* src_ar64, uint8_t* dst_argb, int width);
+void AB64ToARGBRow_C(const uint16_t* src_ab64, uint8_t* dst_argb, int width);
+void AR64ShuffleRow_C(const uint8_t* src_ar64,
+                      uint8_t* dst_ar64,
+                      const uint8_t* shuffler,
+                      int width);
+void ARGBToAR64Row_SSSE3(const uint8_t* src_argb,
+                         uint16_t* dst_ar64,
+                         int width);
+void ARGBToAB64Row_SSSE3(const uint8_t* src_argb,
+                         uint16_t* dst_ab64,
+                         int width);
+void AR64ToARGBRow_SSSE3(const uint16_t* src_ar64,
+                         uint8_t* dst_argb,
+                         int width);
+void AB64ToARGBRow_SSSE3(const uint16_t* src_ab64,
+                         uint8_t* dst_argb,
+                         int width);
+void ARGBToAR64Row_AVX2(const uint8_t* src_argb, uint16_t* dst_ar64, int width);
+void ARGBToAB64Row_AVX2(const uint8_t* src_argb, uint16_t* dst_ab64, int width);
+void AR64ToARGBRow_AVX2(const uint16_t* src_ar64, uint8_t* dst_argb, int width);
+void AB64ToARGBRow_AVX2(const uint16_t* src_ab64, uint8_t* dst_argb, int width);
+void ARGBToAR64Row_NEON(const uint8_t* src_argb, uint16_t* dst_ar64, int width);
+void ARGBToAB64Row_NEON(const uint8_t* src_argb, uint16_t* dst_ab64, int width);
+void AR64ToARGBRow_NEON(const uint16_t* src_ar64, uint8_t* dst_argb, int width);
+void AB64ToARGBRow_NEON(const uint16_t* src_ab64, uint8_t* dst_argb, int width);
+void ARGBToAR64Row_Any_SSSE3(const uint8_t* src_ptr,
+                             uint16_t* dst_ptr,
+                             int width);
+void ARGBToAB64Row_Any_SSSE3(const uint8_t* src_ptr,
+                             uint16_t* dst_ptr,
+                             int width);
+void AR64ToARGBRow_Any_SSSE3(const uint16_t* src_ptr,
+                             uint8_t* dst_ptr,
+                             int width);
+void AB64ToARGBRow_Any_SSSE3(const uint16_t* src_ptr,
+                             uint8_t* dst_ptr,
+                             int width);
+void ARGBToAR64Row_Any_AVX2(const uint8_t* src_ptr,
+                            uint16_t* dst_ptr,
+                            int width);
+void ARGBToAB64Row_Any_AVX2(const uint8_t* src_ptr,
+                            uint16_t* dst_ptr,
+                            int width);
+void AR64ToARGBRow_Any_AVX2(const uint16_t* src_ptr,
+                            uint8_t* dst_ptr,
+                            int width);
+void AB64ToARGBRow_Any_AVX2(const uint16_t* src_ptr,
+                            uint8_t* dst_ptr,
+                            int width);
+void ARGBToAR64Row_Any_NEON(const uint8_t* src_ptr,
+                            uint16_t* dst_ptr,
+                            int width);
+void ARGBToAB64Row_Any_NEON(const uint8_t* src_ptr,
+                            uint16_t* dst_ptr,
+                            int width);
+void AR64ToARGBRow_Any_NEON(const uint16_t* src_ptr,
+                            uint8_t* dst_ptr,
+                            int width);
+void AB64ToARGBRow_Any_NEON(const uint16_t* src_ptr,
+                            uint8_t* dst_ptr,
+                            int width);
+
 void J400ToARGBRow_SSE2(const uint8_t* src_y, uint8_t* dst_argb, int width);
 void J400ToARGBRow_AVX2(const uint8_t* src_y, uint8_t* dst_argb, int width);
 void J400ToARGBRow_NEON(const uint8_t* src_y, uint8_t* dst_argb, int width);
@@ -3449,15 +3671,15 @@ void I400ToARGBRow_MMI(const uint8_t* src_y,
                        int width);
 void I400ToARGBRow_Any_SSE2(const uint8_t* src_ptr,
                             uint8_t* dst_ptr,
-                            const struct YuvConstants* yuvconstants,
+                            const struct YuvConstants* param,
                             int width);
 void I400ToARGBRow_Any_AVX2(const uint8_t* src_ptr,
                             uint8_t* dst_ptr,
-                            const struct YuvConstants* yuvconstants,
+                            const struct YuvConstants* param,
                             int width);
 void I400ToARGBRow_Any_NEON(const uint8_t* src_ptr,
                             uint8_t* dst_ptr,
-                            const struct YuvConstants* yuvconstants,
+                            const struct YuvConstants* param,
                             int width);
 void I400ToARGBRow_Any_MSA(const uint8_t* src_ptr,
                            uint8_t* dst_ptr,
@@ -3469,11 +3691,11 @@ void I400ToARGBRow_Any_MMI(const uint8_t* src_ptr,
                            int width);
 
 // ARGB preattenuated alpha blend.
-void ARGBBlendRow_SSSE3(const uint8_t* src_argb0,
+void ARGBBlendRow_SSSE3(const uint8_t* src_argb,
                         const uint8_t* src_argb1,
                         uint8_t* dst_argb,
                         int width);
-void ARGBBlendRow_NEON(const uint8_t* src_argb0,
+void ARGBBlendRow_NEON(const uint8_t* src_argb,
                        const uint8_t* src_argb1,
                        uint8_t* dst_argb,
                        int width);
@@ -3485,7 +3707,7 @@ void ARGBBlendRow_MMI(const uint8_t* src_argb0,
                       const uint8_t* src_argb1,
                       uint8_t* dst_argb,
                       int width);
-void ARGBBlendRow_C(const uint8_t* src_argb0,
+void ARGBBlendRow_C(const uint8_t* src_argb,
                     const uint8_t* src_argb1,
                     uint8_t* dst_argb,
                     int width);
@@ -3529,11 +3751,11 @@ void BlendPlaneRow_C(const uint8_t* src0,
 
 // ARGB multiply images. Same API as Blend, but these require
 // pointer and width alignment for SSE2.
-void ARGBMultiplyRow_C(const uint8_t* src_argb0,
+void ARGBMultiplyRow_C(const uint8_t* src_argb,
                        const uint8_t* src_argb1,
                        uint8_t* dst_argb,
                        int width);
-void ARGBMultiplyRow_SSE2(const uint8_t* src_argb0,
+void ARGBMultiplyRow_SSE2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width);
@@ -3541,7 +3763,7 @@ void ARGBMultiplyRow_Any_SSE2(const uint8_t* y_buf,
                               const uint8_t* uv_buf,
                               uint8_t* dst_ptr,
                               int width);
-void ARGBMultiplyRow_AVX2(const uint8_t* src_argb0,
+void ARGBMultiplyRow_AVX2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width);
@@ -3549,7 +3771,7 @@ void ARGBMultiplyRow_Any_AVX2(const uint8_t* y_buf,
                               const uint8_t* uv_buf,
                               uint8_t* dst_ptr,
                               int width);
-void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
+void ARGBMultiplyRow_NEON(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width);
@@ -3575,11 +3797,11 @@ void ARGBMultiplyRow_Any_MMI(const uint8_t* y_buf,
                              int width);
 
 // ARGB add images.
-void ARGBAddRow_C(const uint8_t* src_argb0,
+void ARGBAddRow_C(const uint8_t* src_argb,
                   const uint8_t* src_argb1,
                   uint8_t* dst_argb,
                   int width);
-void ARGBAddRow_SSE2(const uint8_t* src_argb0,
+void ARGBAddRow_SSE2(const uint8_t* src_argb,
                      const uint8_t* src_argb1,
                      uint8_t* dst_argb,
                      int width);
@@ -3587,7 +3809,7 @@ void ARGBAddRow_Any_SSE2(const uint8_t* y_buf,
                          const uint8_t* uv_buf,
                          uint8_t* dst_ptr,
                          int width);
-void ARGBAddRow_AVX2(const uint8_t* src_argb0,
+void ARGBAddRow_AVX2(const uint8_t* src_argb,
                      const uint8_t* src_argb1,
                      uint8_t* dst_argb,
                      int width);
@@ -3595,7 +3817,7 @@ void ARGBAddRow_Any_AVX2(const uint8_t* y_buf,
                          const uint8_t* uv_buf,
                          uint8_t* dst_ptr,
                          int width);
-void ARGBAddRow_NEON(const uint8_t* src_argb0,
+void ARGBAddRow_NEON(const uint8_t* src_argb,
                      const uint8_t* src_argb1,
                      uint8_t* dst_argb,
                      int width);
@@ -3622,11 +3844,11 @@ void ARGBAddRow_Any_MMI(const uint8_t* y_buf,
 
 // ARGB subtract images. Same API as Blend, but these require
 // pointer and width alignment for SSE2.
-void ARGBSubtractRow_C(const uint8_t* src_argb0,
+void ARGBSubtractRow_C(const uint8_t* src_argb,
                        const uint8_t* src_argb1,
                        uint8_t* dst_argb,
                        int width);
-void ARGBSubtractRow_SSE2(const uint8_t* src_argb0,
+void ARGBSubtractRow_SSE2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width);
@@ -3634,7 +3856,7 @@ void ARGBSubtractRow_Any_SSE2(const uint8_t* y_buf,
                               const uint8_t* uv_buf,
                               uint8_t* dst_ptr,
                               int width);
-void ARGBSubtractRow_AVX2(const uint8_t* src_argb0,
+void ARGBSubtractRow_AVX2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width);
@@ -3642,7 +3864,7 @@ void ARGBSubtractRow_Any_AVX2(const uint8_t* y_buf,
                               const uint8_t* uv_buf,
                               uint8_t* dst_ptr,
                               int width);
-void ARGBSubtractRow_NEON(const uint8_t* src_argb0,
+void ARGBSubtractRow_NEON(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width);
@@ -3849,9 +4071,9 @@ void NV21ToRGB24Row_Any_NEON(const uint8_t* y_buf,
                              uint8_t* dst_ptr,
                              const struct YuvConstants* yuvconstants,
                              int width);
-void NV21ToYUV24Row_Any_NEON(const uint8_t* src_y,
-                             const uint8_t* src_vu,
-                             uint8_t* dst_yuv24,
+void NV21ToYUV24Row_Any_NEON(const uint8_t* y_buf,
+                             const uint8_t* uv_buf,
+                             uint8_t* dst_ptr,
                              int width);
 void NV12ToRGB565Row_Any_NEON(const uint8_t* y_buf,
                               const uint8_t* uv_buf,
@@ -4053,7 +4275,7 @@ void YUY2ToUV422Row_C(const uint8_t* src_yuy2,
                       int width);
 void YUY2ToYRow_Any_AVX2(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void YUY2ToUVRow_Any_AVX2(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
@@ -4063,7 +4285,7 @@ void YUY2ToUV422Row_Any_AVX2(const uint8_t* src_ptr,
                              int width);
 void YUY2ToYRow_Any_SSE2(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void YUY2ToUVRow_Any_SSE2(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
@@ -4073,7 +4295,7 @@ void YUY2ToUV422Row_Any_SSE2(const uint8_t* src_ptr,
                              int width);
 void YUY2ToYRow_Any_NEON(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void YUY2ToUVRow_Any_NEON(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
@@ -4174,7 +4396,7 @@ void UYVYToUV422Row_C(const uint8_t* src_uyvy,
                       int width);
 void UYVYToYRow_Any_AVX2(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void UYVYToUVRow_Any_AVX2(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
@@ -4184,7 +4406,7 @@ void UYVYToUV422Row_Any_AVX2(const uint8_t* src_ptr,
                              int width);
 void UYVYToYRow_Any_SSE2(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void UYVYToUVRow_Any_SSE2(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
@@ -4194,7 +4416,7 @@ void UYVYToUV422Row_Any_SSE2(const uint8_t* src_ptr,
                              int width);
 void UYVYToYRow_Any_NEON(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void UYVYToUVRow_Any_NEON(const uint8_t* src_ptr,
-                          int src_stride_ptr,
+                          int src_stride,
                           uint8_t* dst_u,
                           uint8_t* dst_v,
                           int width);
@@ -4231,29 +4453,29 @@ void SwapUVRow_AVX2(const uint8_t* src_uv, uint8_t* dst_vu, int width);
 void SwapUVRow_Any_AVX2(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
 void AYUVToYRow_C(const uint8_t* src_ayuv, uint8_t* dst_y, int width);
 void AYUVToUVRow_C(const uint8_t* src_ayuv,
-                   int stride_ayuv,
+                   int src_stride_ayuv,
                    uint8_t* dst_uv,
                    int width);
 void AYUVToVURow_C(const uint8_t* src_ayuv,
-                   int stride_ayuv,
+                   int src_stride_ayuv,
                    uint8_t* dst_vu,
                    int width);
 void AYUVToYRow_NEON(const uint8_t* src_ayuv, uint8_t* dst_y, int width);
 void AYUVToUVRow_NEON(const uint8_t* src_ayuv,
-                      int stride_ayuv,
+                      int src_stride_ayuv,
                       uint8_t* dst_uv,
                       int width);
 void AYUVToVURow_NEON(const uint8_t* src_ayuv,
-                      int stride_ayuv,
+                      int src_stride_ayuv,
                       uint8_t* dst_vu,
                       int width);
-void AYUVToYRow_Any_NEON(const uint8_t* src_ayuv, uint8_t* dst_y, int width);
-void AYUVToUVRow_Any_NEON(const uint8_t* src_ayuv,
-                          int stride_ayuv,
-                          uint8_t* dst_uv,
+void AYUVToYRow_Any_NEON(const uint8_t* src_ptr, uint8_t* dst_ptr, int width);
+void AYUVToUVRow_Any_NEON(const uint8_t* src_ptr,
+                          int src_stride,
+                          uint8_t* dst_vu,
                           int width);
-void AYUVToVURow_Any_NEON(const uint8_t* src_ayuv,
-                          int stride_ayuv,
+void AYUVToVURow_Any_NEON(const uint8_t* src_ptr,
+                          int src_stride,
                           uint8_t* dst_vu,
                           int width);
 
diff --git a/include/libyuv/scale_row.h b/include/libyuv/scale_row.h
index 86a2cf08..833af1c4 100644
--- a/include/libyuv/scale_row.h
+++ b/include/libyuv/scale_row.h
@@ -19,7 +19,7 @@ namespace libyuv {
 extern "C" {
 #endif
 
-#if defined(__pnacl__) || defined(__CLR_VER) ||            \
+#if defined(__pnacl__) || defined(__CLR_VER) || \
     (defined(__native_client__) && defined(__x86_64__)) || \
     (defined(__i386__) && !defined(__SSE__) && !defined(__clang__))
 #define LIBYUV_DISABLE_X86
@@ -75,7 +75,7 @@ extern "C" {
 // The following are available for gcc/clang x86 platforms:
 // TODO(fbarchard): Port to Visual C
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 #define HAS_SCALEUVROWDOWN2BOX_SSSE3
 #define HAS_SCALEROWUP2LINEAR_SSE2
 #define HAS_SCALEROWUP2LINEAR_SSSE3
@@ -94,8 +94,8 @@ extern "C" {
 // The following are available for gcc/clang x86 platforms, but
 // require clang 3.4 or gcc 4.7.
 // TODO(fbarchard): Port to Visual C
-#if !defined(LIBYUV_DISABLE_X86) &&                                     \
-    (defined(__x86_64__) || defined(__i386__)) && !defined(_MSC_VER) && \
+#if !defined(LIBYUV_DISABLE_X86) &&                  \
+    (defined(__x86_64__) || defined(__i386__)) &&    \
     (defined(CLANG_HAS_AVX2) || defined(GCC_HAS_AVX2))
 #define HAS_SCALEUVROWDOWN2BOX_AVX2
 #define HAS_SCALEROWUP2LINEAR_AVX2
diff --git a/include/libyuv/version.h b/include/libyuv/version.h
index 7016100d..e08a0723 100644
--- a/include/libyuv/version.h
+++ b/include/libyuv/version.h
@@ -11,6 +11,6 @@
 #ifndef INCLUDE_LIBYUV_VERSION_H_
 #define INCLUDE_LIBYUV_VERSION_H_
 
-#define LIBYUV_VERSION 1780
+#define LIBYUV_VERSION 1796
 
 #endif  // INCLUDE_LIBYUV_VERSION_H_
diff --git a/include/libyuv/video_common.h b/include/libyuv/video_common.h
index 0da3fb55..32b8a521 100644
--- a/include/libyuv/video_common.h
+++ b/include/libyuv/video_common.h
@@ -65,12 +65,14 @@ enum FourCC {
   // 1 Secondary YUV format: row biplanar.  deprecated.
   FOURCC_M420 = FOURCC('M', '4', '2', '0'),
 
-  // 11 Primary RGB formats: 4 32 bpp, 2 24 bpp, 3 16 bpp, 1 10 bpc
+  // 13 Primary RGB formats: 4 32 bpp, 2 24 bpp, 3 16 bpp, 1 10 bpc 2 64 bpp
   FOURCC_ARGB = FOURCC('A', 'R', 'G', 'B'),
   FOURCC_BGRA = FOURCC('B', 'G', 'R', 'A'),
   FOURCC_ABGR = FOURCC('A', 'B', 'G', 'R'),
   FOURCC_AR30 = FOURCC('A', 'R', '3', '0'),  // 10 bit per channel. 2101010.
   FOURCC_AB30 = FOURCC('A', 'B', '3', '0'),  // ABGR version of 10 bit
+  FOURCC_AR64 = FOURCC('A', 'R', '6', '4'),  // 16 bit per channel.
+  FOURCC_AB64 = FOURCC('A', 'B', '6', '4'),  // ABGR version of 16 bit
   FOURCC_24BG = FOURCC('2', '4', 'B', 'G'),
   FOURCC_RAW = FOURCC('r', 'a', 'w', ' '),
   FOURCC_RGBA = FOURCC('R', 'G', 'B', 'A'),
@@ -163,6 +165,8 @@ enum FourCCBpp {
   FOURCC_BPP_RGBA = 32,
   FOURCC_BPP_AR30 = 32,
   FOURCC_BPP_AB30 = 32,
+  FOURCC_BPP_AR64 = 64,
+  FOURCC_BPP_AB64 = 64,
   FOURCC_BPP_24BG = 24,
   FOURCC_BPP_RAW = 24,
   FOURCC_BPP_RGBP = 16,
diff --git a/source/compare_common.cc b/source/compare_common.cc
index d4b170ad..d1cab8d2 100644
--- a/source/compare_common.cc
+++ b/source/compare_common.cc
@@ -17,36 +17,6 @@ namespace libyuv {
 extern "C" {
 #endif
 
-#if ORIGINAL_OPT
-uint32_t HammingDistance_C1(const uint8_t* src_a,
-                            const uint8_t* src_b,
-                            int count) {
-  uint32_t diff = 0u;
-
-  int i;
-  for (i = 0; i < count; ++i) {
-    int x = src_a[i] ^ src_b[i];
-    if (x & 1)
-      ++diff;
-    if (x & 2)
-      ++diff;
-    if (x & 4)
-      ++diff;
-    if (x & 8)
-      ++diff;
-    if (x & 16)
-      ++diff;
-    if (x & 32)
-      ++diff;
-    if (x & 64)
-      ++diff;
-    if (x & 128)
-      ++diff;
-  }
-  return diff;
-}
-#endif
-
 // Hakmem method for hamming distance.
 uint32_t HammingDistance_C(const uint8_t* src_a,
                            const uint8_t* src_b,
diff --git a/source/compare_gcc.cc b/source/compare_gcc.cc
index 6700f969..7dcbf7dc 100644
--- a/source/compare_gcc.cc
+++ b/source/compare_gcc.cc
@@ -20,7 +20,7 @@ extern "C" {
 
 // This module is for GCC x86 and x64.
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 
 #if defined(__x86_64__)
 uint32_t HammingDistance_SSE42(const uint8_t* src_a,
diff --git a/source/compare_win.cc b/source/compare_win.cc
index d57d3d9d..9bb27f1d 100644
--- a/source/compare_win.cc
+++ b/source/compare_win.cc
@@ -22,8 +22,9 @@ namespace libyuv {
 extern "C" {
 #endif
 
-// This module is for 32 bit Visual C x86 and clangcl
-#if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && defined(_MSC_VER)
+// This module is for 32 bit Visual C x86
+#if !defined(LIBYUV_DISABLE_X86) && defined(_MSC_VER) && \
+    !defined(__clang__) && defined(_M_IX86)
 
 uint32_t HammingDistance_SSE42(const uint8_t* src_a,
                                const uint8_t* src_b,
@@ -77,8 +78,7 @@ __declspec(naked) uint32_t
   }
 }
 
-// Visual C 2012 required for AVX2.
-#if _MSC_VER >= 1700
+#ifdef HAS_SUMSQUAREERROR_AVX2
 // C4752: found Intel(R) Advanced Vector Extensions; consider using /arch:AVX.
 #pragma warning(disable : 4752)
 __declspec(naked) uint32_t
@@ -118,7 +118,7 @@ __declspec(naked) uint32_t
     ret
   }
 }
-#endif  // _MSC_VER >= 1700
+#endif  // HAS_SUMSQUAREERROR_AVX2
 
 uvec32 kHash16x33 = {0x92d9e201, 0, 0, 0};  // 33 ^ 16
 uvec32 kHashMul0 = {
@@ -196,7 +196,7 @@ __declspec(naked) uint32_t
 }
 
 // Visual C 2012 required for AVX2.
-#if _MSC_VER >= 1700
+#ifdef HAS_HASHDJB2_AVX2
 __declspec(naked) uint32_t
     HashDjb2_AVX2(const uint8_t* src, int count, uint32_t seed) {
   __asm {
@@ -231,7 +231,7 @@ __declspec(naked) uint32_t
     ret
   }
 }
-#endif  // _MSC_VER >= 1700
+#endif  // HAS_HASHDJB2_AVX2
 
 #endif  // !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86)
 
diff --git a/source/convert.cc b/source/convert.cc
index 1bd59659..69f7fb6e 100644
--- a/source/convert.cc
+++ b/source/convert.cc
@@ -400,7 +400,7 @@ int I210ToI010(const uint16_t* src_y,
 }
 
 // Any I[420]1[02] to P[420]1[02] format with mirroring.
-static int Ix1xToPx1x(const uint16_t* src_y,
+static int IxxxToPxxx(const uint16_t* src_y,
                       int src_stride_y,
                       const uint16_t* src_u,
                       int src_stride_u,
@@ -441,7 +441,7 @@ int I010ToP010(const uint16_t* src_y,
                int dst_stride_uv,
                int width,
                int height) {
-  return Ix1xToPx1x(src_y, src_stride_y, src_u, src_stride_u, src_v,
+  return IxxxToPxxx(src_y, src_stride_y, src_u, src_stride_u, src_v,
                     src_stride_v, dst_y, dst_stride_y, dst_uv, dst_stride_uv,
                     width, height, 1, 1, 10);
 }
@@ -459,7 +459,7 @@ int I210ToP210(const uint16_t* src_y,
                int dst_stride_uv,
                int width,
                int height) {
-  return Ix1xToPx1x(src_y, src_stride_y, src_u, src_stride_u, src_v,
+  return IxxxToPxxx(src_y, src_stride_y, src_u, src_stride_u, src_v,
                     src_stride_v, dst_y, dst_stride_y, dst_uv, dst_stride_uv,
                     width, height, 1, 0, 10);
 }
@@ -477,7 +477,7 @@ int I012ToP012(const uint16_t* src_y,
                int dst_stride_uv,
                int width,
                int height) {
-  return Ix1xToPx1x(src_y, src_stride_y, src_u, src_stride_u, src_v,
+  return IxxxToPxxx(src_y, src_stride_y, src_u, src_stride_u, src_v,
                     src_stride_v, dst_y, dst_stride_y, dst_uv, dst_stride_uv,
                     width, height, 1, 1, 12);
 }
@@ -495,7 +495,7 @@ int I212ToP212(const uint16_t* src_y,
                int dst_stride_uv,
                int width,
                int height) {
-  return Ix1xToPx1x(src_y, src_stride_y, src_u, src_stride_u, src_v,
+  return IxxxToPxxx(src_y, src_stride_y, src_u, src_stride_u, src_v,
                     src_stride_v, dst_y, dst_stride_y, dst_uv, dst_stride_uv,
                     width, height, 1, 0, 12);
 }
@@ -1368,6 +1368,18 @@ int ARGBToI420(const uint8_t* src_argb,
     src_argb = src_argb + (height - 1) * src_stride_argb;
     src_stride_argb = -src_stride_argb;
   }
+#if defined(HAS_ARGBTOYROW_NEON) && defined(HAS_ARGBTOUVROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    ARGBToYRow = ARGBToYRow_Any_NEON;
+    ARGBToUVRow = ARGBToUVRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToYRow = ARGBToYRow_NEON;
+      if (IS_ALIGNED(width, 16)) {
+        ARGBToUVRow = ARGBToUVRow_NEON;
+      }
+    }
+  }
+#endif
 #if defined(HAS_ARGBTOYROW_SSSE3) && defined(HAS_ARGBTOUVROW_SSSE3)
   if (TestCpuFlag(kCpuHasSSSE3)) {
     ARGBToUVRow = ARGBToUVRow_Any_SSSE3;
@@ -1388,22 +1400,6 @@ int ARGBToI420(const uint8_t* src_argb,
     }
   }
 #endif
-#if defined(HAS_ARGBTOYROW_NEON)
-  if (TestCpuFlag(kCpuHasNEON)) {
-    ARGBToYRow = ARGBToYRow_Any_NEON;
-    if (IS_ALIGNED(width, 8)) {
-      ARGBToYRow = ARGBToYRow_NEON;
-    }
-  }
-#endif
-#if defined(HAS_ARGBTOUVROW_NEON)
-  if (TestCpuFlag(kCpuHasNEON)) {
-    ARGBToUVRow = ARGBToUVRow_Any_NEON;
-    if (IS_ALIGNED(width, 16)) {
-      ARGBToUVRow = ARGBToUVRow_NEON;
-    }
-  }
-#endif
 #if defined(HAS_ARGBTOYROW_MMI) && defined(HAS_ARGBTOUVROW_MMI)
   if (TestCpuFlag(kCpuHasMMI)) {
     ARGBToYRow = ARGBToYRow_Any_MMI;
@@ -1771,7 +1767,7 @@ int RGB24ToI420(const uint8_t* src_rgb24,
   }
 
 // Neon version does direct RGB24 to YUV.
-#if defined(HAS_RGB24TOYROW_NEON)
+#if defined(HAS_RGB24TOYROW_NEON) && defined(HAS_RGB24TOUVROW_NEON)
   if (TestCpuFlag(kCpuHasNEON)) {
     RGB24ToUVRow = RGB24ToUVRow_Any_NEON;
     RGB24ToYRow = RGB24ToYRow_Any_NEON;
@@ -1808,6 +1804,14 @@ int RGB24ToI420(const uint8_t* src_rgb24,
 #endif
 // Other platforms do intermediate conversion from RGB24 to ARGB.
 #else
+#if defined(HAS_RGB24TOARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    RGB24ToARGBRow = RGB24ToARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      RGB24ToARGBRow = RGB24ToARGBRow_NEON;
+    }
+  }
+#endif
 #if defined(HAS_RGB24TOARGBROW_SSSE3)
   if (TestCpuFlag(kCpuHasSSSE3)) {
     RGB24ToARGBRow = RGB24ToARGBRow_Any_SSSE3;
@@ -1816,6 +1820,18 @@ int RGB24ToI420(const uint8_t* src_rgb24,
     }
   }
 #endif
+#if defined(HAS_ARGBTOYROW_NEON) && defined(HAS_ARGBTOUVROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    ARGBToUVRow = ARGBToUVRow_Any_NEON;
+    ARGBToYRow = ARGBToYRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToYRow = ARGBToYRow_NEON;
+      if (IS_ALIGNED(width, 16)) {
+        ARGBToUVRow = ARGBToUVRow_NEON;
+      }
+    }
+  }
+#endif
 #if defined(HAS_ARGBTOYROW_SSSE3) && defined(HAS_ARGBTOUVROW_SSSE3)
   if (TestCpuFlag(kCpuHasSSSE3)) {
     ARGBToUVRow = ARGBToUVRow_Any_SSSE3;
@@ -1960,6 +1976,14 @@ int RGB24ToJ420(const uint8_t* src_rgb24,
   }
 #endif
 #else
+#if defined(HAS_RGB24TOARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    RGB24ToARGBRow = RGB24ToARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      RGB24ToARGBRow = RGB24ToARGBRow_NEON;
+    }
+  }
+#endif
 #if defined(HAS_RGB24TOARGBROW_SSSE3)
   if (TestCpuFlag(kCpuHasSSSE3)) {
     RGB24ToARGBRow = RGB24ToARGBRow_Any_SSSE3;
@@ -1968,6 +1992,18 @@ int RGB24ToJ420(const uint8_t* src_rgb24,
     }
   }
 #endif
+#if defined(HAS_ARGBTOYJROW_NEON) && defined(HAS_ARGBTOUVJROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    ARGBToUVJRow = ARGBToUVJRow_Any_NEON;
+    ARGBToYJRow = ARGBToYJRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToYJRow = ARGBToYJRow_NEON;
+      if (IS_ALIGNED(width, 16)) {
+        ARGBToUVJRow = ARGBToUVJRow_NEON;
+      }
+    }
+  }
+#endif
 #if defined(HAS_ARGBTOYJROW_SSSE3) && defined(HAS_ARGBTOUVJROW_SSSE3)
   if (TestCpuFlag(kCpuHasSSSE3)) {
     ARGBToUVJRow = ARGBToUVJRow_Any_SSSE3;
@@ -2111,6 +2147,26 @@ int RAWToI420(const uint8_t* src_raw,
 #endif
 // Other platforms do intermediate conversion from RAW to ARGB.
 #else
+#if defined(HAS_RAWTOARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    RAWToARGBRow = RAWToARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      RAWToARGBRow = RAWToARGBRow_NEON;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOYROW_NEON) && defined(HAS_ARGBTOUVROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    ARGBToUVRow = ARGBToUVRow_Any_NEON;
+    ARGBToYRow = ARGBToYRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToYRow = ARGBToYRow_NEON;
+      if (IS_ALIGNED(width, 16)) {
+        ARGBToUVRow = ARGBToUVRow_NEON;
+      }
+    }
+  }
+#endif
 #if defined(HAS_RAWTOARGBROW_SSSE3)
   if (TestCpuFlag(kCpuHasSSSE3)) {
     RAWToARGBRow = RAWToARGBRow_Any_SSSE3;
@@ -2186,6 +2242,178 @@ int RAWToI420(const uint8_t* src_raw,
   return 0;
 }
 
+// TODO(fbarchard): Use Matrix version to implement I420 and J420.
+// Convert RAW to J420.
+LIBYUV_API
+int RAWToJ420(const uint8_t* src_raw,
+              int src_stride_raw,
+              uint8_t* dst_y,
+              int dst_stride_y,
+              uint8_t* dst_u,
+              int dst_stride_u,
+              uint8_t* dst_v,
+              int dst_stride_v,
+              int width,
+              int height) {
+  int y;
+#if (defined(HAS_RAWTOYJROW_NEON) && defined(HAS_RAWTOUVJROW_NEON)) || \
+    defined(HAS_RAWTOYJROW_MSA) || defined(HAS_RAWTOYJROW_MMI)
+  void (*RAWToUVJRow)(const uint8_t* src_raw, int src_stride_raw,
+                      uint8_t* dst_u, uint8_t* dst_v, int width) =
+      RAWToUVJRow_C;
+  void (*RAWToYJRow)(const uint8_t* src_raw, uint8_t* dst_y, int width) =
+      RAWToYJRow_C;
+#else
+  void (*RAWToARGBRow)(const uint8_t* src_rgb, uint8_t* dst_argb, int width) =
+      RAWToARGBRow_C;
+  void (*ARGBToUVJRow)(const uint8_t* src_argb0, int src_stride_argb,
+                       uint8_t* dst_u, uint8_t* dst_v, int width) =
+      ARGBToUVJRow_C;
+  void (*ARGBToYJRow)(const uint8_t* src_argb, uint8_t* dst_y, int width) =
+      ARGBToYJRow_C;
+#endif
+  if (!src_raw || !dst_y || !dst_u || !dst_v || width <= 0 || height == 0) {
+    return -1;
+  }
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    src_raw = src_raw + (height - 1) * src_stride_raw;
+    src_stride_raw = -src_stride_raw;
+  }
+
+// Neon version does direct RAW to YUV.
+#if defined(HAS_RAWTOYJROW_NEON) && defined(HAS_RAWTOUVJROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    RAWToUVJRow = RAWToUVJRow_Any_NEON;
+    RAWToYJRow = RAWToYJRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      RAWToYJRow = RAWToYJRow_NEON;
+      if (IS_ALIGNED(width, 16)) {
+        RAWToUVJRow = RAWToUVJRow_NEON;
+      }
+    }
+  }
+// MMI and MSA version does direct RAW to YUV.
+#elif (defined(HAS_RAWTOYJROW_MMI) || defined(HAS_RAWTOYJROW_MSA))
+#if defined(HAS_RAWTOYJROW_MMI) && defined(HAS_RAWTOUVJROW_MMI)
+  if (TestCpuFlag(kCpuHasMMI)) {
+    RAWToUVJRow = RAWToUVJRow_Any_MMI;
+    RAWToYJRow = RAWToYJRow_Any_MMI;
+    if (IS_ALIGNED(width, 8)) {
+      RAWToYJRow = RAWToYJRow_MMI;
+      if (IS_ALIGNED(width, 16)) {
+        RAWToUVJRow = RAWToUVJRow_MMI;
+      }
+    }
+  }
+#endif
+#if defined(HAS_RAWTOYJROW_MSA) && defined(HAS_RAWTOUVJROW_MSA)
+  if (TestCpuFlag(kCpuHasMSA)) {
+    RAWToUVJRow = RAWToUVJRow_Any_MSA;
+    RAWToYJRow = RAWToYJRow_Any_MSA;
+    if (IS_ALIGNED(width, 16)) {
+      RAWToYJRow = RAWToYJRow_MSA;
+      RAWToUVJRow = RAWToUVJRow_MSA;
+    }
+  }
+#endif
+#else
+#if defined(HAS_RAWTOARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    RAWToARGBRow = RAWToARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      RAWToARGBRow = RAWToARGBRow_NEON;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOYJROW_NEON) && defined(HAS_ARGBTOUVJROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    ARGBToUVJRow = ARGBToUVJRow_Any_NEON;
+    ARGBToYJRow = ARGBToYJRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToYJRow = ARGBToYJRow_NEON;
+      if (IS_ALIGNED(width, 16)) {
+        ARGBToUVJRow = ARGBToUVJRow_NEON;
+      }
+    }
+  }
+#endif
+#if defined(HAS_RAWTOARGBROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    RAWToARGBRow = RAWToARGBRow_Any_SSSE3;
+    if (IS_ALIGNED(width, 16)) {
+      RAWToARGBRow = RAWToARGBRow_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOYJROW_SSSE3) && defined(HAS_ARGBTOUVJROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    ARGBToUVJRow = ARGBToUVJRow_Any_SSSE3;
+    ARGBToYJRow = ARGBToYJRow_Any_SSSE3;
+    if (IS_ALIGNED(width, 16)) {
+      ARGBToUVJRow = ARGBToUVJRow_SSSE3;
+      ARGBToYJRow = ARGBToYJRow_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOYJROW_AVX2) && defined(HAS_ARGBTOUVJROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    ARGBToUVJRow = ARGBToUVJRow_Any_AVX2;
+    ARGBToYJRow = ARGBToYJRow_Any_AVX2;
+    if (IS_ALIGNED(width, 32)) {
+      ARGBToUVJRow = ARGBToUVJRow_AVX2;
+      ARGBToYJRow = ARGBToYJRow_AVX2;
+    }
+  }
+#endif
+#endif
+
+  {
+#if !((defined(HAS_RAWTOYJROW_NEON) && defined(HAS_RAWTOUVJROW_NEON)) || \
+      defined(HAS_RAWTOYJROW_MSA) || defined(HAS_RAWTOYJROW_MMI))
+    // Allocate 2 rows of ARGB.
+    const int kRowSize = (width * 4 + 31) & ~31;
+    align_buffer_64(row, kRowSize * 2);
+#endif
+
+    for (y = 0; y < height - 1; y += 2) {
+#if ((defined(HAS_RAWTOYJROW_NEON) && defined(HAS_RAWTOUVJROW_NEON)) || \
+     defined(HAS_RAWTOYJROW_MSA) || defined(HAS_RAWTOYJROW_MMI))
+      RAWToUVJRow(src_raw, src_stride_raw, dst_u, dst_v, width);
+      RAWToYJRow(src_raw, dst_y, width);
+      RAWToYJRow(src_raw + src_stride_raw, dst_y + dst_stride_y, width);
+#else
+      RAWToARGBRow(src_raw, row, width);
+      RAWToARGBRow(src_raw + src_stride_raw, row + kRowSize, width);
+      ARGBToUVJRow(row, kRowSize, dst_u, dst_v, width);
+      ARGBToYJRow(row, dst_y, width);
+      ARGBToYJRow(row + kRowSize, dst_y + dst_stride_y, width);
+#endif
+      src_raw += src_stride_raw * 2;
+      dst_y += dst_stride_y * 2;
+      dst_u += dst_stride_u;
+      dst_v += dst_stride_v;
+    }
+    if (height & 1) {
+#if ((defined(HAS_RAWTOYJROW_NEON) && defined(HAS_RAWTOUVJROW_NEON)) || \
+     defined(HAS_RAWTOYJROW_MSA) || defined(HAS_RAWTOYJROW_MMI))
+      RAWToUVJRow(src_raw, 0, dst_u, dst_v, width);
+      RAWToYJRow(src_raw, dst_y, width);
+#else
+      RAWToARGBRow(src_raw, row, width);
+      ARGBToUVJRow(row, 0, dst_u, dst_v, width);
+      ARGBToYJRow(row, dst_y, width);
+#endif
+    }
+#if !((defined(HAS_RAWTOYJROW_NEON) && defined(HAS_RAWTOUVJROW_NEON)) || \
+      defined(HAS_RAWTOYJROW_MSA) || defined(HAS_RAWTOYJROW_MMI))
+    free_aligned_buffer_64(row);
+#endif
+  }
+  return 0;
+}
+
 // Convert RGB565 to I420.
 LIBYUV_API
 int RGB565ToI420(const uint8_t* src_rgb565,
diff --git a/source/convert_argb.cc b/source/convert_argb.cc
index 2b3d52d2..d8f7b277 100644
--- a/source/convert_argb.cc
+++ b/source/convert_argb.cc
@@ -2791,6 +2791,10 @@ static const uvec8 kShuffleMaskABGRToARGB = {
 static const uvec8 kShuffleMaskRGBAToARGB = {
     1u, 2u, 3u, 0u, 5u, 6u, 7u, 4u, 9u, 10u, 11u, 8u, 13u, 14u, 15u, 12u};
 
+// Shuffle table for converting AR64 to AB64.
+static const uvec8 kShuffleMaskAR64ToAB64 = {
+    4u, 5u, 2u, 3u, 0u, 1u, 6u, 7u, 12u, 13u, 10u, 11u, 8u, 9u, 14u, 15u};
+
 // Convert BGRA to ARGB.
 LIBYUV_API
 int BGRAToARGB(const uint8_t* src_bgra,
@@ -2800,7 +2804,7 @@ int BGRAToARGB(const uint8_t* src_bgra,
                int width,
                int height) {
   return ARGBShuffle(src_bgra, src_stride_bgra, dst_argb, dst_stride_argb,
-                     (const uint8_t*)(&kShuffleMaskBGRAToARGB), width, height);
+                     (const uint8_t*)&kShuffleMaskBGRAToARGB, width, height);
 }
 
 // Convert ARGB to BGRA (same as BGRAToARGB).
@@ -2812,7 +2816,7 @@ int ARGBToBGRA(const uint8_t* src_bgra,
                int width,
                int height) {
   return ARGBShuffle(src_bgra, src_stride_bgra, dst_argb, dst_stride_argb,
-                     (const uint8_t*)(&kShuffleMaskBGRAToARGB), width, height);
+                     (const uint8_t*)&kShuffleMaskBGRAToARGB, width, height);
 }
 
 // Convert ABGR to ARGB.
@@ -2824,7 +2828,7 @@ int ABGRToARGB(const uint8_t* src_abgr,
                int width,
                int height) {
   return ARGBShuffle(src_abgr, src_stride_abgr, dst_argb, dst_stride_argb,
-                     (const uint8_t*)(&kShuffleMaskABGRToARGB), width, height);
+                     (const uint8_t*)&kShuffleMaskABGRToARGB, width, height);
 }
 
 // Convert ARGB to ABGR to (same as ABGRToARGB).
@@ -2836,7 +2840,7 @@ int ARGBToABGR(const uint8_t* src_abgr,
                int width,
                int height) {
   return ARGBShuffle(src_abgr, src_stride_abgr, dst_argb, dst_stride_argb,
-                     (const uint8_t*)(&kShuffleMaskABGRToARGB), width, height);
+                     (const uint8_t*)&kShuffleMaskABGRToARGB, width, height);
 }
 
 // Convert RGBA to ARGB.
@@ -2848,7 +2852,19 @@ int RGBAToARGB(const uint8_t* src_rgba,
                int width,
                int height) {
   return ARGBShuffle(src_rgba, src_stride_rgba, dst_argb, dst_stride_argb,
-                     (const uint8_t*)(&kShuffleMaskRGBAToARGB), width, height);
+                     (const uint8_t*)&kShuffleMaskRGBAToARGB, width, height);
+}
+
+// Convert AR64 To AB64.
+LIBYUV_API
+int AR64ToAB64(const uint16_t* src_ar64,
+               int src_stride_ar64,
+               uint16_t* dst_ab64,
+               int dst_stride_ab64,
+               int width,
+               int height) {
+  return AR64Shuffle(src_ar64, src_stride_ar64, dst_ab64, dst_stride_ab64,
+                     (const uint8_t*)&kShuffleMaskAR64ToAB64, width, height);
 }
 
 // Convert RGB24 to ARGB.
@@ -3357,6 +3373,124 @@ int AR30ToAB30(const uint8_t* src_ar30,
   return 0;
 }
 
+// Convert AR64 to ARGB.
+LIBYUV_API
+int AR64ToARGB(const uint16_t* src_ar64,
+               int src_stride_ar64,
+               uint8_t* dst_argb,
+               int dst_stride_argb,
+               int width,
+               int height) {
+  int y;
+  void (*AR64ToARGBRow)(const uint16_t* src_ar64, uint8_t* dst_argb,
+                        int width) = AR64ToARGBRow_C;
+  if (!src_ar64 || !dst_argb || width <= 0 || height == 0) {
+    return -1;
+  }
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    src_ar64 = src_ar64 + (height - 1) * src_stride_ar64;
+    src_stride_ar64 = -src_stride_ar64;
+  }
+  // Coalesce rows.
+  if (src_stride_ar64 == width * 4 && dst_stride_argb == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_ar64 = dst_stride_argb = 0;
+  }
+#if defined(HAS_AR64TOARGBROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    AR64ToARGBRow = AR64ToARGBRow_Any_SSSE3;
+    if (IS_ALIGNED(width, 4)) {
+      AR64ToARGBRow = AR64ToARGBRow_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_AR64TOARGBROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    AR64ToARGBRow = AR64ToARGBRow_Any_AVX2;
+    if (IS_ALIGNED(width, 8)) {
+      AR64ToARGBRow = AR64ToARGBRow_AVX2;
+    }
+  }
+#endif
+#if defined(HAS_AR64TOARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    AR64ToARGBRow = AR64ToARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      AR64ToARGBRow = AR64ToARGBRow_NEON;
+    }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    AR64ToARGBRow(src_ar64, dst_argb, width);
+    src_ar64 += src_stride_ar64;
+    dst_argb += dst_stride_argb;
+  }
+  return 0;
+}
+
+// Convert AB64 to ARGB.
+LIBYUV_API
+int AB64ToARGB(const uint16_t* src_ab64,
+               int src_stride_ab64,
+               uint8_t* dst_argb,
+               int dst_stride_argb,
+               int width,
+               int height) {
+  int y;
+  void (*AB64ToARGBRow)(const uint16_t* src_ar64, uint8_t* dst_argb,
+                        int width) = AB64ToARGBRow_C;
+  if (!src_ab64 || !dst_argb || width <= 0 || height == 0) {
+    return -1;
+  }
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    src_ab64 = src_ab64 + (height - 1) * src_stride_ab64;
+    src_stride_ab64 = -src_stride_ab64;
+  }
+  // Coalesce rows.
+  if (src_stride_ab64 == width * 4 && dst_stride_argb == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_ab64 = dst_stride_argb = 0;
+  }
+#if defined(HAS_AB64TOARGBROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    AB64ToARGBRow = AB64ToARGBRow_Any_SSSE3;
+    if (IS_ALIGNED(width, 4)) {
+      AB64ToARGBRow = AB64ToARGBRow_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_AB64TOARGBROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    AB64ToARGBRow = AB64ToARGBRow_Any_AVX2;
+    if (IS_ALIGNED(width, 8)) {
+      AB64ToARGBRow = AB64ToARGBRow_AVX2;
+    }
+  }
+#endif
+#if defined(HAS_AB64TOARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    AB64ToARGBRow = AB64ToARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      AB64ToARGBRow = AB64ToARGBRow_NEON;
+    }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    AB64ToARGBRow(src_ab64, dst_argb, width);
+    src_ab64 += src_stride_ab64;
+    dst_argb += dst_stride_argb;
+  }
+  return 0;
+}
+
 // Convert NV12 to ARGB with matrix.
 LIBYUV_API
 int NV12ToARGBMatrix(const uint8_t* src_y,
@@ -5176,6 +5310,40 @@ int H420ToAR30(const uint8_t* src_y,
                           &kYvuH709Constants, width, height);
 }
 
+// Convert I420 to AB30.
+LIBYUV_API
+int I420ToAB30(const uint8_t* src_y,
+               int src_stride_y,
+               const uint8_t* src_u,
+               int src_stride_u,
+               const uint8_t* src_v,
+               int src_stride_v,
+               uint8_t* dst_ab30,
+               int dst_stride_ab30,
+               int width,
+               int height) {
+  return I420ToAR30Matrix(src_y, src_stride_y, src_v, src_stride_v, src_u,
+                          src_stride_u, dst_ab30, dst_stride_ab30,
+                          &kYvuI601Constants, width, height);
+}
+
+// Convert H420 to AB30.
+LIBYUV_API
+int H420ToAB30(const uint8_t* src_y,
+               int src_stride_y,
+               const uint8_t* src_u,
+               int src_stride_u,
+               const uint8_t* src_v,
+               int src_stride_v,
+               uint8_t* dst_ab30,
+               int dst_stride_ab30,
+               int width,
+               int height) {
+  return I420ToAR30Matrix(src_y, src_stride_y, src_v, src_stride_v, src_u,
+                          src_stride_u, dst_ab30, dst_stride_ab30,
+                          &kYvuH709Constants, width, height);
+}
+
 #ifdef __cplusplus
 }  // extern "C"
 }  // namespace libyuv
diff --git a/source/convert_from_argb.cc b/source/convert_from_argb.cc
index 4ba4bb5e..8285b6c3 100644
--- a/source/convert_from_argb.cc
+++ b/source/convert_from_argb.cc
@@ -1298,7 +1298,7 @@ int ARGBToRGB24(const uint8_t* src_argb,
 #if defined(HAS_ARGBTORGB24ROW_NEON)
   if (TestCpuFlag(kCpuHasNEON)) {
     ARGBToRGB24Row = ARGBToRGB24Row_Any_NEON;
-    if (IS_ALIGNED(width, 8)) {
+    if (IS_ALIGNED(width, 16)) {
       ARGBToRGB24Row = ARGBToRGB24Row_NEON;
     }
   }
@@ -2009,6 +2009,124 @@ int ARGBToJ422(const uint8_t* src_argb,
   return 0;
 }
 
+// Convert ARGB to AR64.
+LIBYUV_API
+int ARGBToAR64(const uint8_t* src_argb,
+               int src_stride_argb,
+               uint16_t* dst_ar64,
+               int dst_stride_ar64,
+               int width,
+               int height) {
+  int y;
+  void (*ARGBToAR64Row)(const uint8_t* src_argb, uint16_t* dst_ar64,
+                        int width) = ARGBToAR64Row_C;
+  if (!src_argb || !dst_ar64 || width <= 0 || height == 0) {
+    return -1;
+  }
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    src_argb = src_argb + (height - 1) * src_stride_argb;
+    src_stride_argb = -src_stride_argb;
+  }
+  // Coalesce rows.
+  if (src_stride_argb == width * 4 && dst_stride_ar64 == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_argb = dst_stride_ar64 = 0;
+  }
+#if defined(HAS_ARGBTOAR64ROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    ARGBToAR64Row = ARGBToAR64Row_Any_SSSE3;
+    if (IS_ALIGNED(width, 4)) {
+      ARGBToAR64Row = ARGBToAR64Row_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOAR64ROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    ARGBToAR64Row = ARGBToAR64Row_Any_AVX2;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToAR64Row = ARGBToAR64Row_AVX2;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOAR64ROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    ARGBToAR64Row = ARGBToAR64Row_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToAR64Row = ARGBToAR64Row_NEON;
+    }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    ARGBToAR64Row(src_argb, dst_ar64, width);
+    src_argb += src_stride_argb;
+    dst_ar64 += dst_stride_ar64;
+  }
+  return 0;
+}
+
+// Convert ARGB to AB64.
+LIBYUV_API
+int ARGBToAB64(const uint8_t* src_argb,
+               int src_stride_argb,
+               uint16_t* dst_ab64,
+               int dst_stride_ab64,
+               int width,
+               int height) {
+  int y;
+  void (*ARGBToAB64Row)(const uint8_t* src_argb, uint16_t* dst_ar64,
+                        int width) = ARGBToAB64Row_C;
+  if (!src_argb || !dst_ab64 || width <= 0 || height == 0) {
+    return -1;
+  }
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    src_argb = src_argb + (height - 1) * src_stride_argb;
+    src_stride_argb = -src_stride_argb;
+  }
+  // Coalesce rows.
+  if (src_stride_argb == width * 4 && dst_stride_ab64 == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_argb = dst_stride_ab64 = 0;
+  }
+#if defined(HAS_ARGBTOAB64ROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    ARGBToAB64Row = ARGBToAB64Row_Any_SSSE3;
+    if (IS_ALIGNED(width, 4)) {
+      ARGBToAB64Row = ARGBToAB64Row_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOAB64ROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    ARGBToAB64Row = ARGBToAB64Row_Any_AVX2;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToAB64Row = ARGBToAB64Row_AVX2;
+    }
+  }
+#endif
+#if defined(HAS_ARGBTOAB64ROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    ARGBToAB64Row = ARGBToAB64Row_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      ARGBToAB64Row = ARGBToAB64Row_NEON;
+    }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    ARGBToAB64Row(src_argb, dst_ab64, width);
+    src_argb += src_stride_argb;
+    dst_ab64 += dst_stride_ab64;
+  }
+  return 0;
+}
+
 // Convert ARGB to J400.
 LIBYUV_API
 int ARGBToJ400(const uint8_t* src_argb,
diff --git a/source/planar_functions.cc b/source/planar_functions.cc
index 219c2165..7cea06c8 100644
--- a/source/planar_functions.cc
+++ b/source/planar_functions.cc
@@ -10,6 +10,7 @@
 
 #include "libyuv/planar_functions.h"
 
+#include <assert.h>
 #include <string.h>  // for memset()
 
 #include "libyuv/cpu_id.h"
@@ -563,9 +564,9 @@ void SplitUVPlane_16(const uint16_t* src_uv,
                      int height,
                      int depth) {
   int y;
-  int scale = 1 << depth;
-  void (*SplitUVRow)(const uint16_t* src_uv, uint16_t* dst_u, uint16_t* dst_v,
-                     int scale, int width) = SplitUVRow_16_C;
+  void (*SplitUVRow_16)(const uint16_t* src_uv, uint16_t* dst_u,
+                        uint16_t* dst_v, int depth, int width) =
+      SplitUVRow_16_C;
   // Negative height means invert the image.
   if (height < 0) {
     height = -height;
@@ -583,24 +584,24 @@ void SplitUVPlane_16(const uint16_t* src_uv,
   }
 #if defined(HAS_SPLITUVROW_16_AVX2)
   if (TestCpuFlag(kCpuHasAVX2)) {
-    SplitUVRow = SplitUVRow_16_Any_AVX2;
+    SplitUVRow_16 = SplitUVRow_16_Any_AVX2;
     if (IS_ALIGNED(width, 16)) {
-      SplitUVRow = SplitUVRow_16_AVX2;
+      SplitUVRow_16 = SplitUVRow_16_AVX2;
     }
   }
 #endif
 #if defined(HAS_SPLITUVROW_16_NEON)
   if (TestCpuFlag(kCpuHasNEON)) {
-    SplitUVRow = SplitUVRow_16_Any_NEON;
+    SplitUVRow_16 = SplitUVRow_16_Any_NEON;
     if (IS_ALIGNED(width, 8)) {
-      SplitUVRow = SplitUVRow_16_NEON;
+      SplitUVRow_16 = SplitUVRow_16_NEON;
     }
   }
 #endif
 
   for (y = 0; y < height; ++y) {
     // Copy a row of UV.
-    SplitUVRow(src_uv, dst_u, dst_v, scale, width);
+    SplitUVRow_16(src_uv, dst_u, dst_v, depth, width);
     dst_u += dst_stride_u;
     dst_v += dst_stride_v;
     src_uv += src_stride_uv;
@@ -618,9 +619,11 @@ void MergeUVPlane_16(const uint16_t* src_u,
                      int height,
                      int depth) {
   int y;
-  int scale = 1 << (16 - depth);
-  void (*MergeUVRow)(const uint16_t* src_u, const uint16_t* src_v,
-                     uint16_t* dst_uv, int scale, int width) = MergeUVRow_16_C;
+  void (*MergeUVRow_16)(const uint16_t* src_u, const uint16_t* src_v,
+                        uint16_t* dst_uv, int depth, int width) =
+      MergeUVRow_16_C;
+  assert(depth >= 8);
+  assert(depth <= 16);
   // Negative height means invert the image.
   if (height < 0) {
     height = -height;
@@ -636,24 +639,24 @@ void MergeUVPlane_16(const uint16_t* src_u,
   }
 #if defined(HAS_MERGEUVROW_16_AVX2)
   if (TestCpuFlag(kCpuHasAVX2)) {
-    MergeUVRow = MergeUVRow_16_Any_AVX2;
+    MergeUVRow_16 = MergeUVRow_16_Any_AVX2;
     if (IS_ALIGNED(width, 16)) {
-      MergeUVRow = MergeUVRow_16_AVX2;
+      MergeUVRow_16 = MergeUVRow_16_AVX2;
     }
   }
 #endif
 #if defined(HAS_MERGEUVROW_16_NEON)
   if (TestCpuFlag(kCpuHasNEON)) {
-    MergeUVRow = MergeUVRow_16_Any_NEON;
+    MergeUVRow_16 = MergeUVRow_16_Any_NEON;
     if (IS_ALIGNED(width, 8)) {
-      MergeUVRow = MergeUVRow_16_NEON;
+      MergeUVRow_16 = MergeUVRow_16_NEON;
     }
   }
 #endif
 
   for (y = 0; y < height; ++y) {
     // Merge a row of U and V into a row of UV.
-    MergeUVRow(src_u, src_v, dst_uv, scale, width);
+    MergeUVRow_16(src_u, src_v, dst_uv, depth, width);
     src_u += src_stride_u;
     src_v += src_stride_v;
     dst_uv += dst_stride_uv;
@@ -671,8 +674,8 @@ void ConvertToMSBPlane_16(const uint16_t* src_y,
                           int depth) {
   int y;
   int scale = 1 << (16 - depth);
-  void (*MultiplyRow)(const uint16_t* src_y, uint16_t* dst_y, int scale,
-                      int width) = MultiplyRow_16_C;
+  void (*MultiplyRow_16)(const uint16_t* src_y, uint16_t* dst_y, int scale,
+                         int width) = MultiplyRow_16_C;
   // Negative height means invert the image.
   if (height < 0) {
     height = -height;
@@ -688,23 +691,23 @@ void ConvertToMSBPlane_16(const uint16_t* src_y,
 
 #if defined(HAS_MULTIPLYROW_16_AVX2)
   if (TestCpuFlag(kCpuHasAVX2)) {
-    MultiplyRow = MultiplyRow_16_Any_AVX2;
+    MultiplyRow_16 = MultiplyRow_16_Any_AVX2;
     if (IS_ALIGNED(width, 32)) {
-      MultiplyRow = MultiplyRow_16_AVX2;
+      MultiplyRow_16 = MultiplyRow_16_AVX2;
     }
   }
 #endif
 #if defined(HAS_MULTIPLYROW_16_NEON)
   if (TestCpuFlag(kCpuHasNEON)) {
-    MultiplyRow = MultiplyRow_16_Any_NEON;
+    MultiplyRow_16 = MultiplyRow_16_Any_NEON;
     if (IS_ALIGNED(width, 16)) {
-      MultiplyRow = MultiplyRow_16_NEON;
+      MultiplyRow_16 = MultiplyRow_16_NEON;
     }
   }
 #endif
 
   for (y = 0; y < height; ++y) {
-    MultiplyRow(src_y, dst_y, scale, width);
+    MultiplyRow_16(src_y, dst_y, scale, width);
     src_y += src_stride_y;
     dst_y += dst_stride_y;
   }
@@ -982,6 +985,142 @@ void MergeRGBPlane(const uint8_t* src_r,
   }
 }
 
+LIBYUV_NOINLINE
+void SplitARGBPlaneAlpha(const uint8_t* src_argb,
+                         int src_stride_argb,
+                         uint8_t* dst_r,
+                         int dst_stride_r,
+                         uint8_t* dst_g,
+                         int dst_stride_g,
+                         uint8_t* dst_b,
+                         int dst_stride_b,
+                         uint8_t* dst_a,
+                         int dst_stride_a,
+                         int width,
+                         int height) {
+  int y;
+  void (*SplitARGBRow)(const uint8_t* src_rgb, uint8_t* dst_r, uint8_t* dst_g,
+                       uint8_t* dst_b, uint8_t* dst_a, int width) =
+      SplitARGBRow_C;
+
+  assert(height > 0);
+
+  if (src_stride_argb == width * 4 && dst_stride_r == width &&
+      dst_stride_g == width && dst_stride_b == width && dst_stride_a == width) {
+    width *= height;
+    height = 1;
+    src_stride_argb = dst_stride_r = dst_stride_g = dst_stride_b =
+        dst_stride_a = 0;
+  }
+
+#if defined(HAS_SPLITARGBROW_SSE2)
+  if (TestCpuFlag(kCpuHasSSE2)) {
+    SplitARGBRow = SplitARGBRow_Any_SSE2;
+    if (IS_ALIGNED(width, 8)) {
+      SplitARGBRow = SplitARGBRow_SSE2;
+    }
+  }
+#endif
+#if defined(HAS_SPLITARGBROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    SplitARGBRow = SplitARGBRow_Any_SSSE3;
+    if (IS_ALIGNED(width, 8)) {
+      SplitARGBRow = SplitARGBRow_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_SPLITARGBROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    SplitARGBRow = SplitARGBRow_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      SplitARGBRow = SplitARGBRow_AVX2;
+    }
+  }
+#endif
+#if defined(HAS_SPLITARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    SplitARGBRow = SplitARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 16)) {
+      SplitARGBRow = SplitARGBRow_NEON;
+    }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    SplitARGBRow(src_argb, dst_r, dst_g, dst_b, dst_a, width);
+    dst_r += dst_stride_r;
+    dst_g += dst_stride_g;
+    dst_b += dst_stride_b;
+    dst_a += dst_stride_a;
+    src_argb += src_stride_argb;
+  }
+}
+
+LIBYUV_NOINLINE
+void SplitARGBPlaneOpaque(const uint8_t* src_argb,
+                          int src_stride_argb,
+                          uint8_t* dst_r,
+                          int dst_stride_r,
+                          uint8_t* dst_g,
+                          int dst_stride_g,
+                          uint8_t* dst_b,
+                          int dst_stride_b,
+                          int width,
+                          int height) {
+  int y;
+  void (*SplitXRGBRow)(const uint8_t* src_rgb, uint8_t* dst_r, uint8_t* dst_g,
+                       uint8_t* dst_b, int width) = SplitXRGBRow_C;
+  assert(height > 0);
+
+  if (src_stride_argb == width * 4 && dst_stride_r == width &&
+      dst_stride_g == width && dst_stride_b == width) {
+    width *= height;
+    height = 1;
+    src_stride_argb = dst_stride_r = dst_stride_g = dst_stride_b = 0;
+  }
+
+#if defined(HAS_SPLITXRGBROW_SSE2)
+  if (TestCpuFlag(kCpuHasSSE2)) {
+    SplitXRGBRow = SplitXRGBRow_Any_SSE2;
+    if (IS_ALIGNED(width, 8)) {
+      SplitXRGBRow = SplitXRGBRow_SSE2;
+    }
+  }
+#endif
+#if defined(HAS_SPLITXRGBROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    SplitXRGBRow = SplitXRGBRow_Any_SSSE3;
+    if (IS_ALIGNED(width, 8)) {
+      SplitXRGBRow = SplitXRGBRow_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_SPLITXRGBROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    SplitXRGBRow = SplitXRGBRow_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      SplitXRGBRow = SplitXRGBRow_AVX2;
+    }
+  }
+#endif
+#if defined(HAS_SPLITXRGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    SplitXRGBRow = SplitXRGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 16)) {
+      SplitXRGBRow = SplitXRGBRow_NEON;
+    }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    SplitXRGBRow(src_argb, dst_r, dst_g, dst_b, width);
+    dst_r += dst_stride_r;
+    dst_g += dst_stride_g;
+    dst_b += dst_stride_b;
+    src_argb += src_stride_argb;
+  }
+}
+
 LIBYUV_API
 void SplitARGBPlane(const uint8_t* src_argb,
                     int src_stride_argb,
@@ -995,137 +1134,146 @@ void SplitARGBPlane(const uint8_t* src_argb,
                     int dst_stride_a,
                     int width,
                     int height) {
-  int y;
-  void (*SplitARGBRow)(const uint8_t* src_rgb, uint8_t* dst_r, uint8_t* dst_g,
-                       uint8_t* dst_b, uint8_t* dst_a, int width) =
-      SplitARGBRow_C;
-  void (*SplitXRGBRow)(const uint8_t* src_rgb, uint8_t* dst_r, uint8_t* dst_g,
-                       uint8_t* dst_b, int width) = SplitXRGBRow_C;
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    dst_r = dst_r + (height - 1) * dst_stride_r;
+    dst_g = dst_g + (height - 1) * dst_stride_g;
+    dst_b = dst_b + (height - 1) * dst_stride_b;
+    dst_a = dst_a + (height - 1) * dst_stride_a;
+    dst_stride_r = -dst_stride_r;
+    dst_stride_g = -dst_stride_g;
+    dst_stride_b = -dst_stride_b;
+    dst_stride_a = -dst_stride_a;
+  }
 
   if (dst_a == NULL) {
-    // Negative height means invert the image.
-    if (height < 0) {
-      height = -height;
-      dst_r = dst_r + (height - 1) * dst_stride_r;
-      dst_g = dst_g + (height - 1) * dst_stride_g;
-      dst_b = dst_b + (height - 1) * dst_stride_b;
-      dst_stride_r = -dst_stride_r;
-      dst_stride_g = -dst_stride_g;
-      dst_stride_b = -dst_stride_b;
-    }
-
-    // Coalesce rows.
-    if (src_stride_argb == width * 4 && dst_stride_r == width &&
-        dst_stride_g == width && dst_stride_b == width) {
-      width *= height;
-      height = 1;
-      src_stride_argb = dst_stride_r = dst_stride_g = dst_stride_b =
-          dst_stride_a = 0;
-    }
+    SplitARGBPlaneOpaque(src_argb, src_stride_argb, dst_r, dst_stride_r, dst_g,
+                         dst_stride_g, dst_b, dst_stride_b, width, height);
+  } else {
+    SplitARGBPlaneAlpha(src_argb, src_stride_argb, dst_r, dst_stride_r, dst_g,
+                        dst_stride_g, dst_b, dst_stride_b, dst_a, dst_stride_a,
+                        width, height);
+  }
+}
 
-#if defined(HAS_SPLITARGBROW_SSE2)
-    if (TestCpuFlag(kCpuHasSSE2)) {
-      SplitXRGBRow = SplitXRGBRow_Any_SSE2;
-      if (IS_ALIGNED(width, 8)) {
-        SplitXRGBRow = SplitXRGBRow_SSE2;
-      }
-    }
-#endif
-#if defined(HAS_SPLITARGBROW_SSSE3)
-    if (TestCpuFlag(kCpuHasSSSE3)) {
-      SplitXRGBRow = SplitXRGBRow_Any_SSSE3;
-      if (IS_ALIGNED(width, 8)) {
-        SplitXRGBRow = SplitXRGBRow_SSSE3;
-      }
+LIBYUV_NOINLINE
+void MergeARGBPlaneAlpha(const uint8_t* src_r,
+                         int src_stride_r,
+                         const uint8_t* src_g,
+                         int src_stride_g,
+                         const uint8_t* src_b,
+                         int src_stride_b,
+                         const uint8_t* src_a,
+                         int src_stride_a,
+                         uint8_t* dst_argb,
+                         int dst_stride_argb,
+                         int width,
+                         int height) {
+  int y;
+  void (*MergeARGBRow)(const uint8_t* src_r, const uint8_t* src_g,
+                       const uint8_t* src_b, const uint8_t* src_a,
+                       uint8_t* dst_argb, int width) = MergeARGBRow_C;
+
+  assert(height > 0);
+
+  if (src_stride_r == width && src_stride_g == width && src_stride_b == width &&
+      src_stride_a == width && dst_stride_argb == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_r = src_stride_g = src_stride_b = src_stride_a =
+        dst_stride_argb = 0;
+  }
+#if defined(HAS_MERGEARGBROW_SSE2)
+  if (TestCpuFlag(kCpuHasSSE2)) {
+    MergeARGBRow = MergeARGBRow_Any_SSE2;
+    if (IS_ALIGNED(width, 8)) {
+      MergeARGBRow = MergeARGBRow_SSE2;
     }
+  }
 #endif
-#if defined(HAS_SPLITARGBROW_AVX2)
-    if (TestCpuFlag(kCpuHasAVX2)) {
-      SplitXRGBRow = SplitXRGBRow_Any_AVX2;
-      if (IS_ALIGNED(width, 16)) {
-        SplitXRGBRow = SplitXRGBRow_AVX2;
-      }
+#if defined(HAS_MERGEARGBROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    MergeARGBRow = MergeARGBRow_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      MergeARGBRow = MergeARGBRow_AVX2;
     }
+  }
 #endif
-#if defined(HAS_SPLITRGBROW_NEON)
-    if (TestCpuFlag(kCpuHasNEON)) {
-      SplitXRGBRow = SplitXRGBRow_Any_NEON;
-      if (IS_ALIGNED(width, 16)) {
-        SplitXRGBRow = SplitXRGBRow_NEON;
-      }
+#if defined(HAS_MERGEARGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    MergeARGBRow = MergeARGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 16)) {
+      MergeARGBRow = MergeARGBRow_NEON;
     }
+  }
 #endif
 
-    for (y = 0; y < height; ++y) {
-      SplitXRGBRow(src_argb, dst_r, dst_g, dst_b, width);
-      dst_r += dst_stride_r;
-      dst_g += dst_stride_g;
-      dst_b += dst_stride_b;
-      src_argb += src_stride_argb;
-    }
-  } else {
-    if (height < 0) {
-      height = -height;
-      dst_r = dst_r + (height - 1) * dst_stride_r;
-      dst_g = dst_g + (height - 1) * dst_stride_g;
-      dst_b = dst_b + (height - 1) * dst_stride_b;
-      dst_a = dst_a + (height - 1) * dst_stride_a;
-      dst_stride_r = -dst_stride_r;
-      dst_stride_g = -dst_stride_g;
-      dst_stride_b = -dst_stride_b;
-      dst_stride_a = -dst_stride_a;
-    }
-
-    if (src_stride_argb == width * 4 && dst_stride_r == width &&
-        dst_stride_g == width && dst_stride_b == width &&
-        dst_stride_a == width) {
-      width *= height;
-      height = 1;
-      src_stride_argb = dst_stride_r = dst_stride_g = dst_stride_b =
-          dst_stride_a = 0;
-    }
+  for (y = 0; y < height; ++y) {
+    MergeARGBRow(src_r, src_g, src_b, src_a, dst_argb, width);
+    src_r += src_stride_r;
+    src_g += src_stride_g;
+    src_b += src_stride_b;
+    src_a += src_stride_a;
+    dst_argb += dst_stride_argb;
+  }
+}
 
-#if defined(HAS_SPLITARGBROW_SSE2)
-    if (TestCpuFlag(kCpuHasSSE2)) {
-      SplitARGBRow = SplitARGBRow_Any_SSE2;
-      if (IS_ALIGNED(width, 8)) {
-        SplitARGBRow = SplitARGBRow_SSE2;
-      }
-    }
-#endif
-#if defined(HAS_SPLITARGBROW_SSSE3)
-    if (TestCpuFlag(kCpuHasSSSE3)) {
-      SplitARGBRow = SplitARGBRow_Any_SSSE3;
-      if (IS_ALIGNED(width, 8)) {
-        SplitARGBRow = SplitARGBRow_SSSE3;
-      }
+LIBYUV_NOINLINE
+void MergeARGBPlaneOpaque(const uint8_t* src_r,
+                          int src_stride_r,
+                          const uint8_t* src_g,
+                          int src_stride_g,
+                          const uint8_t* src_b,
+                          int src_stride_b,
+                          uint8_t* dst_argb,
+                          int dst_stride_argb,
+                          int width,
+                          int height) {
+  int y;
+  void (*MergeXRGBRow)(const uint8_t* src_r, const uint8_t* src_g,
+                       const uint8_t* src_b, uint8_t* dst_argb, int width) =
+      MergeXRGBRow_C;
+
+  assert(height > 0);
+
+  if (src_stride_r == width && src_stride_g == width && src_stride_b == width &&
+      dst_stride_argb == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_r = src_stride_g = src_stride_b = dst_stride_argb = 0;
+  }
+#if defined(HAS_MERGEXRGBROW_SSE2)
+  if (TestCpuFlag(kCpuHasSSE2)) {
+    MergeXRGBRow = MergeXRGBRow_Any_SSE2;
+    if (IS_ALIGNED(width, 8)) {
+      MergeXRGBRow = MergeXRGBRow_SSE2;
     }
+  }
 #endif
-#if defined(HAS_SPLITARGBROW_AVX2)
-    if (TestCpuFlag(kCpuHasAVX2)) {
-      SplitARGBRow = SplitARGBRow_Any_AVX2;
-      if (IS_ALIGNED(width, 16)) {
-        SplitARGBRow = SplitARGBRow_AVX2;
-      }
+#if defined(HAS_MERGEXRGBROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    MergeXRGBRow = MergeXRGBRow_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      MergeXRGBRow = MergeXRGBRow_AVX2;
     }
+  }
 #endif
-#if defined(HAS_SPLITRGBROW_NEON)
-    if (TestCpuFlag(kCpuHasNEON)) {
-      SplitARGBRow = SplitARGBRow_Any_NEON;
-      if (IS_ALIGNED(width, 16)) {
-        SplitARGBRow = SplitARGBRow_NEON;
-      }
+#if defined(HAS_MERGEXRGBROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    MergeXRGBRow = MergeXRGBRow_Any_NEON;
+    if (IS_ALIGNED(width, 16)) {
+      MergeXRGBRow = MergeXRGBRow_NEON;
     }
+  }
 #endif
 
-    for (y = 0; y < height; ++y) {
-      SplitARGBRow(src_argb, dst_r, dst_g, dst_b, dst_a, width);
-      dst_r += dst_stride_r;
-      dst_g += dst_stride_g;
-      dst_b += dst_stride_b;
-      dst_a += dst_stride_a;
-      src_argb += src_stride_argb;
-    }
+  for (y = 0; y < height; ++y) {
+    MergeXRGBRow(src_r, src_g, src_b, dst_argb, width);
+    src_r += src_stride_r;
+    src_g += src_stride_g;
+    src_b += src_stride_b;
+    dst_argb += dst_stride_argb;
   }
 }
 
@@ -1142,107 +1290,357 @@ void MergeARGBPlane(const uint8_t* src_r,
                     int dst_stride_argb,
                     int width,
                     int height) {
-  int y;
-  void (*MergeARGBRow)(const uint8_t* src_r, const uint8_t* src_g,
-                       const uint8_t* src_b, const uint8_t* src_a,
-                       uint8_t* dst_argb, int width) = MergeARGBRow_C;
-  void (*MergeXRGBRow)(const uint8_t* src_r, const uint8_t* src_g,
-                       const uint8_t* src_b, uint8_t* dst_argb, int width) =
-      MergeXRGBRow_C;
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    dst_argb = dst_argb + (height - 1) * dst_stride_argb;
+    dst_stride_argb = -dst_stride_argb;
+  }
 
   if (src_a == NULL) {
-    // Negative height means invert the image.
-    if (height < 0) {
-      height = -height;
-      dst_argb = dst_argb + (height - 1) * dst_stride_argb;
-      dst_stride_argb = -dst_stride_argb;
-    }
-    // Coalesce rows.
-    if (src_stride_r == width && src_stride_g == width &&
-        src_stride_b == width && dst_stride_argb == width * 4) {
-      width *= height;
-      height = 1;
-      src_stride_r = src_stride_g = src_stride_b = dst_stride_argb = 0;
+    MergeARGBPlaneOpaque(src_r, src_stride_r, src_g, src_stride_g, src_b,
+                         src_stride_b, dst_argb, dst_stride_argb, width,
+                         height);
+  } else {
+    MergeARGBPlaneAlpha(src_r, src_stride_r, src_g, src_stride_g, src_b,
+                        src_stride_b, src_a, src_stride_a, dst_argb,
+                        dst_stride_argb, width, height);
+  }
+}
+
+// TODO(yuan): Support 2 bit alpha channel.
+LIBYUV_API
+void MergeXR30Plane(const uint16_t* src_r,
+                    int src_stride_r,
+                    const uint16_t* src_g,
+                    int src_stride_g,
+                    const uint16_t* src_b,
+                    int src_stride_b,
+                    uint8_t* dst_ar30,
+                    int dst_stride_ar30,
+                    int width,
+                    int height,
+                    int depth) {
+  int y;
+  void (*MergeXR30Row)(const uint16_t* src_r, const uint16_t* src_g,
+                       const uint16_t* src_b, uint8_t* dst_ar30, int depth,
+                       int width) = MergeXR30Row_C;
+
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    dst_ar30 = dst_ar30 + (height - 1) * dst_stride_ar30;
+    dst_stride_ar30 = -dst_stride_ar30;
+  }
+  // Coalesce rows.
+  if (src_stride_r == width && src_stride_g == width && src_stride_b == width &&
+      dst_stride_ar30 == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_r = src_stride_g = src_stride_b = dst_stride_ar30 = 0;
+  }
+#if defined(HAS_MERGEXR30ROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    MergeXR30Row = MergeXR30Row_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      MergeXR30Row = MergeXR30Row_AVX2;
     }
-#if defined(HAS_MERGEARGBROW_SSE2)
-    if (TestCpuFlag(kCpuHasSSE2)) {
-      MergeXRGBRow = MergeXRGBRow_Any_SSE2;
+  }
+#endif
+#if defined(HAS_MERGEXR30ROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    if (depth == 10) {
+      MergeXR30Row = MergeXR30Row_10_Any_NEON;
+      if (IS_ALIGNED(width, 8)) {
+        MergeXR30Row = MergeXR30Row_10_NEON;
+      }
+    } else {
+      MergeXR30Row = MergeXR30Row_Any_NEON;
       if (IS_ALIGNED(width, 8)) {
-        MergeXRGBRow = MergeXRGBRow_SSE2;
+        MergeXR30Row = MergeXR30Row_NEON;
       }
     }
+  }
 #endif
-#if defined(HAS_MERGEARGBROW_AVX2)
-    if (TestCpuFlag(kCpuHasAVX2)) {
-      MergeXRGBRow = MergeXRGBRow_Any_AVX2;
-      if (IS_ALIGNED(width, 16)) {
-        MergeXRGBRow = MergeXRGBRow_AVX2;
-      }
+
+  for (y = 0; y < height; ++y) {
+    MergeXR30Row(src_r, src_g, src_b, dst_ar30, depth, width);
+    src_r += src_stride_r;
+    src_g += src_stride_g;
+    src_b += src_stride_b;
+    dst_ar30 += dst_stride_ar30;
+  }
+}
+
+LIBYUV_NOINLINE
+static void MergeAR64PlaneAlpha(const uint16_t* src_r,
+                                int src_stride_r,
+                                const uint16_t* src_g,
+                                int src_stride_g,
+                                const uint16_t* src_b,
+                                int src_stride_b,
+                                const uint16_t* src_a,
+                                int src_stride_a,
+                                uint16_t* dst_ar64,
+                                int dst_stride_ar64,
+                                int width,
+                                int height,
+                                int depth) {
+  int y;
+  void (*MergeAR64Row)(const uint16_t* src_r, const uint16_t* src_g,
+                       const uint16_t* src_b, const uint16_t* src_a,
+                       uint16_t* dst_argb, int depth, int width) =
+      MergeAR64Row_C;
+
+  if (src_stride_r == width && src_stride_g == width && src_stride_b == width &&
+      src_stride_a == width && dst_stride_ar64 == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_r = src_stride_g = src_stride_b = src_stride_a =
+        dst_stride_ar64 = 0;
+  }
+#if defined(HAS_MERGEAR64ROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    MergeAR64Row = MergeAR64Row_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      MergeAR64Row = MergeAR64Row_AVX2;
     }
+  }
 #endif
-#if defined(HAS_MERGERGBROW_NEON)
-    if (TestCpuFlag(kCpuHasNEON)) {
-      MergeXRGBRow = MergeXRGBRow_Any_NEON;
-      if (IS_ALIGNED(width, 16)) {
-        MergeXRGBRow = MergeXRGBRow_NEON;
-      }
+#if defined(HAS_MERGEAR64ROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    MergeAR64Row = MergeAR64Row_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      MergeAR64Row = MergeAR64Row_NEON;
     }
+  }
 #endif
 
-    for (y = 0; y < height; ++y) {
-      MergeXRGBRow(src_r, src_g, src_b, dst_argb, width);
-      src_r += src_stride_r;
-      src_g += src_stride_g;
-      src_b += src_stride_b;
-      dst_argb += dst_stride_argb;
+  for (y = 0; y < height; ++y) {
+    MergeAR64Row(src_r, src_g, src_b, src_a, dst_ar64, depth, width);
+    src_r += src_stride_r;
+    src_g += src_stride_g;
+    src_b += src_stride_b;
+    src_a += src_stride_a;
+    dst_ar64 += dst_stride_ar64;
+  }
+}
+
+LIBYUV_NOINLINE
+static void MergeAR64PlaneOpaque(const uint16_t* src_r,
+                                 int src_stride_r,
+                                 const uint16_t* src_g,
+                                 int src_stride_g,
+                                 const uint16_t* src_b,
+                                 int src_stride_b,
+                                 uint16_t* dst_ar64,
+                                 int dst_stride_ar64,
+                                 int width,
+                                 int height,
+                                 int depth) {
+  int y;
+  void (*MergeXR64Row)(const uint16_t* src_r, const uint16_t* src_g,
+                       const uint16_t* src_b, uint16_t* dst_argb, int depth,
+                       int width) = MergeXR64Row_C;
+
+  // Coalesce rows.
+  if (src_stride_r == width && src_stride_g == width && src_stride_b == width &&
+      dst_stride_ar64 == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_r = src_stride_g = src_stride_b = dst_stride_ar64 = 0;
+  }
+#if defined(HAS_MERGEXR64ROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    MergeXR64Row = MergeXR64Row_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      MergeXR64Row = MergeXR64Row_AVX2;
     }
-  } else {
-    if (height < 0) {
-      height = -height;
-      dst_argb = dst_argb + (height - 1) * dst_stride_argb;
-      dst_stride_argb = -dst_stride_argb;
+  }
+#endif
+#if defined(HAS_MERGEXR64ROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    MergeXR64Row = MergeXR64Row_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      MergeXR64Row = MergeXR64Row_NEON;
     }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    MergeXR64Row(src_r, src_g, src_b, dst_ar64, depth, width);
+    src_r += src_stride_r;
+    src_g += src_stride_g;
+    src_b += src_stride_b;
+    dst_ar64 += dst_stride_ar64;
+  }
+}
+
+LIBYUV_API
+void MergeAR64Plane(const uint16_t* src_r,
+                    int src_stride_r,
+                    const uint16_t* src_g,
+                    int src_stride_g,
+                    const uint16_t* src_b,
+                    int src_stride_b,
+                    const uint16_t* src_a,
+                    int src_stride_a,
+                    uint16_t* dst_ar64,
+                    int dst_stride_ar64,
+                    int width,
+                    int height,
+                    int depth) {
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    dst_ar64 = dst_ar64 + (height - 1) * dst_stride_ar64;
+    dst_stride_ar64 = -dst_stride_ar64;
+  }
+
+  if (src_a == NULL) {
+    MergeAR64PlaneOpaque(src_r, src_stride_r, src_g, src_stride_g, src_b,
+                         src_stride_b, dst_ar64, dst_stride_ar64, width, height,
+                         depth);
+  } else {
+    MergeAR64PlaneAlpha(src_r, src_stride_r, src_g, src_stride_g, src_b,
+                        src_stride_b, src_a, src_stride_a, dst_ar64,
+                        dst_stride_ar64, width, height, depth);
+  }
+}
 
-    if (src_stride_r == width && src_stride_g == width &&
-        src_stride_b == width && src_stride_a == width &&
-        dst_stride_argb == width * 4) {
-      width *= height;
-      height = 1;
-      src_stride_r = src_stride_g = src_stride_b = src_stride_a =
-          dst_stride_argb = 0;
+LIBYUV_NOINLINE
+static void MergeARGB16To8PlaneAlpha(const uint16_t* src_r,
+                                     int src_stride_r,
+                                     const uint16_t* src_g,
+                                     int src_stride_g,
+                                     const uint16_t* src_b,
+                                     int src_stride_b,
+                                     const uint16_t* src_a,
+                                     int src_stride_a,
+                                     uint8_t* dst_argb,
+                                     int dst_stride_argb,
+                                     int width,
+                                     int height,
+                                     int depth) {
+  int y;
+  void (*MergeARGB16To8Row)(const uint16_t* src_r, const uint16_t* src_g,
+                            const uint16_t* src_b, const uint16_t* src_a,
+                            uint8_t* dst_argb, int depth, int width) =
+      MergeARGB16To8Row_C;
+
+  if (src_stride_r == width && src_stride_g == width && src_stride_b == width &&
+      src_stride_a == width && dst_stride_argb == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_r = src_stride_g = src_stride_b = src_stride_a =
+        dst_stride_argb = 0;
+  }
+#if defined(HAS_MERGEARGB16TO8ROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    MergeARGB16To8Row = MergeARGB16To8Row_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      MergeARGB16To8Row = MergeARGB16To8Row_AVX2;
     }
-#if defined(HAS_MERGEARGBROW_SSE2)
-    if (TestCpuFlag(kCpuHasSSE2)) {
-      MergeARGBRow = MergeARGBRow_Any_SSE2;
-      if (IS_ALIGNED(width, 8)) {
-        MergeARGBRow = MergeARGBRow_SSE2;
-      }
+  }
+#endif
+#if defined(HAS_MERGEARGB16TO8ROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    MergeARGB16To8Row = MergeARGB16To8Row_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      MergeARGB16To8Row = MergeARGB16To8Row_NEON;
     }
+  }
 #endif
-#if defined(HAS_MERGEARGBROW_AVX2)
-    if (TestCpuFlag(kCpuHasAVX2)) {
-      MergeARGBRow = MergeARGBRow_Any_AVX2;
-      if (IS_ALIGNED(width, 16)) {
-        MergeARGBRow = MergeARGBRow_AVX2;
-      }
+
+  for (y = 0; y < height; ++y) {
+    MergeARGB16To8Row(src_r, src_g, src_b, src_a, dst_argb, depth, width);
+    src_r += src_stride_r;
+    src_g += src_stride_g;
+    src_b += src_stride_b;
+    src_a += src_stride_a;
+    dst_argb += dst_stride_argb;
+  }
+}
+
+LIBYUV_NOINLINE
+static void MergeARGB16To8PlaneOpaque(const uint16_t* src_r,
+                                      int src_stride_r,
+                                      const uint16_t* src_g,
+                                      int src_stride_g,
+                                      const uint16_t* src_b,
+                                      int src_stride_b,
+                                      uint8_t* dst_argb,
+                                      int dst_stride_argb,
+                                      int width,
+                                      int height,
+                                      int depth) {
+  int y;
+  void (*MergeXRGB16To8Row)(const uint16_t* src_r, const uint16_t* src_g,
+                            const uint16_t* src_b, uint8_t* dst_argb, int depth,
+                            int width) = MergeXRGB16To8Row_C;
+
+  // Coalesce rows.
+  if (src_stride_r == width && src_stride_g == width && src_stride_b == width &&
+      dst_stride_argb == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_r = src_stride_g = src_stride_b = dst_stride_argb = 0;
+  }
+#if defined(HAS_MERGEXRGB16TO8ROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    MergeXRGB16To8Row = MergeXRGB16To8Row_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      MergeXRGB16To8Row = MergeXRGB16To8Row_AVX2;
     }
+  }
 #endif
-#if defined(HAS_MERGERGBROW_NEON)
-    if (TestCpuFlag(kCpuHasNEON)) {
-      MergeARGBRow = MergeARGBRow_Any_NEON;
-      if (IS_ALIGNED(width, 16)) {
-        MergeARGBRow = MergeARGBRow_NEON;
-      }
+#if defined(HAS_MERGEXRGB16TO8ROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    MergeXRGB16To8Row = MergeXRGB16To8Row_Any_NEON;
+    if (IS_ALIGNED(width, 8)) {
+      MergeXRGB16To8Row = MergeXRGB16To8Row_NEON;
     }
+  }
 #endif
 
-    for (y = 0; y < height; ++y) {
-      MergeARGBRow(src_r, src_g, src_b, src_a, dst_argb, width);
-      src_r += src_stride_r;
-      src_g += src_stride_g;
-      src_b += src_stride_b;
-      dst_argb += dst_stride_argb;
-    }
+  for (y = 0; y < height; ++y) {
+    MergeXRGB16To8Row(src_r, src_g, src_b, dst_argb, depth, width);
+    src_r += src_stride_r;
+    src_g += src_stride_g;
+    src_b += src_stride_b;
+    dst_argb += dst_stride_argb;
+  }
+}
+
+LIBYUV_API
+void MergeARGB16To8Plane(const uint16_t* src_r,
+                         int src_stride_r,
+                         const uint16_t* src_g,
+                         int src_stride_g,
+                         const uint16_t* src_b,
+                         int src_stride_b,
+                         const uint16_t* src_a,
+                         int src_stride_a,
+                         uint8_t* dst_argb,
+                         int dst_stride_argb,
+                         int width,
+                         int height,
+                         int depth) {
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    dst_argb = dst_argb + (height - 1) * dst_stride_argb;
+    dst_stride_argb = -dst_stride_argb;
+  }
+
+  if (src_a == NULL) {
+    MergeARGB16To8PlaneOpaque(src_r, src_stride_r, src_g, src_stride_g, src_b,
+                              src_stride_b, dst_argb, dst_stride_argb, width,
+                              height, depth);
+  } else {
+    MergeARGB16To8PlaneAlpha(src_r, src_stride_r, src_g, src_stride_g, src_b,
+                             src_stride_b, src_a, src_stride_a, dst_argb,
+                             dst_stride_argb, width, height, depth);
   }
 }
 
@@ -2244,12 +2642,12 @@ int ARGBAdd(const uint8_t* src_argb0,
     height = 1;
     src_stride_argb0 = src_stride_argb1 = dst_stride_argb = 0;
   }
-#if defined(HAS_ARGBADDROW_SSE2) && (defined(_MSC_VER) && !defined(__clang__))
+#if defined(HAS_ARGBADDROW_SSE2)
   if (TestCpuFlag(kCpuHasSSE2)) {
     ARGBAddRow = ARGBAddRow_SSE2;
   }
 #endif
-#if defined(HAS_ARGBADDROW_SSE2) && !(defined(_MSC_VER) && !defined(__clang__))
+#if defined(HAS_ARGBADDROW_SSE2)
   if (TestCpuFlag(kCpuHasSSE2)) {
     ARGBAddRow = ARGBAddRow_Any_SSE2;
     if (IS_ALIGNED(width, 4)) {
@@ -3527,6 +3925,76 @@ int ARGBShuffle(const uint8_t* src_bgra,
   return 0;
 }
 
+// Shuffle AR64 channel order.  e.g. AR64 to AB64.
+LIBYUV_API
+int AR64Shuffle(const uint16_t* src_ar64,
+                int src_stride_ar64,
+                uint16_t* dst_ar64,
+                int dst_stride_ar64,
+                const uint8_t* shuffler,
+                int width,
+                int height) {
+  int y;
+  void (*AR64ShuffleRow)(const uint8_t* src_ar64, uint8_t* dst_ar64,
+                         const uint8_t* shuffler, int width) = AR64ShuffleRow_C;
+  if (!src_ar64 || !dst_ar64 || width <= 0 || height == 0) {
+    return -1;
+  }
+  // Negative height means invert the image.
+  if (height < 0) {
+    height = -height;
+    src_ar64 = src_ar64 + (height - 1) * src_stride_ar64;
+    src_stride_ar64 = -src_stride_ar64;
+  }
+  // Coalesce rows.
+  if (src_stride_ar64 == width * 4 && dst_stride_ar64 == width * 4) {
+    width *= height;
+    height = 1;
+    src_stride_ar64 = dst_stride_ar64 = 0;
+  }
+  // Assembly versions can be reused if it's implemented with shuffle.
+#if defined(HAS_ARGBSHUFFLEROW_SSSE3)
+  if (TestCpuFlag(kCpuHasSSSE3)) {
+    AR64ShuffleRow = ARGBShuffleRow_Any_SSSE3;
+    if (IS_ALIGNED(width, 8)) {
+      AR64ShuffleRow = ARGBShuffleRow_SSSE3;
+    }
+  }
+#endif
+#if defined(HAS_ARGBSHUFFLEROW_AVX2)
+  if (TestCpuFlag(kCpuHasAVX2)) {
+    AR64ShuffleRow = ARGBShuffleRow_Any_AVX2;
+    if (IS_ALIGNED(width, 16)) {
+      AR64ShuffleRow = ARGBShuffleRow_AVX2;
+    }
+  }
+#endif
+#if defined(HAS_ARGBSHUFFLEROW_NEON)
+  if (TestCpuFlag(kCpuHasNEON)) {
+    AR64ShuffleRow = ARGBShuffleRow_Any_NEON;
+    if (IS_ALIGNED(width, 4)) {
+      AR64ShuffleRow = ARGBShuffleRow_NEON;
+    }
+  }
+#endif
+#if defined(HAS_ARGBSHUFFLEROW_MMI)
+  if (TestCpuFlag(kCpuHasMMI)) {
+    AR64ShuffleRow = ARGBShuffleRow_Any_MMI;
+    if (IS_ALIGNED(width, 2)) {
+      AR64ShuffleRow = ARGBShuffleRow_MMI;
+    }
+  }
+#endif
+
+  for (y = 0; y < height; ++y) {
+    AR64ShuffleRow((uint8_t*)(src_ar64), (uint8_t*)(dst_ar64), shuffler,
+                   width * 2);
+    src_ar64 += src_stride_ar64;
+    dst_ar64 += dst_stride_ar64;
+  }
+  return 0;
+}
+
 // Gauss blur a float plane using Gaussian 5x5 filter with
 // coefficients of 1, 4, 6, 4, 1.
 // Each destination pixel is a blur of the 5x5
diff --git a/source/rotate_gcc.cc b/source/rotate_gcc.cc
index fd359d4a..8401d4f0 100644
--- a/source/rotate_gcc.cc
+++ b/source/rotate_gcc.cc
@@ -18,7 +18,7 @@ extern "C" {
 
 // This module is for GCC x86 and x64.
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 
 // Transpose 8x8. 32 or 64 bit, but not NaCL for 64 bit.
 #if defined(HAS_TRANSPOSEWX8_SSSE3)
diff --git a/source/rotate_win.cc b/source/rotate_win.cc
index e887dd52..a78873f8 100644
--- a/source/rotate_win.cc
+++ b/source/rotate_win.cc
@@ -16,8 +16,9 @@ namespace libyuv {
 extern "C" {
 #endif
 
-// This module is for 32 bit Visual C x86 and clangcl
-#if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && defined(_MSC_VER)
+// This module is for 32 bit Visual C x86
+#if !defined(LIBYUV_DISABLE_X86) && defined(_MSC_VER) && \
+    !defined(__clang__) && defined(_M_IX86)
 
 __declspec(naked) void TransposeWx8_SSSE3(const uint8_t* src,
                                           int src_stride,
diff --git a/source/row_any.cc b/source/row_any.cc
index f68d2ed6..5b113fb4 100644
--- a/source/row_any.cc
+++ b/source/row_any.cc
@@ -61,6 +61,8 @@ ANY41(MergeARGBRow_Any_AVX2, MergeARGBRow_AVX2, 0, 0, 4, 15)
 ANY41(MergeARGBRow_Any_NEON, MergeARGBRow_NEON, 0, 0, 4, 15)
 #endif
 
+// Note that odd width replication includes 444 due to implementation
+// on arm that subsamples 444 to 422 internally.
 // Any 4 planes to 1 with yuvconstants
 #define ANY41C(NAMEANY, ANY_SIMD, UVSHIFT, DUVSHIFT, BPP, MASK)              \
   void NAMEANY(const uint8_t* y_buf, const uint8_t* u_buf,                   \
@@ -77,6 +79,10 @@ ANY41(MergeARGBRow_Any_NEON, MergeARGBRow_NEON, 0, 0, 4, 15)
     memcpy(temp + 64, u_buf + (n >> UVSHIFT), SS(r, UVSHIFT));               \
     memcpy(temp + 128, v_buf + (n >> UVSHIFT), SS(r, UVSHIFT));              \
     memcpy(temp + 192, a_buf + n, r);                                        \
+    if (width & 1) {                                                         \
+      temp[64 + SS(r, UVSHIFT)] = temp[64 + SS(r, UVSHIFT) - 1];             \
+      temp[128 + SS(r, UVSHIFT)] = temp[128 + SS(r, UVSHIFT) - 1];           \
+    }                                                                        \
     ANY_SIMD(temp, temp + 64, temp + 128, temp + 192, temp + 256,            \
              yuvconstants, MASK + 1);                                        \
     memcpy(dst_ptr + (n >> DUVSHIFT) * BPP, temp + 256,                      \
@@ -183,6 +189,56 @@ ANY41CT(I410AlphaToARGBRow_Any_AVX2,
 
 #undef ANY41CT
 
+// Any 4 planes to 1 plane with parameter
+#define ANY41PT(NAMEANY, ANY_SIMD, STYPE, SBPP, DTYPE, BPP, MASK)          \
+  void NAMEANY(const STYPE* r_buf, const STYPE* g_buf, const STYPE* b_buf, \
+               const STYPE* a_buf, DTYPE* dst_ptr, int depth, int width) { \
+    SIMD_ALIGNED(STYPE temp[16 * 4]);                                      \
+    SIMD_ALIGNED(DTYPE out[64]);                                           \
+    memset(temp, 0, 16 * 4 * SBPP); /* for YUY2 and msan */                \
+    int r = width & MASK;                                                  \
+    int n = width & ~MASK;                                                 \
+    if (n > 0) {                                                           \
+      ANY_SIMD(r_buf, g_buf, b_buf, a_buf, dst_ptr, depth, n);             \
+    }                                                                      \
+    memcpy(temp, r_buf + n, r * SBPP);                                     \
+    memcpy(temp + 16, g_buf + n, r * SBPP);                                \
+    memcpy(temp + 32, b_buf + n, r * SBPP);                                \
+    memcpy(temp + 48, a_buf + n, r * SBPP);                                \
+    ANY_SIMD(temp, temp + 16, temp + 32, temp + 48, out, depth, MASK + 1); \
+    memcpy((uint8_t*)dst_ptr + n * BPP, out, r * BPP);                     \
+  }
+
+#ifdef HAS_MERGEAR64ROW_AVX2
+ANY41PT(MergeAR64Row_Any_AVX2, MergeAR64Row_AVX2, uint16_t, 2, uint16_t, 8, 15)
+#endif
+
+#ifdef HAS_MERGEAR64ROW_NEON
+ANY41PT(MergeAR64Row_Any_NEON, MergeAR64Row_NEON, uint16_t, 2, uint16_t, 8, 7)
+#endif
+
+#ifdef HAS_MERGEARGB16TO8ROW_AVX2
+ANY41PT(MergeARGB16To8Row_Any_AVX2,
+        MergeARGB16To8Row_AVX2,
+        uint16_t,
+        2,
+        uint8_t,
+        4,
+        15)
+#endif
+
+#ifdef HAS_MERGEARGB16TO8ROW_NEON
+ANY41PT(MergeARGB16To8Row_Any_NEON,
+        MergeARGB16To8Row_NEON,
+        uint16_t,
+        2,
+        uint8_t,
+        4,
+        7)
+#endif
+
+#undef ANY41PT
+
 // Any 3 planes to 1.
 #define ANY31(NAMEANY, ANY_SIMD, UVSHIFT, DUVSHIFT, BPP, MASK)      \
   void NAMEANY(const uint8_t* y_buf, const uint8_t* u_buf,          \
@@ -212,13 +268,13 @@ ANY31(MergeRGBRow_Any_NEON, MergeRGBRow_NEON, 0, 0, 3, 15)
 #ifdef HAS_MERGERGBROW_MMI
 ANY31(MergeRGBRow_Any_MMI, MergeRGBRow_MMI, 0, 0, 3, 7)
 #endif
-#ifdef HAS_MERGEARGBROW_SSE2
+#ifdef HAS_MERGEXRGBROW_SSE2
 ANY31(MergeXRGBRow_Any_SSE2, MergeXRGBRow_SSE2, 0, 0, 4, 7)
 #endif
-#ifdef HAS_MERGEARGBROW_AVX2
+#ifdef HAS_MERGEXRGBROW_AVX2
 ANY31(MergeXRGBRow_Any_AVX2, MergeXRGBRow_AVX2, 0, 0, 4, 15)
 #endif
-#ifdef HAS_MERGEARGBROW_NEON
+#ifdef HAS_MERGEXRGBROW_NEON
 ANY31(MergeXRGBRow_Any_NEON, MergeXRGBRow_NEON, 0, 0, 4, 15)
 #endif
 #ifdef HAS_I422TOYUY2ROW_SSE2
@@ -424,6 +480,70 @@ ANY31CT(I212ToAR30Row_Any_AVX2, I212ToAR30Row_AVX2, 1, 0, uint16_t, 2, 4, 15)
 #endif
 #undef ANY31CT
 
+// Any 3 planes to 1 plane with parameter
+#define ANY31PT(NAMEANY, ANY_SIMD, STYPE, SBPP, DTYPE, BPP, MASK)          \
+  void NAMEANY(const STYPE* r_buf, const STYPE* g_buf, const STYPE* b_buf, \
+               DTYPE* dst_ptr, int depth, int width) {                     \
+    SIMD_ALIGNED(STYPE temp[16 * 3]);                                      \
+    SIMD_ALIGNED(DTYPE out[64]);                                           \
+    memset(temp, 0, 16 * 3 * SBPP); /* for YUY2 and msan */                \
+    int r = width & MASK;                                                  \
+    int n = width & ~MASK;                                                 \
+    if (n > 0) {                                                           \
+      ANY_SIMD(r_buf, g_buf, b_buf, dst_ptr, depth, n);                    \
+    }                                                                      \
+    memcpy(temp, r_buf + n, r * SBPP);                                     \
+    memcpy(temp + 16, g_buf + n, r * SBPP);                                \
+    memcpy(temp + 32, b_buf + n, r * SBPP);                                \
+    ANY_SIMD(temp, temp + 16, temp + 32, out, depth, MASK + 1);            \
+    memcpy((uint8_t*)dst_ptr + n * BPP, out, r * BPP);                     \
+  }
+
+#ifdef HAS_MERGEXR30ROW_AVX2
+ANY31PT(MergeXR30Row_Any_AVX2, MergeXR30Row_AVX2, uint16_t, 2, uint8_t, 4, 15)
+#endif
+
+#ifdef HAS_MERGEXR30ROW_NEON
+ANY31PT(MergeXR30Row_Any_NEON, MergeXR30Row_NEON, uint16_t, 2, uint8_t, 4, 3)
+ANY31PT(MergeXR30Row_10_Any_NEON,
+        MergeXR30Row_10_NEON,
+        uint16_t,
+        2,
+        uint8_t,
+        4,
+        3)
+#endif
+
+#ifdef HAS_MERGEXR64ROW_AVX2
+ANY31PT(MergeXR64Row_Any_AVX2, MergeXR64Row_AVX2, uint16_t, 2, uint16_t, 8, 15)
+#endif
+
+#ifdef HAS_MERGEXR64ROW_NEON
+ANY31PT(MergeXR64Row_Any_NEON, MergeXR64Row_NEON, uint16_t, 2, uint16_t, 8, 7)
+#endif
+
+#ifdef HAS_MERGEXRGB16TO8ROW_AVX2
+ANY31PT(MergeXRGB16To8Row_Any_AVX2,
+        MergeXRGB16To8Row_AVX2,
+        uint16_t,
+        2,
+        uint8_t,
+        4,
+        15)
+#endif
+
+#ifdef HAS_MERGEXRGB16TO8ROW_NEON
+ANY31PT(MergeXRGB16To8Row_Any_NEON,
+        MergeXRGB16To8Row_NEON,
+        uint16_t,
+        2,
+        uint8_t,
+        4,
+        7)
+#endif
+
+#undef ANY31PT
+
 // Any 2 planes to 1.
 #define ANY21(NAMEANY, ANY_SIMD, UVSHIFT, SBPP, SBPP2, BPP, MASK)             \
   void NAMEANY(const uint8_t* y_buf, const uint8_t* uv_buf, uint8_t* dst_ptr, \
@@ -797,7 +917,7 @@ ANY11(ARGB1555ToARGBRow_Any_AVX2, ARGB1555ToARGBRow_AVX2, 0, 2, 4, 15)
 ANY11(ARGB4444ToARGBRow_Any_AVX2, ARGB4444ToARGBRow_AVX2, 0, 2, 4, 15)
 #endif
 #if defined(HAS_ARGBTORGB24ROW_NEON)
-ANY11(ARGBToRGB24Row_Any_NEON, ARGBToRGB24Row_NEON, 0, 4, 3, 7)
+ANY11(ARGBToRGB24Row_Any_NEON, ARGBToRGB24Row_NEON, 0, 4, 3, 15)
 ANY11(ARGBToRAWRow_Any_NEON, ARGBToRAWRow_NEON, 0, 4, 3, 7)
 ANY11(ARGBToRGB565Row_Any_NEON, ARGBToRGB565Row_NEON, 0, 4, 2, 7)
 ANY11(ARGBToARGB1555Row_Any_NEON, ARGBToARGB1555Row_NEON, 0, 4, 2, 7)
@@ -1237,6 +1357,72 @@ ANY11P(ARGBShuffleRow_Any_MMI, ARGBShuffleRow_MMI, const uint8_t*, 4, 4, 1)
 #undef ANY11P
 #undef ANY11P
 
+// Any 1 to 1 with type
+#define ANY11T(NAMEANY, ANY_SIMD, SBPP, BPP, STYPE, DTYPE, MASK)  \
+  void NAMEANY(const STYPE* src_ptr, DTYPE* dst_ptr, int width) { \
+    SIMD_ALIGNED(uint8_t temp[(MASK + 1) * SBPP]);                \
+    SIMD_ALIGNED(uint8_t out[(MASK + 1) * BPP]);                  \
+    memset(temp, 0, (MASK + 1) * SBPP); /* for msan */            \
+    int r = width & MASK;                                         \
+    int n = width & ~MASK;                                        \
+    if (n > 0) {                                                  \
+      ANY_SIMD(src_ptr, dst_ptr, n);                              \
+    }                                                             \
+    memcpy(temp, (uint8_t*)(src_ptr) + n * SBPP, r * SBPP);       \
+    ANY_SIMD((STYPE*)temp, (DTYPE*)out, MASK + 1);                \
+    memcpy((uint8_t*)(dst_ptr) + n * BPP, out, r * BPP);          \
+  }
+
+#ifdef HAS_ARGBTOAR64ROW_SSSE3
+ANY11T(ARGBToAR64Row_Any_SSSE3, ARGBToAR64Row_SSSE3, 4, 8, uint8_t, uint16_t, 3)
+#endif
+
+#ifdef HAS_ARGBTOAB64ROW_SSSE3
+ANY11T(ARGBToAB64Row_Any_SSSE3, ARGBToAB64Row_SSSE3, 4, 8, uint8_t, uint16_t, 3)
+#endif
+
+#ifdef HAS_AR64TOARGBROW_SSSE3
+ANY11T(AR64ToARGBRow_Any_SSSE3, AR64ToARGBRow_SSSE3, 8, 4, uint16_t, uint8_t, 3)
+#endif
+
+#ifdef HAS_ARGBTOAR64ROW_SSSE3
+ANY11T(AB64ToARGBRow_Any_SSSE3, AB64ToARGBRow_SSSE3, 8, 4, uint16_t, uint8_t, 3)
+#endif
+
+#ifdef HAS_ARGBTOAR64ROW_AVX2
+ANY11T(ARGBToAR64Row_Any_AVX2, ARGBToAR64Row_AVX2, 4, 8, uint8_t, uint16_t, 7)
+#endif
+
+#ifdef HAS_ARGBTOAB64ROW_AVX2
+ANY11T(ARGBToAB64Row_Any_AVX2, ARGBToAB64Row_AVX2, 4, 8, uint8_t, uint16_t, 7)
+#endif
+
+#ifdef HAS_AR64TOARGBROW_AVX2
+ANY11T(AR64ToARGBRow_Any_AVX2, AR64ToARGBRow_AVX2, 8, 4, uint16_t, uint8_t, 7)
+#endif
+
+#ifdef HAS_ARGBTOAR64ROW_AVX2
+ANY11T(AB64ToARGBRow_Any_AVX2, AB64ToARGBRow_AVX2, 8, 4, uint16_t, uint8_t, 7)
+#endif
+
+#ifdef HAS_ARGBTOAR64ROW_NEON
+ANY11T(ARGBToAR64Row_Any_NEON, ARGBToAR64Row_NEON, 4, 8, uint8_t, uint16_t, 7)
+#endif
+
+#ifdef HAS_ARGBTOAB64ROW_NEON
+ANY11T(ARGBToAB64Row_Any_NEON, ARGBToAB64Row_NEON, 4, 8, uint8_t, uint16_t, 7)
+#endif
+
+#ifdef HAS_AR64TOARGBROW_NEON
+ANY11T(AR64ToARGBRow_Any_NEON, AR64ToARGBRow_NEON, 8, 4, uint16_t, uint8_t, 7)
+#endif
+
+#ifdef HAS_ARGBTOAR64ROW_NEON
+ANY11T(AB64ToARGBRow_Any_NEON, AB64ToARGBRow_NEON, 8, 4, uint16_t, uint8_t, 7)
+#endif
+
+#undef ANY11T
+
 // Any 1 to 1 with parameter and shorts.  BPP measures in shorts.
 #define ANY11C(NAMEANY, ANY_SIMD, SBPP, BPP, STYPE, DTYPE, MASK)             \
   void NAMEANY(const STYPE* src_ptr, DTYPE* dst_ptr, int scale, int width) { \
@@ -1403,38 +1589,38 @@ ANY11C(UYVYToARGBRow_Any_MMI, UYVYToARGBRow_MMI, 1, 4, 4, 7)
 #undef ANY11C
 
 // Any 1 to 1 interpolate.  Takes 2 rows of source via stride.
-#define ANY11T(NAMEANY, ANY_SIMD, SBPP, BPP, MASK)                           \
-  void NAMEANY(uint8_t* dst_ptr, const uint8_t* src_ptr,                     \
-               ptrdiff_t src_stride_ptr, int width, int source_y_fraction) { \
-    SIMD_ALIGNED(uint8_t temp[64 * 3]);                                      \
-    memset(temp, 0, 64 * 2); /* for msan */                                  \
-    int r = width & MASK;                                                    \
-    int n = width & ~MASK;                                                   \
-    if (n > 0) {                                                             \
-      ANY_SIMD(dst_ptr, src_ptr, src_stride_ptr, n, source_y_fraction);      \
-    }                                                                        \
-    memcpy(temp, src_ptr + n * SBPP, r * SBPP);                              \
-    memcpy(temp + 64, src_ptr + src_stride_ptr + n * SBPP, r * SBPP);        \
-    ANY_SIMD(temp + 128, temp, 64, MASK + 1, source_y_fraction);             \
-    memcpy(dst_ptr + n * BPP, temp + 128, r * BPP);                          \
+#define ANY11I(NAMEANY, ANY_SIMD, SBPP, BPP, MASK)                             \
+  void NAMEANY(uint8_t* dst_ptr, const uint8_t* src_ptr, ptrdiff_t src_stride, \
+               int width, int source_y_fraction) {                             \
+    SIMD_ALIGNED(uint8_t temp[64 * 3]);                                        \
+    memset(temp, 0, 64 * 2); /* for msan */                                    \
+    int r = width & MASK;                                                      \
+    int n = width & ~MASK;                                                     \
+    if (n > 0) {                                                               \
+      ANY_SIMD(dst_ptr, src_ptr, src_stride, n, source_y_fraction);            \
+    }                                                                          \
+    memcpy(temp, src_ptr + n * SBPP, r * SBPP);                                \
+    memcpy(temp + 64, src_ptr + src_stride + n * SBPP, r * SBPP);              \
+    ANY_SIMD(temp + 128, temp, 64, MASK + 1, source_y_fraction);               \
+    memcpy(dst_ptr + n * BPP, temp + 128, r * BPP);                            \
   }
 
 #ifdef HAS_INTERPOLATEROW_AVX2
-ANY11T(InterpolateRow_Any_AVX2, InterpolateRow_AVX2, 1, 1, 31)
+ANY11I(InterpolateRow_Any_AVX2, InterpolateRow_AVX2, 1, 1, 31)
 #endif
 #ifdef HAS_INTERPOLATEROW_SSSE3
-ANY11T(InterpolateRow_Any_SSSE3, InterpolateRow_SSSE3, 1, 1, 15)
+ANY11I(InterpolateRow_Any_SSSE3, InterpolateRow_SSSE3, 1, 1, 15)
 #endif
 #ifdef HAS_INTERPOLATEROW_NEON
-ANY11T(InterpolateRow_Any_NEON, InterpolateRow_NEON, 1, 1, 15)
+ANY11I(InterpolateRow_Any_NEON, InterpolateRow_NEON, 1, 1, 15)
 #endif
 #ifdef HAS_INTERPOLATEROW_MSA
-ANY11T(InterpolateRow_Any_MSA, InterpolateRow_MSA, 1, 1, 31)
+ANY11I(InterpolateRow_Any_MSA, InterpolateRow_MSA, 1, 1, 31)
 #endif
 #ifdef HAS_INTERPOLATEROW_MMI
-ANY11T(InterpolateRow_Any_MMI, InterpolateRow_MMI, 1, 1, 7)
+ANY11I(InterpolateRow_Any_MMI, InterpolateRow_MMI, 1, 1, 7)
 #endif
-#undef ANY11T
+#undef ANY11I
 
 // Any 1 to 1 mirror.
 #define ANY11M(NAMEANY, ANY_SIMD, BPP, MASK)                              \
@@ -1645,16 +1831,16 @@ ANY13(SplitRGBRow_Any_NEON, SplitRGBRow_NEON, 3, 15)
 #ifdef HAS_SPLITRGBROW_MMI
 ANY13(SplitRGBRow_Any_MMI, SplitRGBRow_MMI, 3, 3)
 #endif
-#ifdef HAS_SPLITARGBROW_SSE2
+#ifdef HAS_SPLITXRGBROW_SSE2
 ANY13(SplitXRGBRow_Any_SSE2, SplitXRGBRow_SSE2, 4, 7)
 #endif
-#ifdef HAS_SPLITARGBROW_SSSE3
+#ifdef HAS_SPLITXRGBROW_SSSE3
 ANY13(SplitXRGBRow_Any_SSSE3, SplitXRGBRow_SSSE3, 4, 7)
 #endif
-#ifdef HAS_SPLITARGBROW_AVX2
+#ifdef HAS_SPLITXRGBROW_AVX2
 ANY13(SplitXRGBRow_Any_AVX2, SplitXRGBRow_AVX2, 4, 15)
 #endif
-#ifdef HAS_SPLITARGBROW_NEON
+#ifdef HAS_SPLITXRGBROW_NEON
 ANY13(SplitXRGBRow_Any_NEON, SplitXRGBRow_NEON, 4, 15)
 #endif
 
@@ -1694,17 +1880,17 @@ ANY14(SplitARGBRow_Any_NEON, SplitARGBRow_NEON, 4, 15)
 // Any 1 to 2 with source stride (2 rows of source).  Outputs UV planes.
 // 128 byte row allows for 32 avx ARGB pixels.
 #define ANY12S(NAMEANY, ANY_SIMD, UVSHIFT, BPP, MASK)                        \
-  void NAMEANY(const uint8_t* src_ptr, int src_stride_ptr, uint8_t* dst_u,   \
+  void NAMEANY(const uint8_t* src_ptr, int src_stride, uint8_t* dst_u,       \
                uint8_t* dst_v, int width) {                                  \
     SIMD_ALIGNED(uint8_t temp[128 * 4]);                                     \
     memset(temp, 0, 128 * 2); /* for msan */                                 \
     int r = width & MASK;                                                    \
     int n = width & ~MASK;                                                   \
     if (n > 0) {                                                             \
-      ANY_SIMD(src_ptr, src_stride_ptr, dst_u, dst_v, n);                    \
+      ANY_SIMD(src_ptr, src_stride, dst_u, dst_v, n);                        \
     }                                                                        \
     memcpy(temp, src_ptr + (n >> UVSHIFT) * BPP, SS(r, UVSHIFT) * BPP);      \
-    memcpy(temp + 128, src_ptr + src_stride_ptr + (n >> UVSHIFT) * BPP,      \
+    memcpy(temp + 128, src_ptr + src_stride + (n >> UVSHIFT) * BPP,          \
            SS(r, UVSHIFT) * BPP);                                            \
     if ((width & 1) && UVSHIFT == 0) { /* repeat last pixel for subsample */ \
       memcpy(temp + SS(r, UVSHIFT) * BPP, temp + SS(r, UVSHIFT) * BPP - BPP, \
@@ -1851,17 +2037,17 @@ ANY12S(UYVYToUVRow_Any_MMI, UYVYToUVRow_MMI, 1, 4, 15)
 // Any 1 to 1 with source stride (2 rows of source).  Outputs UV plane.
 // 128 byte row allows for 32 avx ARGB pixels.
 #define ANY11S(NAMEANY, ANY_SIMD, UVSHIFT, BPP, MASK)                        \
-  void NAMEANY(const uint8_t* src_ptr, int src_stride_ptr, uint8_t* dst_vu,  \
+  void NAMEANY(const uint8_t* src_ptr, int src_stride, uint8_t* dst_vu,      \
                int width) {                                                  \
     SIMD_ALIGNED(uint8_t temp[128 * 3]);                                     \
     memset(temp, 0, 128 * 2); /* for msan */                                 \
     int r = width & MASK;                                                    \
     int n = width & ~MASK;                                                   \
     if (n > 0) {                                                             \
-      ANY_SIMD(src_ptr, src_stride_ptr, dst_vu, n);                          \
+      ANY_SIMD(src_ptr, src_stride, dst_vu, n);                              \
     }                                                                        \
     memcpy(temp, src_ptr + (n >> UVSHIFT) * BPP, SS(r, UVSHIFT) * BPP);      \
-    memcpy(temp + 128, src_ptr + src_stride_ptr + (n >> UVSHIFT) * BPP,      \
+    memcpy(temp + 128, src_ptr + src_stride + (n >> UVSHIFT) * BPP,          \
            SS(r, UVSHIFT) * BPP);                                            \
     if ((width & 1) && UVSHIFT == 0) { /* repeat last pixel for subsample */ \
       memcpy(temp + SS(r, UVSHIFT) * BPP, temp + SS(r, UVSHIFT) * BPP - BPP, \
diff --git a/source/row_common.cc b/source/row_common.cc
index 9a5543c4..4d0dce2d 100644
--- a/source/row_common.cc
+++ b/source/row_common.cc
@@ -10,6 +10,7 @@
 
 #include "libyuv/row.h"
 
+#include <assert.h>
 #include <stdio.h>
 #include <string.h>  // For memcpy and memset.
 
@@ -21,15 +22,14 @@ namespace libyuv {
 extern "C" {
 #endif
 
-// These 2 macros control YUV to RGB using unsigned math to extend range.
-// They can be used separately to enable new code and old data (clamped)
+// This macro control YUV to RGB using unsigned math to extend range of
+// YUV to RGB coefficients to 0 to 4 instead of 0 to 2 for more accuracy on B:
 // LIBYUV_UNLIMITED_DATA
-// LIBYUV_UNLIMITED_CODE
 
-// The following ifdef from row_win makes the C code match the row_win code,
-// which is 7 bit fixed point.
+// The following macro from row_win makes the C code match the row_win code,
+// which is 7 bit fixed point for ARGBToI420:
 #if !defined(LIBYUV_DISABLE_X86) && defined(_MSC_VER) && \
-    (defined(_M_IX86) || (defined(_M_X64) && !defined(__clang__)))
+    !defined(__clang__) && (defined(_M_IX86) || defined(_M_X64))
 #define LIBYUV_RGB7 1
 #endif
 
@@ -55,6 +55,11 @@ static __inline int32_t clamp1023(int32_t v) {
   return (-(v >= 1023) | v) & 1023;
 }
 
+// clamp to max
+static __inline int32_t ClampMax(int32_t v, int32_t max) {
+  return (-(v >= max) | v) & max;
+}
+
 static __inline uint32_t Abs(int32_t v) {
   int m = -(v < 0);
   return (v + m) ^ m;
@@ -72,6 +77,10 @@ static __inline int32_t clamp1023(int32_t v) {
   return (v > 1023) ? 1023 : v;
 }
 
+static __inline int32_t ClampMax(int32_t v, int32_t max) {
+  return (v > max) ? max : v;
+}
+
 static __inline uint32_t Abs(int32_t v) {
   return (v < 0) ? -v : v;
 }
@@ -418,6 +427,82 @@ void ARGBToAR30Row_C(const uint8_t* src_argb, uint8_t* dst_ar30, int width) {
   }
 }
 
+void ARGBToAR64Row_C(const uint8_t* src_argb, uint16_t* dst_ar64, int width) {
+  int x;
+  for (x = 0; x < width; ++x) {
+    dst_ar64[0] = src_argb[0] * 0x0101;
+    dst_ar64[1] = src_argb[1] * 0x0101;
+    dst_ar64[2] = src_argb[2] * 0x0101;
+    dst_ar64[3] = src_argb[3] * 0x0101;
+    dst_ar64 += 4;
+    src_argb += 4;
+  }
+}
+
+void ARGBToAB64Row_C(const uint8_t* src_argb, uint16_t* dst_ab64, int width) {
+  int x;
+  for (x = 0; x < width; ++x) {
+    dst_ab64[0] = src_argb[2] * 0x0101;
+    dst_ab64[1] = src_argb[1] * 0x0101;
+    dst_ab64[2] = src_argb[0] * 0x0101;
+    dst_ab64[3] = src_argb[3] * 0x0101;
+    dst_ab64 += 4;
+    src_argb += 4;
+  }
+}
+
+void AR64ToARGBRow_C(const uint16_t* src_ar64, uint8_t* dst_argb, int width) {
+  int x;
+  for (x = 0; x < width; ++x) {
+    dst_argb[0] = src_ar64[0] >> 8;
+    dst_argb[1] = src_ar64[1] >> 8;
+    dst_argb[2] = src_ar64[2] >> 8;
+    dst_argb[3] = src_ar64[3] >> 8;
+    dst_argb += 4;
+    src_ar64 += 4;
+  }
+}
+
+void AB64ToARGBRow_C(const uint16_t* src_ab64, uint8_t* dst_argb, int width) {
+  int x;
+  for (x = 0; x < width; ++x) {
+    dst_argb[0] = src_ab64[2] >> 8;
+    dst_argb[1] = src_ab64[1] >> 8;
+    dst_argb[2] = src_ab64[0] >> 8;
+    dst_argb[3] = src_ab64[3] >> 8;
+    dst_argb += 4;
+    src_ab64 += 4;
+  }
+}
+
+// TODO(fbarchard): Make shuffle compatible with SIMD versions
+void AR64ShuffleRow_C(const uint8_t* src_ar64,
+                      uint8_t* dst_ar64,
+                      const uint8_t* shuffler,
+                      int width) {
+  const uint16_t* src_ar64_16 = (const uint16_t*)src_ar64;
+  uint16_t* dst_ar64_16 = (uint16_t*)dst_ar64;
+  int index0 = shuffler[0] / 2;
+  int index1 = shuffler[2] / 2;
+  int index2 = shuffler[4] / 2;
+  int index3 = shuffler[6] / 2;
+  // Shuffle a row of AR64.
+  int x;
+  for (x = 0; x < width / 2; ++x) {
+    // To support in-place conversion.
+    uint16_t b = src_ar64_16[index0];
+    uint16_t g = src_ar64_16[index1];
+    uint16_t r = src_ar64_16[index2];
+    uint16_t a = src_ar64_16[index3];
+    dst_ar64_16[0] = b;
+    dst_ar64_16[1] = g;
+    dst_ar64_16[2] = r;
+    dst_ar64_16[3] = a;
+    src_ar64_16 += 4;
+    dst_ar64_16 += 4;
+  }
+}
+
 #ifdef LIBYUV_RGB7
 // Old 7 bit math for compatibility on unsupported platforms.
 static __inline int RGBToY(uint8_t r, uint8_t g, uint8_t b) {
@@ -467,80 +552,80 @@ static __inline int RGB2xToV(uint16_t r, uint16_t g, uint16_t b) {
 // Intel version mimic SSE/AVX which does 2 pavgb
 #if LIBYUV_ARGBTOUV_PAVGB
 
-#define MAKEROWY(NAME, R, G, B, BPP)                                         \
-  void NAME##ToYRow_C(const uint8_t* src_argb0, uint8_t* dst_y, int width) { \
-    int x;                                                                   \
-    for (x = 0; x < width; ++x) {                                            \
-      dst_y[0] = RGBToY(src_argb0[R], src_argb0[G], src_argb0[B]);           \
-      src_argb0 += BPP;                                                      \
-      dst_y += 1;                                                            \
-    }                                                                        \
-  }                                                                          \
-  void NAME##ToUVRow_C(const uint8_t* src_rgb0, int src_stride_rgb,          \
-                       uint8_t* dst_u, uint8_t* dst_v, int width) {          \
-    const uint8_t* src_rgb1 = src_rgb0 + src_stride_rgb;                     \
-    int x;                                                                   \
-    for (x = 0; x < width - 1; x += 2) {                                     \
-      uint8_t ab = AVGB(AVGB(src_rgb0[B], src_rgb1[B]),                      \
-                        AVGB(src_rgb0[B + BPP], src_rgb1[B + BPP]));         \
-      uint8_t ag = AVGB(AVGB(src_rgb0[G], src_rgb1[G]),                      \
-                        AVGB(src_rgb0[G + BPP], src_rgb1[G + BPP]));         \
-      uint8_t ar = AVGB(AVGB(src_rgb0[R], src_rgb1[R]),                      \
-                        AVGB(src_rgb0[R + BPP], src_rgb1[R + BPP]));         \
-      dst_u[0] = RGBToU(ar, ag, ab);                                         \
-      dst_v[0] = RGBToV(ar, ag, ab);                                         \
-      src_rgb0 += BPP * 2;                                                   \
-      src_rgb1 += BPP * 2;                                                   \
-      dst_u += 1;                                                            \
-      dst_v += 1;                                                            \
-    }                                                                        \
-    if (width & 1) {                                                         \
-      uint8_t ab = AVGB(src_rgb0[B], src_rgb1[B]);                           \
-      uint8_t ag = AVGB(src_rgb0[G], src_rgb1[G]);                           \
-      uint8_t ar = AVGB(src_rgb0[R], src_rgb1[R]);                           \
-      dst_u[0] = RGBToU(ar, ag, ab);                                         \
-      dst_v[0] = RGBToV(ar, ag, ab);                                         \
-    }                                                                        \
+#define MAKEROWY(NAME, R, G, B, BPP)                                       \
+  void NAME##ToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width) { \
+    int x;                                                                 \
+    for (x = 0; x < width; ++x) {                                          \
+      dst_y[0] = RGBToY(src_rgb[R], src_rgb[G], src_rgb[B]);               \
+      src_rgb += BPP;                                                      \
+      dst_y += 1;                                                          \
+    }                                                                      \
+  }                                                                        \
+  void NAME##ToUVRow_C(const uint8_t* src_rgb, int src_stride_rgb,         \
+                       uint8_t* dst_u, uint8_t* dst_v, int width) {        \
+    const uint8_t* src_rgb1 = src_rgb + src_stride_rgb;                    \
+    int x;                                                                 \
+    for (x = 0; x < width - 1; x += 2) {                                   \
+      uint8_t ab = AVGB(AVGB(src_rgb[B], src_rgb1[B]),                     \
+                        AVGB(src_rgb[B + BPP], src_rgb1[B + BPP]));        \
+      uint8_t ag = AVGB(AVGB(src_rgb[G], src_rgb1[G]),                     \
+                        AVGB(src_rgb[G + BPP], src_rgb1[G + BPP]));        \
+      uint8_t ar = AVGB(AVGB(src_rgb[R], src_rgb1[R]),                     \
+                        AVGB(src_rgb[R + BPP], src_rgb1[R + BPP]));        \
+      dst_u[0] = RGBToU(ar, ag, ab);                                       \
+      dst_v[0] = RGBToV(ar, ag, ab);                                       \
+      src_rgb += BPP * 2;                                                  \
+      src_rgb1 += BPP * 2;                                                 \
+      dst_u += 1;                                                          \
+      dst_v += 1;                                                          \
+    }                                                                      \
+    if (width & 1) {                                                       \
+      uint8_t ab = AVGB(src_rgb[B], src_rgb1[B]);                          \
+      uint8_t ag = AVGB(src_rgb[G], src_rgb1[G]);                          \
+      uint8_t ar = AVGB(src_rgb[R], src_rgb1[R]);                          \
+      dst_u[0] = RGBToU(ar, ag, ab);                                       \
+      dst_v[0] = RGBToV(ar, ag, ab);                                       \
+    }                                                                      \
   }
 #else
 // ARM version does sum / 2 then multiply by 2x smaller coefficients
-#define MAKEROWY(NAME, R, G, B, BPP)                                         \
-  void NAME##ToYRow_C(const uint8_t* src_argb0, uint8_t* dst_y, int width) { \
-    int x;                                                                   \
-    for (x = 0; x < width; ++x) {                                            \
-      dst_y[0] = RGBToY(src_argb0[R], src_argb0[G], src_argb0[B]);           \
-      src_argb0 += BPP;                                                      \
-      dst_y += 1;                                                            \
-    }                                                                        \
-  }                                                                          \
-  void NAME##ToUVRow_C(const uint8_t* src_rgb0, int src_stride_rgb,          \
-                       uint8_t* dst_u, uint8_t* dst_v, int width) {          \
-    const uint8_t* src_rgb1 = src_rgb0 + src_stride_rgb;                     \
-    int x;                                                                   \
-    for (x = 0; x < width - 1; x += 2) {                                     \
-      uint16_t ab = (src_rgb0[B] + src_rgb0[B + BPP] + src_rgb1[B] +         \
-                     src_rgb1[B + BPP] + 1) >>                               \
-                    1;                                                       \
-      uint16_t ag = (src_rgb0[G] + src_rgb0[G + BPP] + src_rgb1[G] +         \
-                     src_rgb1[G + BPP] + 1) >>                               \
-                    1;                                                       \
-      uint16_t ar = (src_rgb0[R] + src_rgb0[R + BPP] + src_rgb1[R] +         \
-                     src_rgb1[R + BPP] + 1) >>                               \
-                    1;                                                       \
-      dst_u[0] = RGB2xToU(ar, ag, ab);                                       \
-      dst_v[0] = RGB2xToV(ar, ag, ab);                                       \
-      src_rgb0 += BPP * 2;                                                   \
-      src_rgb1 += BPP * 2;                                                   \
-      dst_u += 1;                                                            \
-      dst_v += 1;                                                            \
-    }                                                                        \
-    if (width & 1) {                                                         \
-      uint16_t ab = src_rgb0[B] + src_rgb1[B];                               \
-      uint16_t ag = src_rgb0[G] + src_rgb1[G];                               \
-      uint16_t ar = src_rgb0[R] + src_rgb1[R];                               \
-      dst_u[0] = RGB2xToU(ar, ag, ab);                                       \
-      dst_v[0] = RGB2xToV(ar, ag, ab);                                       \
-    }                                                                        \
+#define MAKEROWY(NAME, R, G, B, BPP)                                       \
+  void NAME##ToYRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width) { \
+    int x;                                                                 \
+    for (x = 0; x < width; ++x) {                                          \
+      dst_y[0] = RGBToY(src_rgb[R], src_rgb[G], src_rgb[B]);               \
+      src_rgb += BPP;                                                      \
+      dst_y += 1;                                                          \
+    }                                                                      \
+  }                                                                        \
+  void NAME##ToUVRow_C(const uint8_t* src_rgb, int src_stride_rgb,         \
+                       uint8_t* dst_u, uint8_t* dst_v, int width) {        \
+    const uint8_t* src_rgb1 = src_rgb + src_stride_rgb;                    \
+    int x;                                                                 \
+    for (x = 0; x < width - 1; x += 2) {                                   \
+      uint16_t ab = (src_rgb[B] + src_rgb[B + BPP] + src_rgb1[B] +         \
+                     src_rgb1[B + BPP] + 1) >>                             \
+                    1;                                                     \
+      uint16_t ag = (src_rgb[G] + src_rgb[G + BPP] + src_rgb1[G] +         \
+                     src_rgb1[G + BPP] + 1) >>                             \
+                    1;                                                     \
+      uint16_t ar = (src_rgb[R] + src_rgb[R + BPP] + src_rgb1[R] +         \
+                     src_rgb1[R + BPP] + 1) >>                             \
+                    1;                                                     \
+      dst_u[0] = RGB2xToU(ar, ag, ab);                                     \
+      dst_v[0] = RGB2xToV(ar, ag, ab);                                     \
+      src_rgb += BPP * 2;                                                  \
+      src_rgb1 += BPP * 2;                                                 \
+      dst_u += 1;                                                          \
+      dst_v += 1;                                                          \
+    }                                                                      \
+    if (width & 1) {                                                       \
+      uint16_t ab = src_rgb[B] + src_rgb1[B];                              \
+      uint16_t ag = src_rgb[G] + src_rgb1[G];                              \
+      uint16_t ar = src_rgb[R] + src_rgb1[R];                              \
+      dst_u[0] = RGB2xToU(ar, ag, ab);                                     \
+      dst_v[0] = RGB2xToV(ar, ag, ab);                                     \
+    }                                                                      \
   }
 #endif
 
@@ -608,80 +693,80 @@ static __inline int RGB2xToVJ(uint16_t r, uint16_t g, uint16_t b) {
 // ARGBToYJ_C and ARGBToUVJ_C
 // Intel version mimic SSE/AVX which does 2 pavgb
 #if LIBYUV_ARGBTOUV_PAVGB
-#define MAKEROWYJ(NAME, R, G, B, BPP)                                         \
-  void NAME##ToYJRow_C(const uint8_t* src_argb0, uint8_t* dst_y, int width) { \
-    int x;                                                                    \
-    for (x = 0; x < width; ++x) {                                             \
-      dst_y[0] = RGBToYJ(src_argb0[R], src_argb0[G], src_argb0[B]);           \
-      src_argb0 += BPP;                                                       \
-      dst_y += 1;                                                             \
-    }                                                                         \
-  }                                                                           \
-  void NAME##ToUVJRow_C(const uint8_t* src_rgb0, int src_stride_rgb,          \
-                        uint8_t* dst_u, uint8_t* dst_v, int width) {          \
-    const uint8_t* src_rgb1 = src_rgb0 + src_stride_rgb;                      \
-    int x;                                                                    \
-    for (x = 0; x < width - 1; x += 2) {                                      \
-      uint8_t ab = AVGB(AVGB(src_rgb0[B], src_rgb1[B]),                       \
-                        AVGB(src_rgb0[B + BPP], src_rgb1[B + BPP]));          \
-      uint8_t ag = AVGB(AVGB(src_rgb0[G], src_rgb1[G]),                       \
-                        AVGB(src_rgb0[G + BPP], src_rgb1[G + BPP]));          \
-      uint8_t ar = AVGB(AVGB(src_rgb0[R], src_rgb1[R]),                       \
-                        AVGB(src_rgb0[R + BPP], src_rgb1[R + BPP]));          \
-      dst_u[0] = RGBToUJ(ar, ag, ab);                                         \
-      dst_v[0] = RGBToVJ(ar, ag, ab);                                         \
-      src_rgb0 += BPP * 2;                                                    \
-      src_rgb1 += BPP * 2;                                                    \
-      dst_u += 1;                                                             \
-      dst_v += 1;                                                             \
-    }                                                                         \
-    if (width & 1) {                                                          \
-      uint8_t ab = AVGB(src_rgb0[B], src_rgb1[B]);                            \
-      uint8_t ag = AVGB(src_rgb0[G], src_rgb1[G]);                            \
-      uint8_t ar = AVGB(src_rgb0[R], src_rgb1[R]);                            \
-      dst_u[0] = RGBToUJ(ar, ag, ab);                                         \
-      dst_v[0] = RGBToVJ(ar, ag, ab);                                         \
-    }                                                                         \
+#define MAKEROWYJ(NAME, R, G, B, BPP)                                       \
+  void NAME##ToYJRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width) { \
+    int x;                                                                  \
+    for (x = 0; x < width; ++x) {                                           \
+      dst_y[0] = RGBToYJ(src_rgb[R], src_rgb[G], src_rgb[B]);               \
+      src_rgb += BPP;                                                       \
+      dst_y += 1;                                                           \
+    }                                                                       \
+  }                                                                         \
+  void NAME##ToUVJRow_C(const uint8_t* src_rgb, int src_stride_rgb,         \
+                        uint8_t* dst_u, uint8_t* dst_v, int width) {        \
+    const uint8_t* src_rgb1 = src_rgb + src_stride_rgb;                     \
+    int x;                                                                  \
+    for (x = 0; x < width - 1; x += 2) {                                    \
+      uint8_t ab = AVGB(AVGB(src_rgb[B], src_rgb1[B]),                      \
+                        AVGB(src_rgb[B + BPP], src_rgb1[B + BPP]));         \
+      uint8_t ag = AVGB(AVGB(src_rgb[G], src_rgb1[G]),                      \
+                        AVGB(src_rgb[G + BPP], src_rgb1[G + BPP]));         \
+      uint8_t ar = AVGB(AVGB(src_rgb[R], src_rgb1[R]),                      \
+                        AVGB(src_rgb[R + BPP], src_rgb1[R + BPP]));         \
+      dst_u[0] = RGBToUJ(ar, ag, ab);                                       \
+      dst_v[0] = RGBToVJ(ar, ag, ab);                                       \
+      src_rgb += BPP * 2;                                                   \
+      src_rgb1 += BPP * 2;                                                  \
+      dst_u += 1;                                                           \
+      dst_v += 1;                                                           \
+    }                                                                       \
+    if (width & 1) {                                                        \
+      uint8_t ab = AVGB(src_rgb[B], src_rgb1[B]);                           \
+      uint8_t ag = AVGB(src_rgb[G], src_rgb1[G]);                           \
+      uint8_t ar = AVGB(src_rgb[R], src_rgb1[R]);                           \
+      dst_u[0] = RGBToUJ(ar, ag, ab);                                       \
+      dst_v[0] = RGBToVJ(ar, ag, ab);                                       \
+    }                                                                       \
   }
 #else
 // ARM version does sum / 2 then multiply by 2x smaller coefficients
-#define MAKEROWYJ(NAME, R, G, B, BPP)                                         \
-  void NAME##ToYJRow_C(const uint8_t* src_argb0, uint8_t* dst_y, int width) { \
-    int x;                                                                    \
-    for (x = 0; x < width; ++x) {                                             \
-      dst_y[0] = RGBToYJ(src_argb0[R], src_argb0[G], src_argb0[B]);           \
-      src_argb0 += BPP;                                                       \
-      dst_y += 1;                                                             \
-    }                                                                         \
-  }                                                                           \
-  void NAME##ToUVJRow_C(const uint8_t* src_rgb0, int src_stride_rgb,          \
-                        uint8_t* dst_u, uint8_t* dst_v, int width) {          \
-    const uint8_t* src_rgb1 = src_rgb0 + src_stride_rgb;                      \
-    int x;                                                                    \
-    for (x = 0; x < width - 1; x += 2) {                                      \
-      uint16_t ab = (src_rgb0[B] + src_rgb0[B + BPP] + src_rgb1[B] +          \
-                     src_rgb1[B + BPP] + 1) >>                                \
-                    1;                                                        \
-      uint16_t ag = (src_rgb0[G] + src_rgb0[G + BPP] + src_rgb1[G] +          \
-                     src_rgb1[G + BPP] + 1) >>                                \
-                    1;                                                        \
-      uint16_t ar = (src_rgb0[R] + src_rgb0[R + BPP] + src_rgb1[R] +          \
-                     src_rgb1[R + BPP] + 1) >>                                \
-                    1;                                                        \
-      dst_u[0] = RGB2xToUJ(ar, ag, ab);                                       \
-      dst_v[0] = RGB2xToVJ(ar, ag, ab);                                       \
-      src_rgb0 += BPP * 2;                                                    \
-      src_rgb1 += BPP * 2;                                                    \
-      dst_u += 1;                                                             \
-      dst_v += 1;                                                             \
-    }                                                                         \
-    if (width & 1) {                                                          \
-      uint16_t ab = (src_rgb0[B] + src_rgb1[B]);                              \
-      uint16_t ag = (src_rgb0[G] + src_rgb1[G]);                              \
-      uint16_t ar = (src_rgb0[R] + src_rgb1[R]);                              \
-      dst_u[0] = RGB2xToUJ(ar, ag, ab);                                       \
-      dst_v[0] = RGB2xToVJ(ar, ag, ab);                                       \
-    }                                                                         \
+#define MAKEROWYJ(NAME, R, G, B, BPP)                                       \
+  void NAME##ToYJRow_C(const uint8_t* src_rgb, uint8_t* dst_y, int width) { \
+    int x;                                                                  \
+    for (x = 0; x < width; ++x) {                                           \
+      dst_y[0] = RGBToYJ(src_rgb[R], src_rgb[G], src_rgb[B]);               \
+      src_rgb += BPP;                                                       \
+      dst_y += 1;                                                           \
+    }                                                                       \
+  }                                                                         \
+  void NAME##ToUVJRow_C(const uint8_t* src_rgb, int src_stride_rgb,         \
+                        uint8_t* dst_u, uint8_t* dst_v, int width) {        \
+    const uint8_t* src_rgb1 = src_rgb + src_stride_rgb;                     \
+    int x;                                                                  \
+    for (x = 0; x < width - 1; x += 2) {                                    \
+      uint16_t ab = (src_rgb[B] + src_rgb[B + BPP] + src_rgb1[B] +          \
+                     src_rgb1[B + BPP] + 1) >>                              \
+                    1;                                                      \
+      uint16_t ag = (src_rgb[G] + src_rgb[G + BPP] + src_rgb1[G] +          \
+                     src_rgb1[G + BPP] + 1) >>                              \
+                    1;                                                      \
+      uint16_t ar = (src_rgb[R] + src_rgb[R + BPP] + src_rgb1[R] +          \
+                     src_rgb1[R + BPP] + 1) >>                              \
+                    1;                                                      \
+      dst_u[0] = RGB2xToUJ(ar, ag, ab);                                     \
+      dst_v[0] = RGB2xToVJ(ar, ag, ab);                                     \
+      src_rgb += BPP * 2;                                                   \
+      src_rgb1 += BPP * 2;                                                  \
+      dst_u += 1;                                                           \
+      dst_v += 1;                                                           \
+    }                                                                       \
+    if (width & 1) {                                                        \
+      uint16_t ab = (src_rgb[B] + src_rgb1[B]);                             \
+      uint16_t ag = (src_rgb[G] + src_rgb1[G]);                             \
+      uint16_t ar = (src_rgb[R] + src_rgb1[R]);                             \
+      dst_u[0] = RGB2xToUJ(ar, ag, ab);                                     \
+      dst_v[0] = RGB2xToVJ(ar, ag, ab);                                     \
+    }                                                                       \
   }
 
 #endif
@@ -1151,16 +1236,16 @@ void ARGBShadeRow_C(const uint8_t* src_argb,
 #define REPEAT8(v) (v) | ((v) << 8)
 #define SHADE(f, v) v* f >> 16
 
-void ARGBMultiplyRow_C(const uint8_t* src_argb0,
+void ARGBMultiplyRow_C(const uint8_t* src_argb,
                        const uint8_t* src_argb1,
                        uint8_t* dst_argb,
                        int width) {
   int i;
   for (i = 0; i < width; ++i) {
-    const uint32_t b = REPEAT8(src_argb0[0]);
-    const uint32_t g = REPEAT8(src_argb0[1]);
-    const uint32_t r = REPEAT8(src_argb0[2]);
-    const uint32_t a = REPEAT8(src_argb0[3]);
+    const uint32_t b = REPEAT8(src_argb[0]);
+    const uint32_t g = REPEAT8(src_argb[1]);
+    const uint32_t r = REPEAT8(src_argb[2]);
+    const uint32_t a = REPEAT8(src_argb[3]);
     const uint32_t b_scale = src_argb1[0];
     const uint32_t g_scale = src_argb1[1];
     const uint32_t r_scale = src_argb1[2];
@@ -1169,7 +1254,7 @@ void ARGBMultiplyRow_C(const uint8_t* src_argb0,
     dst_argb[1] = SHADE(g, g_scale);
     dst_argb[2] = SHADE(r, r_scale);
     dst_argb[3] = SHADE(a, a_scale);
-    src_argb0 += 4;
+    src_argb += 4;
     src_argb1 += 4;
     dst_argb += 4;
   }
@@ -1179,16 +1264,16 @@ void ARGBMultiplyRow_C(const uint8_t* src_argb0,
 
 #define SHADE(f, v) clamp255(v + f)
 
-void ARGBAddRow_C(const uint8_t* src_argb0,
+void ARGBAddRow_C(const uint8_t* src_argb,
                   const uint8_t* src_argb1,
                   uint8_t* dst_argb,
                   int width) {
   int i;
   for (i = 0; i < width; ++i) {
-    const int b = src_argb0[0];
-    const int g = src_argb0[1];
-    const int r = src_argb0[2];
-    const int a = src_argb0[3];
+    const int b = src_argb[0];
+    const int g = src_argb[1];
+    const int r = src_argb[2];
+    const int a = src_argb[3];
     const int b_add = src_argb1[0];
     const int g_add = src_argb1[1];
     const int r_add = src_argb1[2];
@@ -1197,7 +1282,7 @@ void ARGBAddRow_C(const uint8_t* src_argb0,
     dst_argb[1] = SHADE(g, g_add);
     dst_argb[2] = SHADE(r, r_add);
     dst_argb[3] = SHADE(a, a_add);
-    src_argb0 += 4;
+    src_argb += 4;
     src_argb1 += 4;
     dst_argb += 4;
   }
@@ -1206,16 +1291,16 @@ void ARGBAddRow_C(const uint8_t* src_argb0,
 
 #define SHADE(f, v) clamp0(f - v)
 
-void ARGBSubtractRow_C(const uint8_t* src_argb0,
+void ARGBSubtractRow_C(const uint8_t* src_argb,
                        const uint8_t* src_argb1,
                        uint8_t* dst_argb,
                        int width) {
   int i;
   for (i = 0; i < width; ++i) {
-    const int b = src_argb0[0];
-    const int g = src_argb0[1];
-    const int r = src_argb0[2];
-    const int a = src_argb0[3];
+    const int b = src_argb[0];
+    const int g = src_argb[1];
+    const int r = src_argb[2];
+    const int a = src_argb[3];
     const int b_sub = src_argb1[0];
     const int g_sub = src_argb1[1];
     const int r_sub = src_argb1[2];
@@ -1224,7 +1309,7 @@ void ARGBSubtractRow_C(const uint8_t* src_argb0,
     dst_argb[1] = SHADE(g, g_sub);
     dst_argb[2] = SHADE(r, r_sub);
     dst_argb[3] = SHADE(a, a_sub);
-    src_argb0 += 4;
+    src_argb += 4;
     src_argb1 += 4;
     dst_argb += 4;
   }
@@ -1334,64 +1419,39 @@ void J400ToARGBRow_C(const uint8_t* src_y, uint8_t* dst_argb, int width) {
 
 // Macros to create SIMD specific yuv to rgb conversion constants.
 
-#if defined(__aarch64__)
-#define MAKEYUVCONSTANTS(name, YG, YB, UB, UG, VG, VR, BB, BG, BR)        \
-  const struct YuvConstants SIMD_ALIGNED(kYuv##name##Constants) = {       \
-      {UB, VR, UB, VR, UB, VR, UB, VR}, {UB, VR, UB, VR, UB, VR, UB, VR}, \
-      {UG, VG, UG, VG, UG, VG, UG, VG}, {UG, VG, UG, VG, UG, VG, UG, VG}, \
-      {BB, BG, BR, YB, 0, 0, 0, 0},     {0x0101 * YG, YG, 0, 0}};         \
-  const struct YuvConstants SIMD_ALIGNED(kYvu##name##Constants) = {       \
-      {VR, UB, VR, UB, VR, UB, VR, UB}, {VR, UB, VR, UB, VR, UB, VR, UB}, \
-      {VG, UG, VG, UG, VG, UG, VG, UG}, {VG, UG, VG, UG, VG, UG, VG, UG}, \
-      {BR, BG, BB, YB, 0, 0, 0, 0},     {0x0101 * YG, YG, 0, 0}};
-
-#elif defined(__arm__)
-#define MAKEYUVCONSTANTS(name, YG, YB, UB, UG, VG, VR, BB, BG, BR)  \
-  const struct YuvConstants SIMD_ALIGNED(kYuv##name##Constants) = { \
-      {UB, UB, UB, UB, VR, VR, VR, VR, 0, 0, 0, 0, 0, 0, 0, 0},     \
-      {UG, UG, UG, UG, VG, VG, VG, VG, 0, 0, 0, 0, 0, 0, 0, 0},     \
-      {BB, BG, BR, YB, 0, 0, 0, 0},                                 \
-      {0x0101 * YG, YG, 0, 0}};                                     \
-  const struct YuvConstants SIMD_ALIGNED(kYvu##name##Constants) = { \
-      {VR, VR, VR, VR, UB, UB, UB, UB, 0, 0, 0, 0, 0, 0, 0, 0},     \
-      {VG, VG, VG, VG, UG, UG, UG, UG, 0, 0, 0, 0, 0, 0, 0, 0},     \
-      {BR, BG, BB, YB, 0, 0, 0, 0},                                 \
-      {0x0101 * YG, YG, 0, 0}};
+// clang-format off
 
+#if defined(__aarch64__) || defined(__arm__)
+// Bias values to round, and subtract 128 from U and V.
+// For B and R this is negative. For G this is positive.
+#define BB (UB * 128 - YB)
+#define BG (UG * 128 + VG * 128 + YB)
+#define BR (VR * 128 - YB)
+
+#define YUBCONSTANTSBODY(YG, YB, UB, UG, VG, VR)         \
+  {{UB, VR, UG, VG, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}, \
+   {YG, BB, BG, BR, YB, 0, 0, 0}}
 #else
-#define MAKEYUVCONSTANTS(name, YG, YB, UB, UG, VG, VR, BB, BG, BR)       \
-  const struct YuvConstants SIMD_ALIGNED(kYuv##name##Constants) = {      \
-      {-UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0,   \
-       -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0},  \
-      {UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG,   \
-       UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG},  \
-      {0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR,   \
-       0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR},  \
-      {BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB},  \
-      {BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG},  \
-      {BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR},  \
-      {YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG},  \
-      {YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB}}; \
-  const struct YuvConstants SIMD_ALIGNED(kYvu##name##Constants) = {      \
-      {-VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0,   \
-       -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0, -VR, 0},  \
-      {VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG,   \
-       VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG},  \
-      {0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB,   \
-       0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB, 0, -UB},  \
-      {BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR, BR},  \
-      {BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG, BG},  \
-      {BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB, BB},  \
-      {YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG},  \
-      {YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB}};
+#define YUBCONSTANTSBODY(YG, YB, UB, UG, VG, VR)                     \
+  {{UB, 0, UB, 0, UB, 0, UB, 0, UB, 0, UB, 0, UB, 0, UB, 0,          \
+    UB, 0, UB, 0, UB, 0, UB, 0, UB, 0, UB, 0, UB, 0, UB, 0},         \
+   {UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG,  \
+    UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG, UG, VG}, \
+   {0, VR, 0, VR, 0, VR, 0, VR, 0, VR, 0, VR, 0, VR, 0, VR,          \
+    0, VR, 0, VR, 0, VR, 0, VR, 0, VR, 0, VR, 0, VR, 0, VR},         \
+   {YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG, YG}, \
+   {YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB, YB}}
 #endif
 
-// TODO(fbarchard): Generate SIMD structures from float matrix.
+// clang-format on
 
-// Bias values to round, and subtract 128 from U and V.
-#define BB (-UB * 128 + YB)
-#define BG (UG * 128 + VG * 128 + YB)
-#define BR (-VR * 128 + YB)
+#define MAKEYUVCONSTANTS(name, YG, YB, UB, UG, VG, VR)            \
+  const struct YuvConstants SIMD_ALIGNED(kYuv##name##Constants) = \
+      YUBCONSTANTSBODY(YG, YB, UB, UG, VG, VR);                   \
+  const struct YuvConstants SIMD_ALIGNED(kYvu##name##Constants) = \
+      YUBCONSTANTSBODY(YG, YB, VR, VG, UG, UB);
+
+// TODO(fbarchard): Generate SIMD structures from float matrix.
 
 // BT.601 limited range YUV to RGB reference
 //  R = (Y - 16) * 1.164             + V * 1.596
@@ -1400,7 +1460,7 @@ void J400ToARGBRow_C(const uint8_t* src_y, uint8_t* dst_argb, int width) {
 // KR = 0.299; KB = 0.114
 
 // U and V contributions to R,G,B.
-#if LIBYUV_UNLIMITED_DATA
+#ifdef LIBYUV_UNLIMITED_DATA
 #define UB 129 /* round(2.018 * 64) */
 #else
 #define UB 128 /* max(128, round(2.018 * 64)) */
@@ -1413,7 +1473,7 @@ void J400ToARGBRow_C(const uint8_t* src_y, uint8_t* dst_argb, int width) {
 #define YG 18997 /* round(1.164 * 64 * 256 * 256 / 257) */
 #define YB -1160 /* 1.164 * 64 * -16 + 64 / 2 */
 
-MAKEYUVCONSTANTS(I601, YG, YB, UB, UG, VG, VR, BB, BG, BR)
+MAKEYUVCONSTANTS(I601, YG, YB, UB, UG, VG, VR)
 
 #undef YG
 #undef YB
@@ -1438,7 +1498,7 @@ MAKEYUVCONSTANTS(I601, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 #define YG 16320 /* round(1.000 * 64 * 256 * 256 / 257) */
 #define YB 32    /* 64 / 2 */
 
-MAKEYUVCONSTANTS(JPEG, YG, YB, UB, UG, VG, VR, BB, BG, BR)
+MAKEYUVCONSTANTS(JPEG, YG, YB, UB, UG, VG, VR)
 
 #undef YG
 #undef YB
@@ -1454,7 +1514,7 @@ MAKEYUVCONSTANTS(JPEG, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 //  KR = 0.2126, KB = 0.0722
 
 // U and V contributions to R,G,B.
-#if LIBYUV_UNLIMITED_DATA
+#ifdef LIBYUV_UNLIMITED_DATA
 #define UB 135 /* round(2.112 * 64) */
 #else
 #define UB 128 /* max(128, round(2.112 * 64)) */
@@ -1467,7 +1527,7 @@ MAKEYUVCONSTANTS(JPEG, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 #define YG 18997 /* round(1.164 * 64 * 256 * 256 / 257) */
 #define YB -1160 /* 1.164 * 64 * -16 + 64 / 2 */
 
-MAKEYUVCONSTANTS(H709, YG, YB, UB, UG, VG, VR, BB, BG, BR)
+MAKEYUVCONSTANTS(H709, YG, YB, UB, UG, VG, VR)
 
 #undef YG
 #undef YB
@@ -1492,7 +1552,7 @@ MAKEYUVCONSTANTS(H709, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 #define YG 16320 /* round(1 * 64 * 256 * 256 / 257) */
 #define YB 32    /* 64 / 2 */
 
-MAKEYUVCONSTANTS(F709, YG, YB, UB, UG, VG, VR, BB, BG, BR)
+MAKEYUVCONSTANTS(F709, YG, YB, UB, UG, VG, VR)
 
 #undef YG
 #undef YB
@@ -1508,7 +1568,7 @@ MAKEYUVCONSTANTS(F709, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 // KR = 0.2627; KB = 0.0593
 
 // U and V contributions to R,G,B.
-#if LIBYUV_UNLIMITED_DATA
+#ifdef LIBYUV_UNLIMITED_DATA
 #define UB 137 /* round(2.142 * 64) */
 #else
 #define UB 128 /* max(128, round(2.142 * 64)) */
@@ -1521,7 +1581,7 @@ MAKEYUVCONSTANTS(F709, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 #define YG 19003 /* round(1.164384 * 64 * 256 * 256 / 257) */
 #define YB -1160 /* 1.164384 * 64 * -16 + 64 / 2 */
 
-MAKEYUVCONSTANTS(2020, YG, YB, UB, UG, VG, VR, BB, BG, BR)
+MAKEYUVCONSTANTS(2020, YG, YB, UB, UG, VG, VR)
 
 #undef YG
 #undef YB
@@ -1545,7 +1605,7 @@ MAKEYUVCONSTANTS(2020, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 #define YG 16320 /* round(1 * 64 * 256 * 256 / 257) */
 #define YB 32    /* 64 / 2 */
 
-MAKEYUVCONSTANTS(V2020, YG, YB, UB, UG, VG, VR, BB, BG, BR)
+MAKEYUVCONSTANTS(V2020, YG, YB, UB, UG, VG, VR)
 
 #undef YG
 #undef YB
@@ -1560,54 +1620,44 @@ MAKEYUVCONSTANTS(V2020, YG, YB, UB, UG, VG, VR, BB, BG, BR)
 
 #undef MAKEYUVCONSTANTS
 
-#if LIBYUV_UNLIMITED_DATA
-
-// C reference code that mimics the YUV assembly.
-// Reads 8 bit YUV and leaves result as 16 bit.
-static __inline void YuvPixel(uint8_t y,
-                              uint8_t u,
-                              uint8_t v,
-                              uint8_t* b,
-                              uint8_t* g,
-                              uint8_t* r,
-                              const struct YuvConstants* yuvconstants) {
-#if defined(__aarch64__)
-  int ub = yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = yuvconstants->kUVToRB[1];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#elif defined(__arm__)
-  int ub = yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[4];
-  int vr = yuvconstants->kUVToRB[4];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
+#if defined(__aarch64__) || defined(__arm__)
+#define LOAD_YUV_CONSTANTS                 \
+  int ub = yuvconstants->kUVCoeff[0];      \
+  int vr = yuvconstants->kUVCoeff[1];      \
+  int ug = yuvconstants->kUVCoeff[2];      \
+  int vg = yuvconstants->kUVCoeff[3];      \
+  int yg = yuvconstants->kRGBCoeffBias[0]; \
+  int bb = yuvconstants->kRGBCoeffBias[1]; \
+  int bg = yuvconstants->kRGBCoeffBias[2]; \
+  int br = yuvconstants->kRGBCoeffBias[3]
+
+#define CALC_RGB16                         \
+  int32_t y1 = (uint32_t)(y32 * yg) >> 16; \
+  int b16 = y1 + (u * ub) - bb;            \
+  int g16 = y1 + bg - (u * ug + v * vg);   \
+  int r16 = y1 + (v * vr) - br
 #else
-  int ub = -yuvconstants->kUVToB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = -yuvconstants->kUVToR[1];
-  int bb = yuvconstants->kUVBiasB[0];
-  int bg = yuvconstants->kUVBiasG[0];
-  int br = yuvconstants->kUVBiasR[0];
-  int yg = yuvconstants->kYToRgb[0];
+#define LOAD_YUV_CONSTANTS            \
+  int ub = yuvconstants->kUVToB[0];   \
+  int ug = yuvconstants->kUVToG[0];   \
+  int vg = yuvconstants->kUVToG[1];   \
+  int vr = yuvconstants->kUVToR[1];   \
+  int yg = yuvconstants->kYToRgb[0];  \
+  int yb = yuvconstants->kYBiasToRgb[0]
+
+#define CALC_RGB16                                \
+  int32_t y1 = ((uint32_t)(y32 * yg) >> 16) + yb; \
+  int8_t ui = u;                                  \
+  int8_t vi = v;                                  \
+  ui -= 0x80;                                     \
+  vi -= 0x80;                                     \
+  int b16 = y1 + (ui * ub);                       \
+  int g16 = y1 - (ui * ug + vi * vg);             \
+  int r16 = y1 + (vi * vr)
 #endif
 
-  uint32_t y1 = (uint32_t)(y * 0x0101 * yg) >> 16;
-  *b = Clamp((int32_t)(y1 + (u * ub) + bb) >> 6);
-  *g = Clamp((int32_t)(y1 - (u * ug + v * vg) + bg) >> 6);
-  *r = Clamp((int32_t)(y1 + (v * vr) + br) >> 6);
-}
-#else
 // C reference code that mimics the YUV assembly.
-// Reads 8 bit YUV and leaves result as 8 bit.
+// Reads 8 bit YUV and leaves result as 16 bit.
 static __inline void YuvPixel(uint8_t y,
                               uint8_t u,
                               uint8_t v,
@@ -1615,41 +1665,13 @@ static __inline void YuvPixel(uint8_t y,
                               uint8_t* g,
                               uint8_t* r,
                               const struct YuvConstants* yuvconstants) {
-#if defined(__aarch64__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = -yuvconstants->kUVToRB[1];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#elif defined(__arm__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[4];
-  int vr = -yuvconstants->kUVToRB[4];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#else
-  int ub = yuvconstants->kUVToB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = yuvconstants->kUVToR[1];
-  int bb = yuvconstants->kUVBiasB[0];
-  int bg = yuvconstants->kUVBiasG[0];
-  int br = yuvconstants->kUVBiasR[0];
-  int yg = yuvconstants->kYToRgb[0];
-#endif
-
-  uint32_t y1 = (uint32_t)(y * 0x0101 * yg) >> 16;
-  *b = Clamp((int32_t)(y1 - (u * ub) + bb) >> 6);
-  *g = Clamp((int32_t)(y1 - (u * ug + v * vg) + bg) >> 6);
-  *r = Clamp((int32_t)(y1 - (v * vr) + br) >> 6);
+  LOAD_YUV_CONSTANTS;
+  uint32_t y32 = y * 0x0101;
+  CALC_RGB16;
+  *b = Clamp((int32_t)(b16) >> 6);
+  *g = Clamp((int32_t)(g16) >> 6);
+  *r = Clamp((int32_t)(r16) >> 6);
 }
-#endif
 
 // Reads 8 bit YUV and leaves result as 16 bit.
 static __inline void YuvPixel8_16(uint8_t y,
@@ -1659,39 +1681,12 @@ static __inline void YuvPixel8_16(uint8_t y,
                                   int* g,
                                   int* r,
                                   const struct YuvConstants* yuvconstants) {
-#if defined(__aarch64__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = -yuvconstants->kUVToRB[1];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#elif defined(__arm__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[4];
-  int vr = -yuvconstants->kUVToRB[4];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#else
-  int ub = yuvconstants->kUVToB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = yuvconstants->kUVToR[1];
-  int bb = yuvconstants->kUVBiasB[0];
-  int bg = yuvconstants->kUVBiasG[0];
-  int br = yuvconstants->kUVBiasR[0];
-  int yg = yuvconstants->kYToRgb[0];
-#endif
-
-  uint32_t y1 = (uint32_t)(y * 0x0101 * yg) >> 16;
-  *b = (int)(y1 - (u * ub) + bb);
-  *g = (int)(y1 - (u * ug + v * vg) + bg);
-  *r = (int)(y1 - (v * vr) + br);
+  LOAD_YUV_CONSTANTS;
+  uint32_t y32 = y * 0x0101;
+  CALC_RGB16;
+  *b = b16;
+  *g = g16;
+  *r = r16;
 }
 
 // C reference code that mimics the YUV 16 bit assembly.
@@ -1703,41 +1698,14 @@ static __inline void YuvPixel10_16(uint16_t y,
                                    int* g,
                                    int* r,
                                    const struct YuvConstants* yuvconstants) {
-#if defined(__aarch64__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = -yuvconstants->kUVToRB[1];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#elif defined(__arm__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[4];
-  int vr = -yuvconstants->kUVToRB[4];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#else
-  int ub = yuvconstants->kUVToB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = yuvconstants->kUVToR[1];
-  int bb = yuvconstants->kUVBiasB[0];
-  int bg = yuvconstants->kUVBiasG[0];
-  int br = yuvconstants->kUVBiasR[0];
-  int yg = yuvconstants->kYToRgb[0];
-#endif
-
-  uint32_t y1 = (uint32_t)((y << 6) * yg) >> 16;
+  LOAD_YUV_CONSTANTS;
+  uint32_t y32 = y << 6;
   u = clamp255(u >> 2);
   v = clamp255(v >> 2);
-  *b = (int)(-(u * ub) + y1 + bb);
-  *g = (int)(-(u * ug + v * vg) + y1 + bg);
-  *r = (int)(-(v * vr) + y1 + br);
+  CALC_RGB16;
+  *b = b16;
+  *g = g16;
+  *r = r16;
 }
 
 // C reference code that mimics the YUV 16 bit assembly.
@@ -1749,41 +1717,14 @@ static __inline void YuvPixel12_16(int16_t y,
                                    int* g,
                                    int* r,
                                    const struct YuvConstants* yuvconstants) {
-#if defined(__aarch64__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = -yuvconstants->kUVToRB[1];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#elif defined(__arm__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[4];
-  int vr = -yuvconstants->kUVToRB[4];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#else
-  int ub = yuvconstants->kUVToB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = yuvconstants->kUVToR[1];
-  int bb = yuvconstants->kUVBiasB[0];
-  int bg = yuvconstants->kUVBiasG[0];
-  int br = yuvconstants->kUVBiasR[0];
-  int yg = yuvconstants->kYToRgb[0];
-#endif
-
-  uint32_t y1 = (uint32_t)((y << 4) * yg) >> 16;
+  LOAD_YUV_CONSTANTS;
+  uint32_t y32 = y << 4;
   u = clamp255(u >> 4);
   v = clamp255(v >> 4);
-  *b = (int)(-(u * ub) + y1 + bb);
-  *g = (int)(-(u * ug + v * vg) + y1 + bg);
-  *r = (int)(-(v * vr) + y1 + br);
+  CALC_RGB16;
+  *b = b16;
+  *g = g16;
+  *r = r16;
 }
 
 // C reference code that mimics the YUV 10 bit assembly.
@@ -1831,41 +1772,14 @@ static __inline void YuvPixel16_8(uint16_t y,
                                   uint8_t* g,
                                   uint8_t* r,
                                   const struct YuvConstants* yuvconstants) {
-#if defined(__aarch64__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = -yuvconstants->kUVToRB[1];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#elif defined(__arm__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[4];
-  int vr = -yuvconstants->kUVToRB[4];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#else
-  int ub = yuvconstants->kUVToB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = yuvconstants->kUVToR[1];
-  int bb = yuvconstants->kUVBiasB[0];
-  int bg = yuvconstants->kUVBiasG[0];
-  int br = yuvconstants->kUVBiasR[0];
-  int yg = yuvconstants->kYToRgb[0];
-#endif
-
-  uint32_t y1 = (uint32_t)(y * yg) >> 16;
+  LOAD_YUV_CONSTANTS;
+  uint32_t y32 = y;
   u = clamp255(u >> 8);
   v = clamp255(v >> 8);
-  *b = Clamp((int32_t)(y1 + -(u * ub) + bb) >> 6);
-  *g = Clamp((int32_t)(y1 + -(u * ug + v * vg) + bg) >> 6);
-  *r = Clamp((int32_t)(y1 + -(v * vr) + br) >> 6);
+  CALC_RGB16;
+  *b = Clamp((int32_t)(b16) >> 6);
+  *g = Clamp((int32_t)(g16) >> 6);
+  *r = Clamp((int32_t)(r16) >> 6);
 }
 
 // C reference code that mimics the YUV 16 bit assembly.
@@ -1877,41 +1791,14 @@ static __inline void YuvPixel16_16(uint16_t y,
                                    int* g,
                                    int* r,
                                    const struct YuvConstants* yuvconstants) {
-#if defined(__aarch64__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = -yuvconstants->kUVToRB[1];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#elif defined(__arm__)
-  int ub = -yuvconstants->kUVToRB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[4];
-  int vr = -yuvconstants->kUVToRB[4];
-  int bb = yuvconstants->kUVBiasBGR[0];
-  int bg = yuvconstants->kUVBiasBGR[1];
-  int br = yuvconstants->kUVBiasBGR[2];
-  int yg = yuvconstants->kYToRgb[1];
-#else
-  int ub = yuvconstants->kUVToB[0];
-  int ug = yuvconstants->kUVToG[0];
-  int vg = yuvconstants->kUVToG[1];
-  int vr = yuvconstants->kUVToR[1];
-  int bb = yuvconstants->kUVBiasB[0];
-  int bg = yuvconstants->kUVBiasG[0];
-  int br = yuvconstants->kUVBiasR[0];
-  int yg = yuvconstants->kYToRgb[0];
-#endif
-
-  uint32_t y1 = (uint32_t)(y * yg) >> 16;
+  LOAD_YUV_CONSTANTS;
+  uint32_t y32 = y;
   u = clamp255(u >> 8);
   v = clamp255(v >> 8);
-  *b = (int)(y1 + -(u * ub) + bb);
-  *g = (int)(y1 + -(u * ug + v * vg) + bg);
-  *r = (int)(y1 + -(v * vr) + br);
+  CALC_RGB16;
+  *b = b16;
+  *g = g16;
+  *r = r16;
 }
 
 // C reference code that mimics the YUV assembly.
@@ -1922,8 +1809,8 @@ static __inline void YPixel(uint8_t y,
                             uint8_t* r,
                             const struct YuvConstants* yuvconstants) {
 #if defined(__aarch64__) || defined(__arm__)
-  int ygb = yuvconstants->kUVBiasBGR[3];
-  int yg = yuvconstants->kYToRgb[1];
+  int yg = yuvconstants->kRGBCoeffBias[0];
+  int ygb = yuvconstants->kRGBCoeffBias[4];
 #else
   int ygb = yuvconstants->kYBiasToRgb[0];
   int yg = yuvconstants->kYToRgb[0];
@@ -1934,38 +1821,6 @@ static __inline void YPixel(uint8_t y,
   *r = Clamp(((int32_t)(y1) + ygb) >> 6);
 }
 
-#if !defined(LIBYUV_DISABLE_NEON) && \
-    (defined(__ARM_NEON__) || defined(__aarch64__) || defined(LIBYUV_NEON))
-// C mimic assembly.
-// TODO(fbarchard): Remove subsampling from Neon.
-void I444ToARGBRow_C(const uint8_t* src_y,
-                     const uint8_t* src_u,
-                     const uint8_t* src_v,
-                     uint8_t* rgb_buf,
-                     const struct YuvConstants* yuvconstants,
-                     int width) {
-  int x;
-  for (x = 0; x < width - 1; x += 2) {
-    uint8_t u = (src_u[0] + src_u[1] + 1) >> 1;
-    uint8_t v = (src_v[0] + src_v[1] + 1) >> 1;
-    YuvPixel(src_y[0], u, v, rgb_buf + 0, rgb_buf + 1, rgb_buf + 2,
-             yuvconstants);
-    rgb_buf[3] = 255;
-    YuvPixel(src_y[1], u, v, rgb_buf + 4, rgb_buf + 5, rgb_buf + 6,
-             yuvconstants);
-    rgb_buf[7] = 255;
-    src_y += 2;
-    src_u += 2;
-    src_v += 2;
-    rgb_buf += 8;  // Advance 2 pixels.
-  }
-  if (width & 1) {
-    YuvPixel(src_y[0], src_u[0], src_v[0], rgb_buf + 0, rgb_buf + 1,
-             rgb_buf + 2, yuvconstants);
-    rgb_buf[3] = 255;
-  }
-}
-#else
 void I444ToARGBRow_C(const uint8_t* src_y,
                      const uint8_t* src_u,
                      const uint8_t* src_v,
@@ -1983,7 +1838,6 @@ void I444ToARGBRow_C(const uint8_t* src_y,
     rgb_buf += 4;  // Advance 1 pixel.
   }
 }
-#endif
 
 // Also used for 420
 void I422ToARGBRow_C(const uint8_t* src_y,
@@ -2329,40 +2183,6 @@ void I422ToAR30Row_C(const uint8_t* src_y,
   }
 }
 
-#if !defined(LIBYUV_DISABLE_NEON) && \
-    (defined(__ARM_NEON__) || defined(__aarch64__) || defined(LIBYUV_NEON))
-// C mimic assembly.
-// TODO(fbarchard): Remove subsampling from Neon.
-void I444AlphaToARGBRow_C(const uint8_t* src_y,
-                          const uint8_t* src_u,
-                          const uint8_t* src_v,
-                          const uint8_t* src_a,
-                          uint8_t* rgb_buf,
-                          const struct YuvConstants* yuvconstants,
-                          int width) {
-  int x;
-  for (x = 0; x < width - 1; x += 2) {
-    uint8_t u = (src_u[0] + src_u[1] + 1) >> 1;
-    uint8_t v = (src_v[0] + src_v[1] + 1) >> 1;
-    YuvPixel(src_y[0], u, v, rgb_buf + 0, rgb_buf + 1, rgb_buf + 2,
-             yuvconstants);
-    rgb_buf[3] = src_a[0];
-    YuvPixel(src_y[1], u, v, rgb_buf + 4, rgb_buf + 5, rgb_buf + 6,
-             yuvconstants);
-    rgb_buf[7] = src_a[1];
-    src_y += 2;
-    src_u += 2;
-    src_v += 2;
-    src_a += 2;
-    rgb_buf += 8;  // Advance 2 pixels.
-  }
-  if (width & 1) {
-    YuvPixel(src_y[0], src_u[0], src_v[0], rgb_buf + 0, rgb_buf + 1,
-             rgb_buf + 2, yuvconstants);
-    rgb_buf[3] = src_a[0];
-  }
-}
-#else
 void I444AlphaToARGBRow_C(const uint8_t* src_y,
                           const uint8_t* src_u,
                           const uint8_t* src_v,
@@ -2382,7 +2202,6 @@ void I444AlphaToARGBRow_C(const uint8_t* src_y,
     rgb_buf += 4;  // Advance 1 pixel.
   }
 }
-#endif
 
 void I422AlphaToARGBRow_C(const uint8_t* src_y,
                           const uint8_t* src_u,
@@ -2933,6 +2752,105 @@ void MergeARGBRow_C(const uint8_t* src_r,
   }
 }
 
+void MergeXR30Row_C(const uint16_t* src_r,
+                    const uint16_t* src_g,
+                    const uint16_t* src_b,
+                    uint8_t* dst_ar30,
+                    int depth,
+                    int width) {
+  assert(depth >= 10);
+  assert(depth <= 16);
+  int x;
+  int shift = depth - 10;
+  uint32_t* dst_ar30_32 = (uint32_t*)dst_ar30;
+  for (x = 0; x < width; ++x) {
+    uint32_t r = clamp1023(src_r[x] >> shift);
+    uint32_t g = clamp1023(src_g[x] >> shift);
+    uint32_t b = clamp1023(src_b[x] >> shift);
+    dst_ar30_32[x] = b | (g << 10) | (r << 20) | 0xc0000000;
+  }
+}
+
+void MergeAR64Row_C(const uint16_t* src_r,
+                    const uint16_t* src_g,
+                    const uint16_t* src_b,
+                    const uint16_t* src_a,
+                    uint16_t* dst_ar64,
+                    int depth,
+                    int width) {
+  assert(depth >= 1);
+  assert(depth <= 16);
+  int x;
+  int shift = 16 - depth;
+  int max = (1 << depth) - 1;
+  for (x = 0; x < width; ++x) {
+    dst_ar64[0] = ClampMax(src_b[x], max) << shift;
+    dst_ar64[1] = ClampMax(src_g[x], max) << shift;
+    dst_ar64[2] = ClampMax(src_r[x], max) << shift;
+    dst_ar64[3] = ClampMax(src_a[x], max) << shift;
+    dst_ar64 += 4;
+  }
+}
+
+void MergeARGB16To8Row_C(const uint16_t* src_r,
+                         const uint16_t* src_g,
+                         const uint16_t* src_b,
+                         const uint16_t* src_a,
+                         uint8_t* dst_argb,
+                         int depth,
+                         int width) {
+  assert(depth >= 8);
+  assert(depth <= 16);
+  int x;
+  int shift = depth - 8;
+  for (x = 0; x < width; ++x) {
+    dst_argb[0] = clamp255(src_b[x] >> shift);
+    dst_argb[1] = clamp255(src_g[x] >> shift);
+    dst_argb[2] = clamp255(src_r[x] >> shift);
+    dst_argb[3] = clamp255(src_a[x] >> shift);
+    dst_argb += 4;
+  }
+}
+
+void MergeXR64Row_C(const uint16_t* src_r,
+                    const uint16_t* src_g,
+                    const uint16_t* src_b,
+                    uint16_t* dst_ar64,
+                    int depth,
+                    int width) {
+  assert(depth >= 1);
+  assert(depth <= 16);
+  int x;
+  int shift = 16 - depth;
+  int max = (1 << depth) - 1;
+  for (x = 0; x < width; ++x) {
+    dst_ar64[0] = ClampMax(src_b[x], max) << shift;
+    dst_ar64[1] = ClampMax(src_g[x], max) << shift;
+    dst_ar64[2] = ClampMax(src_r[x], max) << shift;
+    dst_ar64[3] = 0xffff;
+    dst_ar64 += 4;
+  }
+}
+
+void MergeXRGB16To8Row_C(const uint16_t* src_r,
+                         const uint16_t* src_g,
+                         const uint16_t* src_b,
+                         uint8_t* dst_argb,
+                         int depth,
+                         int width) {
+  assert(depth >= 8);
+  assert(depth <= 16);
+  int x;
+  int shift = depth - 8;
+  for (x = 0; x < width; ++x) {
+    dst_argb[0] = clamp255(src_b[x] >> shift);
+    dst_argb[1] = clamp255(src_g[x] >> shift);
+    dst_argb[2] = clamp255(src_r[x] >> shift);
+    dst_argb[3] = 0xff;
+    dst_argb += 4;
+  }
+}
+
 void SplitXRGBRow_C(const uint8_t* src_argb,
                     uint8_t* dst_r,
                     uint8_t* dst_g,
@@ -2969,6 +2887,8 @@ void MergeUVRow_16_C(const uint16_t* src_u,
                      int depth,
                      int width) {
   int shift = 16 - depth;
+  assert(depth >= 8);
+  assert(depth <= 16);
   int x;
   for (x = 0; x < width; ++x) {
     dst_uv[0] = src_u[x] << shift;
@@ -2985,6 +2905,8 @@ void SplitUVRow_16_C(const uint16_t* src_uv,
                      int width) {
   int shift = 16 - depth;
   int x;
+  assert(depth >= 8);
+  assert(depth <= 16);
   for (x = 0; x < width; ++x) {
     dst_u[x] = src_uv[0] >> shift;
     dst_v[x] = src_uv[1] >> shift;
@@ -3022,6 +2944,9 @@ void Convert16To8Row_C(const uint16_t* src_y,
                        int scale,
                        int width) {
   int x;
+  assert(scale >= 256);
+  assert(scale <= 32768);
+
   for (x = 0; x < width; ++x) {
     dst_y[x] = clamp255((src_y[x] * scale) >> 16);
   }
@@ -3155,19 +3080,19 @@ void UYVYToYRow_C(const uint8_t* src_uyvy, uint8_t* dst_y, int width) {
 
 #define BLEND(f, b, a) clamp255((((256 - a) * b) >> 8) + f)
 
-// Blend src_argb0 over src_argb1 and store to dst_argb.
-// dst_argb may be src_argb0 or src_argb1.
+// Blend src_argb over src_argb1 and store to dst_argb.
+// dst_argb may be src_argb or src_argb1.
 // This code mimics the SSSE3 version for better testability.
-void ARGBBlendRow_C(const uint8_t* src_argb0,
+void ARGBBlendRow_C(const uint8_t* src_argb,
                     const uint8_t* src_argb1,
                     uint8_t* dst_argb,
                     int width) {
   int x;
   for (x = 0; x < width - 1; x += 2) {
-    uint32_t fb = src_argb0[0];
-    uint32_t fg = src_argb0[1];
-    uint32_t fr = src_argb0[2];
-    uint32_t a = src_argb0[3];
+    uint32_t fb = src_argb[0];
+    uint32_t fg = src_argb[1];
+    uint32_t fr = src_argb[2];
+    uint32_t a = src_argb[3];
     uint32_t bb = src_argb1[0];
     uint32_t bg = src_argb1[1];
     uint32_t br = src_argb1[2];
@@ -3176,10 +3101,10 @@ void ARGBBlendRow_C(const uint8_t* src_argb0,
     dst_argb[2] = BLEND(fr, br, a);
     dst_argb[3] = 255u;
 
-    fb = src_argb0[4 + 0];
-    fg = src_argb0[4 + 1];
-    fr = src_argb0[4 + 2];
-    a = src_argb0[4 + 3];
+    fb = src_argb[4 + 0];
+    fg = src_argb[4 + 1];
+    fr = src_argb[4 + 2];
+    a = src_argb[4 + 3];
     bb = src_argb1[4 + 0];
     bg = src_argb1[4 + 1];
     br = src_argb1[4 + 2];
@@ -3187,16 +3112,16 @@ void ARGBBlendRow_C(const uint8_t* src_argb0,
     dst_argb[4 + 1] = BLEND(fg, bg, a);
     dst_argb[4 + 2] = BLEND(fr, br, a);
     dst_argb[4 + 3] = 255u;
-    src_argb0 += 8;
+    src_argb += 8;
     src_argb1 += 8;
     dst_argb += 8;
   }
 
   if (width & 1) {
-    uint32_t fb = src_argb0[0];
-    uint32_t fg = src_argb0[1];
-    uint32_t fr = src_argb0[2];
-    uint32_t a = src_argb0[3];
+    uint32_t fb = src_argb[0];
+    uint32_t fg = src_argb[1];
+    uint32_t fr = src_argb[2];
+    uint32_t a = src_argb[3];
     uint32_t bb = src_argb1[0];
     uint32_t bg = src_argb1[1];
     uint32_t br = src_argb1[2];
@@ -3721,7 +3646,7 @@ void ARGBCopyYToAlphaRow_C(const uint8_t* src, uint8_t* dst, int width) {
 // Maximum temporary width for wrappers to process at a time, in pixels.
 #define MAXTWIDTH 2048
 
-#if !(defined(_MSC_VER) && defined(_M_IX86)) && \
+#if !(defined(_MSC_VER) && !defined(__clang__) && defined(_M_IX86)) && \
     defined(HAS_I422TORGB565ROW_SSSE3)
 // row_win.cc has asm version, but GCC uses 2 step wrapper.
 void I422ToRGB565Row_SSSE3(const uint8_t* src_y,
@@ -4188,13 +4113,14 @@ void NV21ToYUV24Row_C(const uint8_t* src_y,
 }
 
 // Filter 2 rows of AYUV UV's (444) into UV (420).
+// AYUV is VUYA in memory.  UV for NV12 is UV order in memory.
 void AYUVToUVRow_C(const uint8_t* src_ayuv,
                    int src_stride_ayuv,
                    uint8_t* dst_uv,
                    int width) {
   // Output a row of UV values, filtering 2x2 rows of AYUV.
   int x;
-  for (x = 0; x < width; x += 2) {
+  for (x = 0; x < width - 1; x += 2) {
     dst_uv[0] = (src_ayuv[1] + src_ayuv[5] + src_ayuv[src_stride_ayuv + 1] +
                  src_ayuv[src_stride_ayuv + 5] + 2) >>
                 2;
@@ -4205,12 +4131,8 @@ void AYUVToUVRow_C(const uint8_t* src_ayuv,
     dst_uv += 2;
   }
   if (width & 1) {
-    dst_uv[0] = (src_ayuv[0] + src_ayuv[0] + src_ayuv[src_stride_ayuv + 0] +
-                 src_ayuv[src_stride_ayuv + 0] + 2) >>
-                2;
-    dst_uv[1] = (src_ayuv[1] + src_ayuv[1] + src_ayuv[src_stride_ayuv + 1] +
-                 src_ayuv[src_stride_ayuv + 1] + 2) >>
-                2;
+    dst_uv[0] = (src_ayuv[1] + src_ayuv[src_stride_ayuv + 1] + 1) >> 1;
+    dst_uv[1] = (src_ayuv[0] + src_ayuv[src_stride_ayuv + 0] + 1) >> 1;
   }
 }
 
@@ -4221,7 +4143,7 @@ void AYUVToVURow_C(const uint8_t* src_ayuv,
                    int width) {
   // Output a row of VU values, filtering 2x2 rows of AYUV.
   int x;
-  for (x = 0; x < width; x += 2) {
+  for (x = 0; x < width - 1; x += 2) {
     dst_vu[0] = (src_ayuv[0] + src_ayuv[4] + src_ayuv[src_stride_ayuv + 0] +
                  src_ayuv[src_stride_ayuv + 4] + 2) >>
                 2;
@@ -4232,12 +4154,8 @@ void AYUVToVURow_C(const uint8_t* src_ayuv,
     dst_vu += 2;
   }
   if (width & 1) {
-    dst_vu[0] = (src_ayuv[0] + src_ayuv[0] + src_ayuv[src_stride_ayuv + 0] +
-                 src_ayuv[src_stride_ayuv + 0] + 2) >>
-                2;
-    dst_vu[1] = (src_ayuv[1] + src_ayuv[1] + src_ayuv[src_stride_ayuv + 1] +
-                 src_ayuv[src_stride_ayuv + 1] + 2) >>
-                2;
+    dst_vu[0] = (src_ayuv[0] + src_ayuv[src_stride_ayuv + 0] + 1) >> 1;
+    dst_vu[1] = (src_ayuv[1] + src_ayuv[src_stride_ayuv + 1] + 1) >> 1;
   }
 }
 
diff --git a/source/row_gcc.cc b/source/row_gcc.cc
index f4d9978b..43e4c710 100644
--- a/source/row_gcc.cc
+++ b/source/row_gcc.cc
@@ -17,7 +17,7 @@ extern "C" {
 
 // This module is for GCC x86 and x64.
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 
 #if defined(HAS_ARGBTOYROW_SSSE3) || defined(HAS_ARGBGRAYROW_SSSE3)
 
@@ -1078,6 +1078,222 @@ void ABGRToAR30Row_AVX2(const uint8_t* src, uint8_t* dst, int width) {
 }
 #endif
 
+static const uvec8 kShuffleARGBToABGR = {2,  1, 0, 3,  6,  5,  4,  7,
+                                         10, 9, 8, 11, 14, 13, 12, 15};
+
+static const uvec8 kShuffleARGBToAB64Lo = {2, 2, 1, 1, 0, 0, 3, 3,
+                                           6, 6, 5, 5, 4, 4, 7, 7};
+static const uvec8 kShuffleARGBToAB64Hi = {10, 10, 9,  9,  8,  8,  11, 11,
+                                           14, 14, 13, 13, 12, 12, 15, 15};
+
+void ARGBToAR64Row_SSSE3(const uint8_t* src_argb,
+                         uint16_t* dst_ar64,
+                         int width) {
+  asm volatile(
+
+      LABELALIGN
+      "1:                                        \n"
+      "movdqu      (%0),%%xmm0                   \n"
+      "movdqa      %%xmm0,%%xmm1                 \n"
+      "punpcklbw   %%xmm0,%%xmm0                 \n"
+      "punpckhbw   %%xmm1,%%xmm1                 \n"
+      "movdqu      %%xmm0,(%1)                   \n"
+      "movdqu      %%xmm1,0x10(%1)               \n"
+      "lea         0x10(%0),%0                   \n"
+      "lea         0x20(%1),%1                   \n"
+      "sub         $0x4,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_argb),  // %0
+        "+r"(dst_ar64),  // %1
+        "+r"(width)      // %2
+      :
+      : "memory", "cc", "xmm0", "xmm1");
+}
+
+void ARGBToAB64Row_SSSE3(const uint8_t* src_argb,
+                         uint16_t* dst_ab64,
+                         int width) {
+  asm volatile(
+
+      "movdqa      %3,%%xmm2                     \n"
+      "movdqa      %4,%%xmm3                     \n" LABELALIGN
+      "1:                                        \n"
+      "movdqu      (%0),%%xmm0                   \n"
+      "movdqa      %%xmm0,%%xmm1                 \n"
+      "pshufb      %%xmm2,%%xmm0                 \n"
+      "pshufb      %%xmm3,%%xmm1                 \n"
+      "movdqu      %%xmm0,(%1)                   \n"
+      "movdqu      %%xmm1,0x10(%1)               \n"
+      "lea         0x10(%0),%0                   \n"
+      "lea         0x20(%1),%1                   \n"
+      "sub         $0x4,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_argb),             // %0
+        "+r"(dst_ab64),             // %1
+        "+r"(width)                 // %2
+      : "m"(kShuffleARGBToAB64Lo),  // %3
+        "m"(kShuffleARGBToAB64Hi)   // %4
+      : "memory", "cc", "xmm0", "xmm1", "xmm2");
+}
+
+void AR64ToARGBRow_SSSE3(const uint16_t* src_ar64,
+                         uint8_t* dst_argb,
+                         int width) {
+  asm volatile(
+
+      LABELALIGN
+      "1:                                        \n"
+      "movdqu      (%0),%%xmm0                   \n"
+      "movdqu      0x10(%0),%%xmm1               \n"
+      "psrlw       $8,%%xmm0                     \n"
+      "psrlw       $8,%%xmm1                     \n"
+      "packuswb    %%xmm1,%%xmm0                 \n"
+      "movdqu      %%xmm0,(%1)                   \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x10(%1),%1                   \n"
+      "sub         $0x4,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_ar64),  // %0
+        "+r"(dst_argb),  // %1
+        "+r"(width)      // %2
+      :
+      : "memory", "cc", "xmm0", "xmm1");
+}
+
+void AB64ToARGBRow_SSSE3(const uint16_t* src_ab64,
+                         uint8_t* dst_argb,
+                         int width) {
+  asm volatile(
+
+      "movdqa      %3,%%xmm2                     \n" LABELALIGN
+      "1:                                        \n"
+      "movdqu      (%0),%%xmm0                   \n"
+      "movdqu      0x10(%0),%%xmm1               \n"
+      "psrlw       $8,%%xmm0                     \n"
+      "psrlw       $8,%%xmm1                     \n"
+      "packuswb    %%xmm1,%%xmm0                 \n"
+      "pshufb      %%xmm2,%%xmm0                 \n"
+      "movdqu      %%xmm0,(%1)                   \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x10(%1),%1                   \n"
+      "sub         $0x4,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_ab64),          // %0
+        "+r"(dst_argb),          // %1
+        "+r"(width)              // %2
+      : "m"(kShuffleARGBToABGR)  // %3
+      : "memory", "cc", "xmm0", "xmm1", "xmm2");
+}
+
+#ifdef HAS_ARGBTOAR64ROW_AVX2
+void ARGBToAR64Row_AVX2(const uint8_t* src_argb,
+                        uint16_t* dst_ar64,
+                        int width) {
+  asm volatile(
+
+      LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"
+      "vpermq      $0xd8,%%ymm0,%%ymm0           \n"
+      "vpunpckhbw  %%ymm0,%%ymm0,%%ymm1          \n"
+      "vpunpcklbw  %%ymm0,%%ymm0,%%ymm0          \n"
+      "vmovdqu     %%ymm0,(%1)                   \n"
+      "vmovdqu     %%ymm1,0x20(%1)               \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x40(%1),%1                   \n"
+      "sub         $0x8,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_argb),  // %0
+        "+r"(dst_ar64),  // %1
+        "+r"(width)      // %2
+      :
+      : "memory", "cc", "xmm0", "xmm1");
+}
+#endif
+
+#ifdef HAS_ARGBTOAB64ROW_AVX2
+void ARGBToAB64Row_AVX2(const uint8_t* src_argb,
+                        uint16_t* dst_ab64,
+                        int width) {
+  asm volatile(
+
+      "vbroadcastf128 %3,%%ymm2                  \n"
+      "vbroadcastf128 %4,%%ymm3                  \n" LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"
+      "vpermq      $0xd8,%%ymm0,%%ymm0           \n"
+      "vpshufb     %%ymm3,%%ymm0,%%ymm1          \n"
+      "vpshufb     %%ymm2,%%ymm0,%%ymm0          \n"
+      "vmovdqu     %%ymm0,(%1)                   \n"
+      "vmovdqu     %%ymm1,0x20(%1)               \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x40(%1),%1                   \n"
+      "sub         $0x8,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_argb),             // %0
+        "+r"(dst_ab64),             // %1
+        "+r"(width)                 // %2
+      : "m"(kShuffleARGBToAB64Lo),  // %3
+        "m"(kShuffleARGBToAB64Hi)   // %3
+      : "memory", "cc", "xmm0", "xmm1", "xmm2");
+}
+#endif
+
+#ifdef HAS_AR64TOARGBROW_AVX2
+void AR64ToARGBRow_AVX2(const uint16_t* src_ar64,
+                        uint8_t* dst_argb,
+                        int width) {
+  asm volatile(
+
+      LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"
+      "vmovdqu     0x20(%0),%%ymm1               \n"
+      "vpsrlw      $8,%%ymm0,%%ymm0              \n"
+      "vpsrlw      $8,%%ymm1,%%ymm1              \n"
+      "vpackuswb   %%ymm1,%%ymm0,%%ymm0          \n"
+      "vpermq      $0xd8,%%ymm0,%%ymm0           \n"
+      "vmovdqu     %%ymm0,(%1)                   \n"
+      "lea         0x40(%0),%0                   \n"
+      "lea         0x20(%1),%1                   \n"
+      "sub         $0x8,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_ar64),  // %0
+        "+r"(dst_argb),  // %1
+        "+r"(width)      // %2
+      :
+      : "memory", "cc", "xmm0", "xmm1");
+}
+#endif
+
+#ifdef HAS_AB64TOARGBROW_AVX2
+void AB64ToARGBRow_AVX2(const uint16_t* src_ab64,
+                        uint8_t* dst_argb,
+                        int width) {
+  asm volatile(
+
+      "vbroadcastf128 %3,%%ymm2                  \n" LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"
+      "vmovdqu     0x20(%0),%%ymm1               \n"
+      "vpsrlw      $8,%%ymm0,%%ymm0              \n"
+      "vpsrlw      $8,%%ymm1,%%ymm1              \n"
+      "vpackuswb   %%ymm1,%%ymm0,%%ymm0          \n"
+      "vpermq      $0xd8,%%ymm0,%%ymm0           \n"
+      "vpshufb     %%ymm2,%%ymm0,%%ymm0          \n"
+      "vmovdqu     %%ymm0,(%1)                   \n"
+      "lea         0x40(%0),%0                   \n"
+      "lea         0x20(%1),%1                   \n"
+      "sub         $0x8,%2                       \n"
+      "jg          1b                            \n"
+      : "+r"(src_ab64),          // %0
+        "+r"(dst_argb),          // %1
+        "+r"(width)              // %2
+      : "m"(kShuffleARGBToABGR)  // %3
+      : "memory", "cc", "xmm0", "xmm1", "xmm2");
+}
+#endif
+
 // clang-format off
 
 // TODO(mraptis): Consider passing R, G, B multipliers as parameter.
@@ -1290,7 +1506,7 @@ void RGBAToYJRow_AVX2(const uint8_t* src_rgba, uint8_t* dst_y, int width) {
 #endif  // HAS_RGBATOYJROW_AVX2
 
 #ifdef HAS_ARGBTOUVROW_SSSE3
-void ARGBToUVRow_SSSE3(const uint8_t* src_argb0,
+void ARGBToUVRow_SSSE3(const uint8_t* src_argb,
                        int src_stride_argb,
                        uint8_t* dst_u,
                        uint8_t* dst_v,
@@ -1342,7 +1558,7 @@ void ARGBToUVRow_SSSE3(const uint8_t* src_argb0,
       "lea         0x8(%1),%1                    \n"
       "sub         $0x10,%3                      \n"
       "jg          1b                            \n"
-      : "+r"(src_argb0),                   // %0
+      : "+r"(src_argb),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1359,7 +1575,7 @@ void ARGBToUVRow_SSSE3(const uint8_t* src_argb0,
 static const lvec8 kShufARGBToUV_AVX = {
     0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13, 6, 7, 14, 15,
     0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13, 6, 7, 14, 15};
-void ARGBToUVRow_AVX2(const uint8_t* src_argb0,
+void ARGBToUVRow_AVX2(const uint8_t* src_argb,
                       int src_stride_argb,
                       uint8_t* dst_u,
                       uint8_t* dst_v,
@@ -1407,7 +1623,7 @@ void ARGBToUVRow_AVX2(const uint8_t* src_argb0,
       "sub         $0x20,%3                      \n"
       "jg          1b                            \n"
       "vzeroupper                                \n"
-      : "+r"(src_argb0),                   // %0
+      : "+r"(src_argb),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1422,7 +1638,7 @@ void ARGBToUVRow_AVX2(const uint8_t* src_argb0,
 #endif  // HAS_ARGBTOUVROW_AVX2
 
 #ifdef HAS_ABGRTOUVROW_AVX2
-void ABGRToUVRow_AVX2(const uint8_t* src_abgr0,
+void ABGRToUVRow_AVX2(const uint8_t* src_abgr,
                       int src_stride_abgr,
                       uint8_t* dst_u,
                       uint8_t* dst_v,
@@ -1470,7 +1686,7 @@ void ABGRToUVRow_AVX2(const uint8_t* src_abgr0,
       "sub         $0x20,%3                      \n"
       "jg          1b                            \n"
       "vzeroupper                                \n"
-      : "+r"(src_abgr0),                   // %0
+      : "+r"(src_abgr),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1485,7 +1701,7 @@ void ABGRToUVRow_AVX2(const uint8_t* src_abgr0,
 #endif  // HAS_ABGRTOUVROW_AVX2
 
 #ifdef HAS_ARGBTOUVJROW_AVX2
-void ARGBToUVJRow_AVX2(const uint8_t* src_argb0,
+void ARGBToUVJRow_AVX2(const uint8_t* src_argb,
                        int src_stride_argb,
                        uint8_t* dst_u,
                        uint8_t* dst_v,
@@ -1534,7 +1750,7 @@ void ARGBToUVJRow_AVX2(const uint8_t* src_argb0,
       "sub         $0x20,%3                      \n"
       "jg          1b                            \n"
       "vzeroupper                                \n"
-      : "+r"(src_argb0),                   // %0
+      : "+r"(src_argb),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1549,7 +1765,7 @@ void ARGBToUVJRow_AVX2(const uint8_t* src_argb0,
 #endif  // HAS_ARGBTOUVJROW_AVX2
 
 #ifdef HAS_ARGBTOUVJROW_SSSE3
-void ARGBToUVJRow_SSSE3(const uint8_t* src_argb0,
+void ARGBToUVJRow_SSSE3(const uint8_t* src_argb,
                         int src_stride_argb,
                         uint8_t* dst_u,
                         uint8_t* dst_v,
@@ -1602,7 +1818,7 @@ void ARGBToUVJRow_SSSE3(const uint8_t* src_argb0,
       "lea         0x8(%1),%1                    \n"
       "sub         $0x10,%3                      \n"
       "jg          1b                            \n"
-      : "+r"(src_argb0),                   // %0
+      : "+r"(src_argb),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1689,7 +1905,7 @@ void BGRAToYRow_SSSE3(const uint8_t* src_bgra, uint8_t* dst_y, int width) {
         "xmm7");
 }
 
-void BGRAToUVRow_SSSE3(const uint8_t* src_bgra0,
+void BGRAToUVRow_SSSE3(const uint8_t* src_bgra,
                        int src_stride_bgra,
                        uint8_t* dst_u,
                        uint8_t* dst_v,
@@ -1741,7 +1957,7 @@ void BGRAToUVRow_SSSE3(const uint8_t* src_bgra0,
       "lea         0x8(%1),%1                    \n"
       "sub         $0x10,%3                      \n"
       "jg          1b                            \n"
-      : "+r"(src_bgra0),                   // %0
+      : "+r"(src_bgra),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1786,7 +2002,7 @@ void RGBAToYRow_SSSE3(const uint8_t* src_rgba, uint8_t* dst_y, int width) {
         "xmm7");
 }
 
-void ABGRToUVRow_SSSE3(const uint8_t* src_abgr0,
+void ABGRToUVRow_SSSE3(const uint8_t* src_abgr,
                        int src_stride_abgr,
                        uint8_t* dst_u,
                        uint8_t* dst_v,
@@ -1838,7 +2054,7 @@ void ABGRToUVRow_SSSE3(const uint8_t* src_abgr0,
       "lea         0x8(%1),%1                    \n"
       "sub         $0x10,%3                      \n"
       "jg          1b                            \n"
-      : "+r"(src_abgr0),                   // %0
+      : "+r"(src_abgr),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1849,7 +2065,7 @@ void ABGRToUVRow_SSSE3(const uint8_t* src_abgr0,
       : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm6", "xmm7");
 }
 
-void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
+void RGBAToUVRow_SSSE3(const uint8_t* src_rgba,
                        int src_stride_rgba,
                        uint8_t* dst_u,
                        uint8_t* dst_v,
@@ -1901,7 +2117,7 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
       "lea         0x8(%1),%1                    \n"
       "sub         $0x10,%3                      \n"
       "jg          1b                            \n"
-      : "+r"(src_rgba0),                   // %0
+      : "+r"(src_rgba),                    // %0
         "+r"(dst_u),                       // %1
         "+r"(dst_v),                       // %2
         "+rm"(width)                       // %3
@@ -1916,21 +2132,21 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
 
 // Read 8 UV from 444
 #define READYUV444                                                \
-  "movq       (%[u_buf]),%%xmm0                               \n" \
+  "movq       (%[u_buf]),%%xmm3                               \n" \
   "movq       0x00(%[u_buf],%[v_buf],1),%%xmm1                \n" \
   "lea        0x8(%[u_buf]),%[u_buf]                          \n" \
-  "punpcklbw  %%xmm1,%%xmm0                                   \n" \
+  "punpcklbw  %%xmm1,%%xmm3                                   \n" \
   "movq       (%[y_buf]),%%xmm4                               \n" \
   "punpcklbw  %%xmm4,%%xmm4                                   \n" \
   "lea        0x8(%[y_buf]),%[y_buf]                          \n"
 
 // Read 4 UV from 422, upsample to 8 UV
 #define READYUV422                                                \
-  "movd       (%[u_buf]),%%xmm0                               \n" \
+  "movd       (%[u_buf]),%%xmm3                               \n" \
   "movd       0x00(%[u_buf],%[v_buf],1),%%xmm1                \n" \
   "lea        0x4(%[u_buf]),%[u_buf]                          \n" \
-  "punpcklbw  %%xmm1,%%xmm0                                   \n" \
-  "punpcklwd  %%xmm0,%%xmm0                                   \n" \
+  "punpcklbw  %%xmm1,%%xmm3                                   \n" \
+  "punpcklwd  %%xmm3,%%xmm3                                   \n" \
   "movq       (%[y_buf]),%%xmm4                               \n" \
   "punpcklbw  %%xmm4,%%xmm4                                   \n" \
   "lea        0x8(%[y_buf]),%[y_buf]                          \n"
@@ -1940,25 +2156,25 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
 // TODO(fbarchard): Consider pmulhuw to replace psraw
 // TODO(fbarchard): Consider pmullw to replace psllw and allow different bits.
 #define READYUV210                                                \
-  "movq       (%[u_buf]),%%xmm0                               \n" \
+  "movq       (%[u_buf]),%%xmm3                               \n" \
   "movq       0x00(%[u_buf],%[v_buf],1),%%xmm1                \n" \
   "lea        0x8(%[u_buf]),%[u_buf]                          \n" \
-  "punpcklwd  %%xmm1,%%xmm0                                   \n" \
-  "psraw      $2,%%xmm0                                       \n" \
-  "packuswb   %%xmm0,%%xmm0                                   \n" \
-  "punpcklwd  %%xmm0,%%xmm0                                   \n" \
+  "punpcklwd  %%xmm1,%%xmm3                                   \n" \
+  "psraw      $2,%%xmm3                                       \n" \
+  "packuswb   %%xmm3,%%xmm3                                   \n" \
+  "punpcklwd  %%xmm3,%%xmm3                                   \n" \
   "movdqu     (%[y_buf]),%%xmm4                               \n" \
   "psllw      $6,%%xmm4                                       \n" \
   "lea        0x10(%[y_buf]),%[y_buf]                         \n"
 
 #define READYUVA210                                               \
-  "movq       (%[u_buf]),%%xmm0                               \n" \
+  "movq       (%[u_buf]),%%xmm3                               \n" \
   "movq       0x00(%[u_buf],%[v_buf],1),%%xmm1                \n" \
   "lea        0x8(%[u_buf]),%[u_buf]                          \n" \
-  "punpcklwd  %%xmm1,%%xmm0                                   \n" \
-  "psraw      $2,%%xmm0                                       \n" \
-  "packuswb   %%xmm0,%%xmm0                                   \n" \
-  "punpcklwd  %%xmm0,%%xmm0                                   \n" \
+  "punpcklwd  %%xmm1,%%xmm3                                   \n" \
+  "psraw      $2,%%xmm3                                       \n" \
+  "packuswb   %%xmm3,%%xmm3                                   \n" \
+  "punpcklwd  %%xmm3,%%xmm3                                   \n" \
   "movdqu     (%[y_buf]),%%xmm4                               \n" \
   "psllw      $6,%%xmm4                                       \n" \
   "lea        0x10(%[y_buf]),%[y_buf]                         \n" \
@@ -1969,30 +2185,30 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
 
 // Read 8 UV from 444 10 bit
 #define READYUV410                                                \
-  "movdqu     (%[u_buf]),%%xmm0                               \n" \
+  "movdqu     (%[u_buf]),%%xmm3                               \n" \
   "movdqu     0x00(%[u_buf],%[v_buf],1),%%xmm2                \n" \
   "lea        0x10(%[u_buf]),%[u_buf]                         \n" \
-  "psraw      $2,%%xmm0                                       \n" \
+  "psraw      $2,%%xmm3                                       \n" \
   "psraw      $2,%%xmm2                                       \n" \
-  "movdqa     %%xmm0,%%xmm1                                   \n" \
-  "punpcklwd  %%xmm2,%%xmm0                                   \n" \
+  "movdqa     %%xmm3,%%xmm1                                   \n" \
+  "punpcklwd  %%xmm2,%%xmm3                                   \n" \
   "punpckhwd  %%xmm2,%%xmm1                                   \n" \
-  "packuswb   %%xmm1,%%xmm0                                   \n" \
+  "packuswb   %%xmm1,%%xmm3                                   \n" \
   "movdqu     (%[y_buf]),%%xmm4                               \n" \
   "psllw      $6,%%xmm4                                       \n" \
   "lea        0x10(%[y_buf]),%[y_buf]                         \n"
 
 // Read 8 UV from 444 10 bit.  With 8 Alpha.
 #define READYUVA410                                               \
-  "movdqu     (%[u_buf]),%%xmm0                               \n" \
+  "movdqu     (%[u_buf]),%%xmm3                               \n" \
   "movdqu     0x00(%[u_buf],%[v_buf],1),%%xmm2                \n" \
   "lea        0x10(%[u_buf]),%[u_buf]                         \n" \
-  "psraw      $2,%%xmm0                                       \n" \
+  "psraw      $2,%%xmm3                                       \n" \
   "psraw      $2,%%xmm2                                       \n" \
-  "movdqa     %%xmm0,%%xmm1                                   \n" \
-  "punpcklwd  %%xmm2,%%xmm0                                   \n" \
+  "movdqa     %%xmm3,%%xmm1                                   \n" \
+  "punpcklwd  %%xmm2,%%xmm3                                   \n" \
   "punpckhwd  %%xmm2,%%xmm1                                   \n" \
-  "packuswb   %%xmm1,%%xmm0                                   \n" \
+  "packuswb   %%xmm1,%%xmm3                                   \n" \
   "movdqu     (%[y_buf]),%%xmm4                               \n" \
   "psllw      $0x6,%%xmm4                                     \n" \
   "lea        0x10(%[y_buf]),%[y_buf]                         \n" \
@@ -2003,24 +2219,24 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
 
 // Read 4 UV from 422 12 bit, upsample to 8 UV
 #define READYUV212                                                \
-  "movq       (%[u_buf]),%%xmm0                               \n" \
+  "movq       (%[u_buf]),%%xmm3                               \n" \
   "movq       0x00(%[u_buf],%[v_buf],1),%%xmm1                \n" \
   "lea        0x8(%[u_buf]),%[u_buf]                          \n" \
-  "punpcklwd  %%xmm1,%%xmm0                                   \n" \
-  "psraw      $0x4,%%xmm0                                     \n" \
-  "packuswb   %%xmm0,%%xmm0                                   \n" \
-  "punpcklwd  %%xmm0,%%xmm0                                   \n" \
+  "punpcklwd  %%xmm1,%%xmm3                                   \n" \
+  "psraw      $0x4,%%xmm3                                     \n" \
+  "packuswb   %%xmm3,%%xmm3                                   \n" \
+  "punpcklwd  %%xmm3,%%xmm3                                   \n" \
   "movdqu     (%[y_buf]),%%xmm4                               \n" \
   "psllw      $0x4,%%xmm4                                     \n" \
   "lea        0x10(%[y_buf]),%[y_buf]                         \n"
 
 // Read 4 UV from 422, upsample to 8 UV.  With 8 Alpha.
 #define READYUVA422                                               \
-  "movd       (%[u_buf]),%%xmm0                               \n" \
+  "movd       (%[u_buf]),%%xmm3                               \n" \
   "movd       0x00(%[u_buf],%[v_buf],1),%%xmm1                \n" \
   "lea        0x4(%[u_buf]),%[u_buf]                          \n" \
-  "punpcklbw  %%xmm1,%%xmm0                                   \n" \
-  "punpcklwd  %%xmm0,%%xmm0                                   \n" \
+  "punpcklbw  %%xmm1,%%xmm3                                   \n" \
+  "punpcklwd  %%xmm3,%%xmm3                                   \n" \
   "movq       (%[y_buf]),%%xmm4                               \n" \
   "punpcklbw  %%xmm4,%%xmm4                                   \n" \
   "lea        0x8(%[y_buf]),%[y_buf]                          \n" \
@@ -2029,10 +2245,10 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
 
 // Read 8 UV from 444.  With 8 Alpha.
 #define READYUVA444                                               \
-  "movq       (%[u_buf]),%%xmm0                               \n" \
+  "movq       (%[u_buf]),%%xmm3                               \n" \
   "movq       0x00(%[u_buf],%[v_buf],1),%%xmm1                \n" \
   "lea        0x8(%[u_buf]),%[u_buf]                          \n" \
-  "punpcklbw  %%xmm1,%%xmm0                                   \n" \
+  "punpcklbw  %%xmm1,%%xmm3                                   \n" \
   "movq       (%[y_buf]),%%xmm4                               \n" \
   "punpcklbw  %%xmm4,%%xmm4                                   \n" \
   "lea        0x8(%[y_buf]),%[y_buf]                          \n" \
@@ -2041,18 +2257,18 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
 
 // Read 4 UV from NV12, upsample to 8 UV
 #define READNV12                                                  \
-  "movq       (%[uv_buf]),%%xmm0                              \n" \
+  "movq       (%[uv_buf]),%%xmm3                              \n" \
   "lea        0x8(%[uv_buf]),%[uv_buf]                        \n" \
-  "punpcklwd  %%xmm0,%%xmm0                                   \n" \
+  "punpcklwd  %%xmm3,%%xmm3                                   \n" \
   "movq       (%[y_buf]),%%xmm4                               \n" \
   "punpcklbw  %%xmm4,%%xmm4                                   \n" \
   "lea        0x8(%[y_buf]),%[y_buf]                          \n"
 
 // Read 4 VU from NV21, upsample to 8 UV
 #define READNV21                                                  \
-  "movq       (%[vu_buf]),%%xmm0                              \n" \
+  "movq       (%[vu_buf]),%%xmm3                              \n" \
   "lea        0x8(%[vu_buf]),%[vu_buf]                        \n" \
-  "pshufb     %[kShuffleNV21], %%xmm0                         \n" \
+  "pshufb     %[kShuffleNV21], %%xmm3                         \n" \
   "movq       (%[y_buf]),%%xmm4                               \n" \
   "punpcklbw  %%xmm4,%%xmm4                                   \n" \
   "lea        0x8(%[y_buf]),%[y_buf]                          \n"
@@ -2061,89 +2277,92 @@ void RGBAToUVRow_SSSE3(const uint8_t* src_rgba0,
 #define READYUY2                                                  \
   "movdqu     (%[yuy2_buf]),%%xmm4                            \n" \
   "pshufb     %[kShuffleYUY2Y], %%xmm4                        \n" \
-  "movdqu     (%[yuy2_buf]),%%xmm0                            \n" \
-  "pshufb     %[kShuffleYUY2UV], %%xmm0                       \n" \
+  "movdqu     (%[yuy2_buf]),%%xmm3                            \n" \
+  "pshufb     %[kShuffleYUY2UV], %%xmm3                       \n" \
   "lea        0x10(%[yuy2_buf]),%[yuy2_buf]                   \n"
 
 // Read 4 UYVY with 8 Y and update 4 UV to 8 UV.
 #define READUYVY                                                  \
   "movdqu     (%[uyvy_buf]),%%xmm4                            \n" \
   "pshufb     %[kShuffleUYVYY], %%xmm4                        \n" \
-  "movdqu     (%[uyvy_buf]),%%xmm0                            \n" \
-  "pshufb     %[kShuffleUYVYUV], %%xmm0                       \n" \
+  "movdqu     (%[uyvy_buf]),%%xmm3                            \n" \
+  "pshufb     %[kShuffleUYVYUV], %%xmm3                       \n" \
   "lea        0x10(%[uyvy_buf]),%[uyvy_buf]                   \n"
 
 // Read 4 UV from P210, upsample to 8 UV
 #define READP210                                                  \
-  "movdqu     (%[uv_buf]),%%xmm0                              \n" \
+  "movdqu     (%[uv_buf]),%%xmm3                              \n" \
   "lea        0x10(%[uv_buf]),%[uv_buf]                       \n" \
-  "psrlw      $0x8,%%xmm0                                     \n" \
-  "packuswb   %%xmm0,%%xmm0                                   \n" \
-  "punpcklwd  %%xmm0,%%xmm0                                   \n" \
+  "psrlw      $0x8,%%xmm3                                     \n" \
+  "packuswb   %%xmm3,%%xmm3                                   \n" \
+  "punpcklwd  %%xmm3,%%xmm3                                   \n" \
   "movdqu     (%[y_buf]),%%xmm4                               \n" \
   "lea        0x10(%[y_buf]),%[y_buf]                         \n"
 
 // Read 8 UV from P410
 #define READP410                                                  \
-  "movdqu     (%[uv_buf]),%%xmm0                              \n" \
+  "movdqu     (%[uv_buf]),%%xmm3                              \n" \
   "movdqu     0x10(%[uv_buf]),%%xmm1                          \n" \
   "lea        0x20(%[uv_buf]),%[uv_buf]                       \n" \
-  "psrlw      $0x8,%%xmm0                                     \n" \
+  "psrlw      $0x8,%%xmm3                                     \n" \
   "psrlw      $0x8,%%xmm1                                     \n" \
-  "packuswb   %%xmm1,%%xmm0                                   \n" \
+  "packuswb   %%xmm1,%%xmm3                                   \n" \
   "movdqu     (%[y_buf]),%%xmm4                               \n" \
   "lea        0x10(%[y_buf]),%[y_buf]                         \n"
 
 #if defined(__x86_64__)
 #define YUVTORGB_SETUP(yuvconstants)                              \
+  "pcmpeqb    %%xmm13,%%xmm13                                 \n" \
   "movdqa     (%[yuvconstants]),%%xmm8                        \n" \
+  "pxor       %%xmm12,%%xmm12                                 \n" \
   "movdqa     32(%[yuvconstants]),%%xmm9                      \n" \
+  "psllw      $7,%%xmm13                                      \n" \
   "movdqa     64(%[yuvconstants]),%%xmm10                     \n" \
+  "pshufb     %%xmm12,%%xmm13                                 \n" \
   "movdqa     96(%[yuvconstants]),%%xmm11                     \n" \
-  "movdqa     128(%[yuvconstants]),%%xmm12                    \n" \
-  "movdqa     160(%[yuvconstants]),%%xmm13                    \n" \
-  "movdqa     192(%[yuvconstants]),%%xmm14                    \n"
+  "movdqa     128(%[yuvconstants]),%%xmm12                    \n"
+
 // Convert 8 pixels: 8 UV and 8 Y
 #define YUVTORGB16(yuvconstants)                                  \
-  "movdqa     %%xmm0,%%xmm1                                   \n" \
-  "movdqa     %%xmm0,%%xmm2                                   \n" \
-  "movdqa     %%xmm0,%%xmm3                                   \n" \
-  "movdqa     %%xmm11,%%xmm0                                  \n" \
-  "pmaddubsw  %%xmm8,%%xmm1                                   \n" \
-  "psubw      %%xmm1,%%xmm0                                   \n" \
-  "movdqa     %%xmm12,%%xmm1                                  \n" \
-  "pmaddubsw  %%xmm9,%%xmm2                                   \n" \
-  "psubw      %%xmm2,%%xmm1                                   \n" \
-  "movdqa     %%xmm13,%%xmm2                                  \n" \
-  "pmaddubsw  %%xmm10,%%xmm3                                  \n" \
-  "psubw      %%xmm3,%%xmm2                                   \n" \
-  "pmulhuw    %%xmm14,%%xmm4                                  \n" \
+  "psubb      %%xmm13,%%xmm3                                  \n" \
+  "pmulhuw    %%xmm11,%%xmm4                                  \n" \
+  "movdqa     %%xmm8,%%xmm0                                   \n" \
+  "movdqa     %%xmm9,%%xmm1                                   \n" \
+  "movdqa     %%xmm10,%%xmm2                                  \n" \
+  "paddw      %%xmm12,%%xmm4                                  \n" \
+  "pmaddubsw  %%xmm3,%%xmm0                                   \n" \
+  "pmaddubsw  %%xmm3,%%xmm1                                   \n" \
+  "pmaddubsw  %%xmm3,%%xmm2                                   \n" \
   "paddsw     %%xmm4,%%xmm0                                   \n" \
-  "paddsw     %%xmm4,%%xmm1                                   \n" \
-  "paddsw     %%xmm4,%%xmm2                                   \n"
-#define YUVTORGB_REGS \
-  "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13", "xmm14",
+  "paddsw     %%xmm4,%%xmm2                                   \n" \
+  "psubsw     %%xmm1,%%xmm4                                   \n" \
+  "movdqa     %%xmm4,%%xmm1                                   \n"
+
+#define YUVTORGB_REGS "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13",
 
 #else
 #define YUVTORGB_SETUP(yuvconstants)
 // Convert 8 pixels: 8 UV and 8 Y
 #define YUVTORGB16(yuvconstants)                                  \
-  "movdqa     %%xmm0,%%xmm1                                   \n" \
-  "movdqa     %%xmm0,%%xmm2                                   \n" \
-  "movdqa     %%xmm0,%%xmm3                                   \n" \
-  "movdqa     96(%[yuvconstants]),%%xmm0                      \n" \
-  "pmaddubsw  (%[yuvconstants]),%%xmm1                        \n" \
-  "psubw      %%xmm1,%%xmm0                                   \n" \
-  "movdqa     128(%[yuvconstants]),%%xmm1                     \n" \
-  "pmaddubsw  32(%[yuvconstants]),%%xmm2                      \n" \
-  "psubw      %%xmm2,%%xmm1                                   \n" \
-  "movdqa     160(%[yuvconstants]),%%xmm2                     \n" \
-  "pmaddubsw  64(%[yuvconstants]),%%xmm3                      \n" \
-  "psubw      %%xmm3,%%xmm2                                   \n" \
-  "pmulhuw    192(%[yuvconstants]),%%xmm4                     \n" \
+  "pcmpeqb    %%xmm0,%%xmm0                                   \n" \
+  "pxor       %%xmm1,%%xmm1                                   \n" \
+  "psllw      $7,%%xmm0                                       \n" \
+  "pshufb     %%xmm1,%%xmm0                                   \n" \
+  "psubb      %%xmm0,%%xmm3                                   \n" \
+  "pmulhuw    96(%[yuvconstants]),%%xmm4                      \n" \
+  "movdqa     (%[yuvconstants]),%%xmm0                        \n" \
+  "movdqa     32(%[yuvconstants]),%%xmm1                      \n" \
+  "movdqa     64(%[yuvconstants]),%%xmm2                      \n" \
+  "pmaddubsw  %%xmm3,%%xmm0                                   \n" \
+  "pmaddubsw  %%xmm3,%%xmm1                                   \n" \
+  "pmaddubsw  %%xmm3,%%xmm2                                   \n" \
+  "movdqa     128(%[yuvconstants]),%%xmm3                     \n" \
+  "paddw      %%xmm3,%%xmm4                                   \n" \
   "paddsw     %%xmm4,%%xmm0                                   \n" \
-  "paddsw     %%xmm4,%%xmm1                                   \n" \
-  "paddsw     %%xmm4,%%xmm2                                   \n"
+  "paddsw     %%xmm4,%%xmm2                                   \n" \
+  "psubsw     %%xmm1,%%xmm4                                   \n" \
+  "movdqa     %%xmm4,%%xmm1                                   \n"
+
 #define YUVTORGB_REGS
 #endif
 
@@ -2359,8 +2578,8 @@ void OMITFP I422ToAR30Row_SSSE3(const uint8_t* y_buf,
       "pcmpeqb     %%xmm5,%%xmm5                 \n"  // AR30 constants
       "psrlw       $14,%%xmm5                    \n"
       "psllw       $4,%%xmm5                     \n"  // 2 alpha bits
-      "pxor        %%xmm6,%%xmm6                 \n"
-      "pcmpeqb     %%xmm7,%%xmm7                 \n"  // 0 for min
+      "pxor        %%xmm6,%%xmm6                 \n"  // 0 for min
+      "pcmpeqb     %%xmm7,%%xmm7                 \n"
       "psrlw       $6,%%xmm7                     \n"  // 1023 for max
 
     LABELALIGN
@@ -2454,8 +2673,8 @@ void OMITFP I210ToAR30Row_SSSE3(const uint16_t* y_buf,
       "pcmpeqb     %%xmm5,%%xmm5                 \n"
       "psrlw       $14,%%xmm5                    \n"
       "psllw       $4,%%xmm5                     \n"  // 2 alpha bits
-      "pxor        %%xmm6,%%xmm6                 \n"
-      "pcmpeqb     %%xmm7,%%xmm7                 \n"  // 0 for min
+      "pxor        %%xmm6,%%xmm6                 \n"  // 0 for min
+      "pcmpeqb     %%xmm7,%%xmm7                 \n"
       "psrlw       $6,%%xmm7                     \n"  // 1023 for max
 
     LABELALIGN
@@ -2489,8 +2708,8 @@ void OMITFP I212ToAR30Row_SSSE3(const uint16_t* y_buf,
       "pcmpeqb     %%xmm5,%%xmm5                 \n"
       "psrlw       $14,%%xmm5                    \n"
       "psllw       $4,%%xmm5                     \n"  // 2 alpha bits
-      "pxor        %%xmm6,%%xmm6                 \n"
-      "pcmpeqb     %%xmm7,%%xmm7                 \n"  // 0 for min
+      "pxor        %%xmm6,%%xmm6                 \n"  // 0 for min
+      "pcmpeqb     %%xmm7,%%xmm7                 \n"
       "psrlw       $6,%%xmm7                     \n"  // 1023 for max
 
     LABELALIGN
@@ -2519,7 +2738,6 @@ void OMITFP I410ToARGBRow_SSSE3(const uint16_t* y_buf,
                                 const struct YuvConstants* yuvconstants,
                                 int width) {
   asm volatile (
-
     YUVTORGB_SETUP(yuvconstants)
       "sub         %[u_buf],%[v_buf]             \n"
       "pcmpeqb     %%xmm5,%%xmm5                 \n"
@@ -2551,9 +2769,8 @@ void OMITFP I210AlphaToARGBRow_SSSE3(const uint16_t* y_buf,
                                      const struct YuvConstants* yuvconstants,
                                      int width) {
   asm volatile(
-
       YUVTORGB_SETUP(
-      yuvconstants) "sub         %[u_buf],%[v_buf]             \n"
+          yuvconstants) "sub         %[u_buf],%[v_buf]             \n"
 
       LABELALIGN "1:                                        \n" READYUVA210
           YUVTORGB(yuvconstants) STOREARGB
@@ -2584,28 +2801,32 @@ void OMITFP I410AlphaToARGBRow_SSSE3(const uint16_t* y_buf,
                                      uint8_t* dst_argb,
                                      const struct YuvConstants* yuvconstants,
                                      int width) {
+  // clang-format off
   asm volatile(
+    YUVTORGB_SETUP(yuvconstants)
+      "sub         %[u_buf],%[v_buf]             \n"
 
-      YUVTORGB_SETUP(
-      yuvconstants) "sub         %[u_buf],%[v_buf]             \n"
-
-      LABELALIGN "1:                                        \n" READYUVA410
-          YUVTORGB(yuvconstants) STOREARGB
+    LABELALIGN
+      "1:                                        \n"
+    READYUVA410
+    YUVTORGB(yuvconstants)
+    STOREARGB
       "subl        $0x8,%[width]                 \n"
       "jg          1b                            \n"
-      : [y_buf] "+r"(y_buf),  // %[y_buf]
-        [u_buf] "+r"(u_buf),  // %[u_buf]
-        [v_buf] "+r"(v_buf),  // %[v_buf]
-        [a_buf] "+r"(a_buf),
-        [dst_argb] "+r"(dst_argb),  // %[dst_argb]
+    : [y_buf] "+r"(y_buf),  // %[y_buf]
+      [u_buf] "+r"(u_buf),  // %[u_buf]
+      [v_buf] "+r"(v_buf),  // %[v_buf]
+      [a_buf] "+r"(a_buf),
+      [dst_argb] "+r"(dst_argb),  // %[dst_argb]
 #if defined(__i386__)
-        [width] "+m"(width)  // %[width]
+      [width] "+m"(width)  // %[width]
 #else
-        [width] "+rm"(width)  // %[width]
+      [width] "+rm"(width)  // %[width]
 #endif
-      : [yuvconstants] "r"(yuvconstants)  // %[yuvconstants]
-      : "memory", "cc", YUVTORGB_REGS "xmm0", "xmm1", "xmm2", "xmm3", "xmm4",
-        "xmm5");
+    : [yuvconstants] "r"(yuvconstants)  // %[yuvconstants]
+    : "memory", "cc", YUVTORGB_REGS "xmm0", "xmm1", "xmm2", "xmm3", "xmm4",
+      "xmm5");
+  // clang-format on
 }
 #endif
 
@@ -2617,14 +2838,13 @@ void OMITFP I410ToAR30Row_SSSE3(const uint16_t* y_buf,
                                 const struct YuvConstants* yuvconstants,
                                 int width) {
   asm volatile (
-
     YUVTORGB_SETUP(yuvconstants)
       "sub         %[u_buf],%[v_buf]             \n"
       "pcmpeqb     %%xmm5,%%xmm5                 \n"
       "psrlw       $14,%%xmm5                    \n"
       "psllw       $4,%%xmm5                     \n"  // 2 alpha bits
-      "pxor        %%xmm6,%%xmm6                 \n"
-      "pcmpeqb     %%xmm7,%%xmm7                 \n"  // 0 for min
+      "pxor        %%xmm6,%%xmm6                 \n"  // 0 for min
+      "pcmpeqb     %%xmm7,%%xmm7                 \n"
       "psrlw       $6,%%xmm7                     \n"  // 1023 for max
 
     LABELALIGN
@@ -2802,7 +3022,6 @@ void OMITFP P210ToARGBRow_SSSE3(const uint16_t* y_buf,
                                 const struct YuvConstants* yuvconstants,
                                 int width) {
   asm volatile(
-
       YUVTORGB_SETUP(
       yuvconstants) "pcmpeqb     %%xmm5,%%xmm5                 \n"
 
@@ -2825,7 +3044,6 @@ void OMITFP P410ToARGBRow_SSSE3(const uint16_t* y_buf,
                                 const struct YuvConstants* yuvconstants,
                                 int width) {
   asm volatile(
-
       YUVTORGB_SETUP(
       yuvconstants) "pcmpeqb     %%xmm5,%%xmm5                 \n"
 
@@ -2848,13 +3066,12 @@ void OMITFP P210ToAR30Row_SSSE3(const uint16_t* y_buf,
                                 const struct YuvConstants* yuvconstants,
                                 int width) {
   asm volatile (
-
     YUVTORGB_SETUP(yuvconstants)
       "pcmpeqb     %%xmm5,%%xmm5                 \n"
       "psrlw       $14,%%xmm5                    \n"
       "psllw       $4,%%xmm5                     \n"  // 2 alpha bits
-      "pxor        %%xmm6,%%xmm6                 \n"
-      "pcmpeqb     %%xmm7,%%xmm7                 \n"  // 0 for min
+      "pxor        %%xmm6,%%xmm6                 \n"  // 0 for min
+      "pcmpeqb     %%xmm7,%%xmm7                 \n"
       "psrlw       $6,%%xmm7                     \n"  // 1023 for max
 
     LABELALIGN
@@ -2880,13 +3097,12 @@ void OMITFP P410ToAR30Row_SSSE3(const uint16_t* y_buf,
                                 const struct YuvConstants* yuvconstants,
                                 int width) {
   asm volatile (
-
     YUVTORGB_SETUP(yuvconstants)
       "pcmpeqb     %%xmm5,%%xmm5                 \n"
       "psrlw       $14,%%xmm5                    \n"
       "psllw       $4,%%xmm5                     \n"  // 2 alpha bits
-      "pxor        %%xmm6,%%xmm6                 \n"
-      "pcmpeqb     %%xmm7,%%xmm7                 \n"  // 0 for min
+      "pxor        %%xmm6,%%xmm6                 \n"  // 0 for min
+      "pcmpeqb     %%xmm7,%%xmm7                 \n"
       "psrlw       $6,%%xmm7                     \n"  // 1023 for max
 
     LABELALIGN
@@ -2939,12 +3155,12 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 16 UV from 444
 #define READYUV444_AVX2                                               \
-  "vmovdqu    (%[u_buf]),%%xmm0                                   \n" \
+  "vmovdqu    (%[u_buf]),%%xmm3                                   \n" \
   "vmovdqu    0x00(%[u_buf],%[v_buf],1),%%xmm1                    \n" \
   "lea        0x10(%[u_buf]),%[u_buf]                             \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                                 \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                                 \n" \
   "vpermq     $0xd8,%%ymm1,%%ymm1                                 \n" \
-  "vpunpcklbw %%ymm1,%%ymm0,%%ymm0                                \n" \
+  "vpunpcklbw %%ymm1,%%ymm3,%%ymm3                                \n" \
   "vmovdqu    (%[y_buf]),%%xmm4                                   \n" \
   "vpermq     $0xd8,%%ymm4,%%ymm4                                 \n" \
   "vpunpcklbw %%ymm4,%%ymm4,%%ymm4                                \n" \
@@ -2952,12 +3168,12 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 8 UV from 422, upsample to 16 UV.
 #define READYUV422_AVX2                                               \
-  "vmovq      (%[u_buf]),%%xmm0                                   \n" \
+  "vmovq      (%[u_buf]),%%xmm3                                   \n" \
   "vmovq      0x00(%[u_buf],%[v_buf],1),%%xmm1                    \n" \
   "lea        0x8(%[u_buf]),%[u_buf]                              \n" \
-  "vpunpcklbw %%ymm1,%%ymm0,%%ymm0                                \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                                 \n" \
-  "vpunpcklwd %%ymm0,%%ymm0,%%ymm0                                \n" \
+  "vpunpcklbw %%ymm1,%%ymm3,%%ymm3                                \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                                 \n" \
+  "vpunpcklwd %%ymm3,%%ymm3,%%ymm3                                \n" \
   "vmovdqu    (%[y_buf]),%%xmm4                                   \n" \
   "vpermq     $0xd8,%%ymm4,%%ymm4                                 \n" \
   "vpunpcklbw %%ymm4,%%ymm4,%%ymm4                                \n" \
@@ -2967,30 +3183,30 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 // TODO(fbarchard): Consider vshufb to replace pack/unpack
 // TODO(fbarchard): Consider vunpcklpd to combine the 2 registers into 1.
 #define READYUV210_AVX2                                            \
-  "vmovdqu    (%[u_buf]),%%xmm0                                \n" \
+  "vmovdqu    (%[u_buf]),%%xmm3                                \n" \
   "vmovdqu    0x00(%[u_buf],%[v_buf],1),%%xmm1                 \n" \
   "lea        0x10(%[u_buf]),%[u_buf]                          \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                              \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                              \n" \
   "vpermq     $0xd8,%%ymm1,%%ymm1                              \n" \
-  "vpunpcklwd %%ymm1,%%ymm0,%%ymm0                             \n" \
-  "vpsraw     $2,%%ymm0,%%ymm0                                 \n" \
-  "vpackuswb  %%ymm0,%%ymm0,%%ymm0                             \n" \
-  "vpunpcklwd %%ymm0,%%ymm0,%%ymm0                             \n" \
+  "vpunpcklwd %%ymm1,%%ymm3,%%ymm3                             \n" \
+  "vpsraw     $2,%%ymm3,%%ymm3                                 \n" \
+  "vpackuswb  %%ymm3,%%ymm3,%%ymm3                             \n" \
+  "vpunpcklwd %%ymm3,%%ymm3,%%ymm3                             \n" \
   "vmovdqu    (%[y_buf]),%%ymm4                                \n" \
   "vpsllw     $6,%%ymm4,%%ymm4                                 \n" \
   "lea        0x20(%[y_buf]),%[y_buf]                          \n"
 
 // Read 8 UV from 210, upsample to 16 UV. With 16 Alpha.
 #define READYUVA210_AVX2                                           \
-  "vmovdqu    (%[u_buf]),%%xmm0                                \n" \
+  "vmovdqu    (%[u_buf]),%%xmm3                                \n" \
   "vmovdqu    0x00(%[u_buf],%[v_buf],1),%%xmm1                 \n" \
   "lea        0x10(%[u_buf]),%[u_buf]                          \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                              \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                              \n" \
   "vpermq     $0xd8,%%ymm1,%%ymm1                              \n" \
-  "vpunpcklwd %%ymm1,%%ymm0,%%ymm0                             \n" \
-  "vpsraw     $2,%%ymm0,%%ymm0                                 \n" \
-  "vpackuswb  %%ymm0,%%ymm0,%%ymm0                             \n" \
-  "vpunpcklwd %%ymm0,%%ymm0,%%ymm0                             \n" \
+  "vpunpcklwd %%ymm1,%%ymm3,%%ymm3                             \n" \
+  "vpsraw     $2,%%ymm3,%%ymm3                                 \n" \
+  "vpackuswb  %%ymm3,%%ymm3,%%ymm3                             \n" \
+  "vpunpcklwd %%ymm3,%%ymm3,%%ymm3                             \n" \
   "vmovdqu    (%[y_buf]),%%ymm4                                \n" \
   "vpsllw     $6,%%ymm4,%%ymm4                                 \n" \
   "lea        0x20(%[y_buf]),%[y_buf]                          \n" \
@@ -3001,43 +3217,43 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 16 UV from 410
 #define READYUV410_AVX2                                            \
-  "vmovdqu    (%[u_buf]),%%ymm0                                \n" \
+  "vmovdqu    (%[u_buf]),%%ymm3                                \n" \
   "vmovdqu    0x00(%[u_buf],%[v_buf],1),%%ymm2                 \n" \
   "lea        0x20(%[u_buf]),%[u_buf]                          \n" \
-  "vpsraw     $2,%%ymm0,%%ymm0                                 \n" \
+  "vpsraw     $2,%%ymm3,%%ymm3                                 \n" \
   "vpsraw     $2,%%ymm2,%%ymm2                                 \n" \
-  "vpunpckhwd %%ymm2,%%ymm0,%%ymm1                             \n" \
-  "vpunpcklwd %%ymm2,%%ymm0,%%ymm0                             \n" \
-  "vpackuswb  %%ymm1,%%ymm0,%%ymm0                             \n" \
+  "vpunpckhwd %%ymm2,%%ymm3,%%ymm1                             \n" \
+  "vpunpcklwd %%ymm2,%%ymm3,%%ymm3                             \n" \
+  "vpackuswb  %%ymm1,%%ymm3,%%ymm3                             \n" \
   "vmovdqu    (%[y_buf]),%%ymm4                                \n" \
   "vpsllw     $6,%%ymm4,%%ymm4                                 \n" \
   "lea        0x20(%[y_buf]),%[y_buf]                          \n"
 
 // Read 8 UV from 212 12 bit, upsample to 16 UV
 #define READYUV212_AVX2                                            \
-  "vmovdqu    (%[u_buf]),%%xmm0                                \n" \
+  "vmovdqu    (%[u_buf]),%%xmm3                                \n" \
   "vmovdqu    0x00(%[u_buf],%[v_buf],1),%%xmm1                 \n" \
   "lea        0x10(%[u_buf]),%[u_buf]                          \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                              \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                              \n" \
   "vpermq     $0xd8,%%ymm1,%%ymm1                              \n" \
-  "vpunpcklwd %%ymm1,%%ymm0,%%ymm0                             \n" \
-  "vpsraw     $0x4,%%ymm0,%%ymm0                               \n" \
-  "vpackuswb  %%ymm0,%%ymm0,%%ymm0                             \n" \
-  "vpunpcklwd %%ymm0,%%ymm0,%%ymm0                             \n" \
+  "vpunpcklwd %%ymm1,%%ymm3,%%ymm3                             \n" \
+  "vpsraw     $0x4,%%ymm3,%%ymm3                               \n" \
+  "vpackuswb  %%ymm3,%%ymm3,%%ymm3                             \n" \
+  "vpunpcklwd %%ymm3,%%ymm3,%%ymm3                             \n" \
   "vmovdqu    (%[y_buf]),%%ymm4                                \n" \
   "vpsllw     $0x4,%%ymm4,%%ymm4                               \n" \
   "lea        0x20(%[y_buf]),%[y_buf]                          \n"
 
 // Read 16 UV from 410. With 16 Alpha.
 #define READYUVA410_AVX2                                           \
-  "vmovdqu    (%[u_buf]),%%ymm0                                \n" \
+  "vmovdqu    (%[u_buf]),%%ymm3                                \n" \
   "vmovdqu    0x00(%[u_buf],%[v_buf],1),%%ymm2                 \n" \
   "lea        0x20(%[u_buf]),%[u_buf]                          \n" \
-  "vpsraw     $2,%%ymm0,%%ymm0                                 \n" \
+  "vpsraw     $2,%%ymm3,%%ymm3                                 \n" \
   "vpsraw     $2,%%ymm2,%%ymm2                                 \n" \
-  "vpunpckhwd %%ymm2,%%ymm0,%%ymm1                             \n" \
-  "vpunpcklwd %%ymm2,%%ymm0,%%ymm0                             \n" \
-  "vpackuswb  %%ymm1,%%ymm0,%%ymm0                             \n" \
+  "vpunpckhwd %%ymm2,%%ymm3,%%ymm1                             \n" \
+  "vpunpcklwd %%ymm2,%%ymm3,%%ymm3                             \n" \
+  "vpackuswb  %%ymm1,%%ymm3,%%ymm3                             \n" \
   "vmovdqu    (%[y_buf]),%%ymm4                                \n" \
   "vpsllw     $6,%%ymm4,%%ymm4                                 \n" \
   "lea        0x20(%[y_buf]),%[y_buf]                          \n" \
@@ -3048,12 +3264,12 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 16 UV from 444.  With 16 Alpha.
 #define READYUVA444_AVX2                                              \
-  "vmovdqu    (%[u_buf]),%%xmm0                                   \n" \
+  "vmovdqu    (%[u_buf]),%%xmm3                                   \n" \
   "vmovdqu    0x00(%[u_buf],%[v_buf],1),%%xmm1                    \n" \
   "lea        0x10(%[u_buf]),%[u_buf]                             \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                                 \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                                 \n" \
   "vpermq     $0xd8,%%ymm1,%%ymm1                                 \n" \
-  "vpunpcklbw %%ymm1,%%ymm0,%%ymm0                                \n" \
+  "vpunpcklbw %%ymm1,%%ymm3,%%ymm3                                \n" \
   "vmovdqu    (%[y_buf]),%%xmm4                                   \n" \
   "vpermq     $0xd8,%%ymm4,%%ymm4                                 \n" \
   "vpunpcklbw %%ymm4,%%ymm4,%%ymm4                                \n" \
@@ -3064,12 +3280,12 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 8 UV from 422, upsample to 16 UV.  With 16 Alpha.
 #define READYUVA422_AVX2                                              \
-  "vmovq      (%[u_buf]),%%xmm0                                   \n" \
+  "vmovq      (%[u_buf]),%%xmm3                                   \n" \
   "vmovq      0x00(%[u_buf],%[v_buf],1),%%xmm1                    \n" \
   "lea        0x8(%[u_buf]),%[u_buf]                              \n" \
-  "vpunpcklbw %%ymm1,%%ymm0,%%ymm0                                \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                                 \n" \
-  "vpunpcklwd %%ymm0,%%ymm0,%%ymm0                                \n" \
+  "vpunpcklbw %%ymm1,%%ymm3,%%ymm3                                \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                                 \n" \
+  "vpunpcklwd %%ymm3,%%ymm3,%%ymm3                                \n" \
   "vmovdqu    (%[y_buf]),%%xmm4                                   \n" \
   "vpermq     $0xd8,%%ymm4,%%ymm4                                 \n" \
   "vpunpcklbw %%ymm4,%%ymm4,%%ymm4                                \n" \
@@ -3080,10 +3296,10 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 8 UV from NV12, upsample to 16 UV.
 #define READNV12_AVX2                                                 \
-  "vmovdqu    (%[uv_buf]),%%xmm0                                  \n" \
+  "vmovdqu    (%[uv_buf]),%%xmm3                                  \n" \
   "lea        0x10(%[uv_buf]),%[uv_buf]                           \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                                 \n" \
-  "vpunpcklwd %%ymm0,%%ymm0,%%ymm0                                \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                                 \n" \
+  "vpunpcklwd %%ymm3,%%ymm3,%%ymm3                                \n" \
   "vmovdqu    (%[y_buf]),%%xmm4                                   \n" \
   "vpermq     $0xd8,%%ymm4,%%ymm4                                 \n" \
   "vpunpcklbw %%ymm4,%%ymm4,%%ymm4                                \n" \
@@ -3091,10 +3307,10 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 8 VU from NV21, upsample to 16 UV.
 #define READNV21_AVX2                                                 \
-  "vmovdqu    (%[vu_buf]),%%xmm0                                  \n" \
+  "vmovdqu    (%[vu_buf]),%%xmm3                                  \n" \
   "lea        0x10(%[vu_buf]),%[vu_buf]                           \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                                 \n" \
-  "vpshufb     %[kShuffleNV21], %%ymm0, %%ymm0                    \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                                 \n" \
+  "vpshufb     %[kShuffleNV21], %%ymm3, %%ymm3                    \n" \
   "vmovdqu    (%[y_buf]),%%xmm4                                   \n" \
   "vpermq     $0xd8,%%ymm4,%%ymm4                                 \n" \
   "vpunpcklbw %%ymm4,%%ymm4,%%ymm4                                \n" \
@@ -3102,23 +3318,23 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 
 // Read 4 UV from P210, upsample to 8 UV
 #define READP210_AVX2                                                 \
-  "vmovdqu    (%[uv_buf]),%%ymm0                                  \n" \
+  "vmovdqu    (%[uv_buf]),%%ymm3                                  \n" \
   "lea        0x20(%[uv_buf]),%[uv_buf]                           \n" \
-  "vpsrlw     $0x8,%%ymm0,%%ymm0                                  \n" \
-  "vpackuswb  %%ymm0,%%ymm0,%%ymm0                                \n" \
-  "vpunpcklwd %%ymm0,%%ymm0,%%ymm0                                \n" \
+  "vpsrlw     $0x8,%%ymm3,%%ymm3                                  \n" \
+  "vpackuswb  %%ymm3,%%ymm3,%%ymm3                                \n" \
+  "vpunpcklwd %%ymm3,%%ymm3,%%ymm3                                \n" \
   "vmovdqu    (%[y_buf]),%%ymm4                                   \n" \
   "lea        0x20(%[y_buf]),%[y_buf]                             \n"
 
 // Read 8 UV from P410
 #define READP410_AVX2                                                 \
-  "vmovdqu    (%[uv_buf]),%%ymm0                                  \n" \
+  "vmovdqu    (%[uv_buf]),%%ymm3                                  \n" \
   "vmovdqu    0x20(%[uv_buf]),%%ymm1                              \n" \
   "lea        0x40(%[uv_buf]),%[uv_buf]                           \n" \
-  "vpsrlw     $0x8,%%ymm0,%%ymm0                                  \n" \
+  "vpsrlw     $0x8,%%ymm3,%%ymm3                                  \n" \
   "vpsrlw     $0x8,%%ymm1,%%ymm1                                  \n" \
-  "vpackuswb  %%ymm1,%%ymm0,%%ymm0                                \n" \
-  "vpermq     $0xd8,%%ymm0,%%ymm0                                 \n" \
+  "vpackuswb  %%ymm1,%%ymm3,%%ymm3                                \n" \
+  "vpermq     $0xd8,%%ymm3,%%ymm3                                 \n" \
   "vmovdqu    (%[y_buf]),%%ymm4                                   \n" \
   "lea        0x20(%[y_buf]),%[y_buf]                             \n"
 
@@ -3126,60 +3342,64 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
 #define READYUY2_AVX2                                                 \
   "vmovdqu    (%[yuy2_buf]),%%ymm4                                \n" \
   "vpshufb    %[kShuffleYUY2Y], %%ymm4, %%ymm4                    \n" \
-  "vmovdqu    (%[yuy2_buf]),%%ymm0                                \n" \
-  "vpshufb    %[kShuffleYUY2UV], %%ymm0, %%ymm0                   \n" \
+  "vmovdqu    (%[yuy2_buf]),%%ymm3                                \n" \
+  "vpshufb    %[kShuffleYUY2UV], %%ymm3, %%ymm3                   \n" \
   "lea        0x20(%[yuy2_buf]),%[yuy2_buf]                       \n"
 
 // Read 8 UYVY with 16 Y and upsample 8 UV to 16 UV.
 #define READUYVY_AVX2                                                 \
   "vmovdqu    (%[uyvy_buf]),%%ymm4                                \n" \
   "vpshufb    %[kShuffleUYVYY], %%ymm4, %%ymm4                    \n" \
-  "vmovdqu    (%[uyvy_buf]),%%ymm0                                \n" \
-  "vpshufb    %[kShuffleUYVYUV], %%ymm0, %%ymm0                   \n" \
+  "vmovdqu    (%[uyvy_buf]),%%ymm3                                \n" \
+  "vpshufb    %[kShuffleUYVYUV], %%ymm3, %%ymm3                   \n" \
   "lea        0x20(%[uyvy_buf]),%[uyvy_buf]                       \n"
 
 #if defined(__x86_64__)
-#define YUVTORGB_SETUP_AVX2(yuvconstants)                            \
-  "vmovdqa     (%[yuvconstants]),%%ymm8                          \n" \
-  "vmovdqa     32(%[yuvconstants]),%%ymm9                        \n" \
-  "vmovdqa     64(%[yuvconstants]),%%ymm10                       \n" \
-  "vmovdqa     96(%[yuvconstants]),%%ymm11                       \n" \
-  "vmovdqa     128(%[yuvconstants]),%%ymm12                      \n" \
-  "vmovdqa     160(%[yuvconstants]),%%ymm13                      \n" \
-  "vmovdqa     192(%[yuvconstants]),%%ymm14                      \n"
+#define YUVTORGB_SETUP_AVX2(yuvconstants)                             \
+  "vpcmpeqb    %%xmm13,%%xmm13,%%xmm13                            \n" \
+  "vmovdqa     (%[yuvconstants]),%%ymm8                           \n" \
+  "vpsllw      $7,%%xmm13,%%xmm13                                 \n" \
+  "vmovdqa     32(%[yuvconstants]),%%ymm9                         \n" \
+  "vpbroadcastb %%xmm13,%%ymm13                                   \n" \
+  "vmovdqa     64(%[yuvconstants]),%%ymm10                        \n" \
+  "vmovdqa     96(%[yuvconstants]),%%ymm11                        \n" \
+  "vmovdqa     128(%[yuvconstants]),%%ymm12                       \n"
 
 #define YUVTORGB16_AVX2(yuvconstants)                                 \
-  "vpmaddubsw  %%ymm10,%%ymm0,%%ymm2                              \n" \
-  "vpmaddubsw  %%ymm9,%%ymm0,%%ymm1                               \n" \
-  "vpmaddubsw  %%ymm8,%%ymm0,%%ymm0                               \n" \
-  "vpsubw      %%ymm2,%%ymm13,%%ymm2                              \n" \
-  "vpsubw      %%ymm1,%%ymm12,%%ymm1                              \n" \
-  "vpsubw      %%ymm0,%%ymm11,%%ymm0                              \n" \
-  "vpmulhuw    %%ymm14,%%ymm4,%%ymm4                              \n" \
+  "vpsubb      %%ymm13,%%ymm3,%%ymm3                              \n" \
+  "vpmulhuw    %%ymm11,%%ymm4,%%ymm4                              \n" \
+  "vpmaddubsw  %%ymm3,%%ymm8,%%ymm0                               \n" \
+  "vpmaddubsw  %%ymm3,%%ymm9,%%ymm1                               \n" \
+  "vpmaddubsw  %%ymm3,%%ymm10,%%ymm2                              \n" \
+  "vpaddw      %%ymm4,%%ymm12,%%ymm4                              \n" \
   "vpaddsw     %%ymm4,%%ymm0,%%ymm0                               \n" \
-  "vpaddsw     %%ymm4,%%ymm1,%%ymm1                               \n" \
+  "vpsubsw     %%ymm1,%%ymm4,%%ymm1                               \n" \
   "vpaddsw     %%ymm4,%%ymm2,%%ymm2                               \n"
 
 #define YUVTORGB_REGS_AVX2 \
-  "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13", "xmm14",
+  "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13",
 
 #else  // Convert 16 pixels: 16 UV and 16 Y.
 
 #define YUVTORGB_SETUP_AVX2(yuvconstants)
 #define YUVTORGB16_AVX2(yuvconstants)                                 \
-  "vpmaddubsw  64(%[yuvconstants]),%%ymm0,%%ymm2                  \n" \
-  "vpmaddubsw  32(%[yuvconstants]),%%ymm0,%%ymm1                  \n" \
-  "vpmaddubsw  (%[yuvconstants]),%%ymm0,%%ymm0                    \n" \
-  "vmovdqu     160(%[yuvconstants]),%%ymm3                        \n" \
-  "vpsubw      %%ymm2,%%ymm3,%%ymm2                               \n" \
-  "vmovdqu     128(%[yuvconstants]),%%ymm3                        \n" \
-  "vpsubw      %%ymm1,%%ymm3,%%ymm1                               \n" \
-  "vmovdqu     96(%[yuvconstants]),%%ymm3                         \n" \
-  "vpsubw      %%ymm0,%%ymm3,%%ymm0                               \n" \
-  "vpmulhuw    192(%[yuvconstants]),%%ymm4,%%ymm4                 \n" \
+  "vpcmpeqb    %%xmm0,%%xmm0,%%xmm0                               \n" \
+  "vpsllw      $7,%%xmm0,%%xmm0                                   \n" \
+  "vpbroadcastb %%xmm0,%%ymm0                                     \n" \
+  "vpsubb      %%ymm0,%%ymm3,%%ymm3                               \n" \
+  "vpmulhuw    96(%[yuvconstants]),%%ymm4,%%ymm4                  \n" \
+  "vmovdqa     (%[yuvconstants]),%%ymm0                           \n" \
+  "vmovdqa     32(%[yuvconstants]),%%ymm1                         \n" \
+  "vmovdqa     64(%[yuvconstants]),%%ymm2                         \n" \
+  "vpmaddubsw  %%ymm3,%%ymm0,%%ymm0                               \n" \
+  "vpmaddubsw  %%ymm3,%%ymm1,%%ymm1                               \n" \
+  "vpmaddubsw  %%ymm3,%%ymm2,%%ymm2                               \n" \
+  "vmovdqa     128(%[yuvconstants]),%%ymm3                        \n" \
+  "vpaddw      %%ymm4,%%ymm3,%%ymm4                               \n" \
   "vpaddsw     %%ymm4,%%ymm0,%%ymm0                               \n" \
-  "vpaddsw     %%ymm4,%%ymm1,%%ymm1                               \n" \
+  "vpsubsw     %%ymm1,%%ymm4,%%ymm1                               \n" \
   "vpaddsw     %%ymm4,%%ymm2,%%ymm2                               \n"
+
 #define YUVTORGB_REGS_AVX2
 #endif
 
@@ -3202,7 +3422,7 @@ void OMITFP I422ToRGBARow_SSSE3(const uint8_t* y_buf,
   "vpunpckhwd %%ymm2,%%ymm0,%%ymm0                                \n" \
   "vmovdqu    %%ymm1,(%[dst_argb])                                \n" \
   "vmovdqu    %%ymm0,0x20(%[dst_argb])                            \n" \
-  "lea       0x40(%[dst_argb]), %[dst_argb]                       \n"
+  "lea        0x40(%[dst_argb]), %[dst_argb]                      \n"
 
 // Store 16 AR30 values.
 #define STOREAR30_AVX2                                                \
@@ -3445,7 +3665,7 @@ void OMITFP I210ToAR30Row_AVX2(const uint16_t* y_buf,
     [width]"+rm"(width)    // %[width]
   : [yuvconstants]"r"(yuvconstants)  // %[yuvconstants]
   : "memory", "cc", YUVTORGB_REGS_AVX2
-    "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5"
+    "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
   );
 }
 #endif  // HAS_I210TOAR30ROW_AVX2
@@ -3485,7 +3705,7 @@ void OMITFP I212ToAR30Row_AVX2(const uint16_t* y_buf,
     [width]"+rm"(width)    // %[width]
   : [yuvconstants]"r"(yuvconstants)  // %[yuvconstants]
   : "memory", "cc", YUVTORGB_REGS_AVX2
-    "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5"
+    "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
   );
 }
 #endif  // HAS_I212TOAR30ROW_AVX2
@@ -3500,7 +3720,6 @@ void OMITFP I410ToARGBRow_AVX2(const uint16_t* y_buf,
                                const struct YuvConstants* yuvconstants,
                                int width) {
   asm volatile (
-
     YUVTORGB_SETUP_AVX2(yuvconstants)
       "sub         %[u_buf],%[v_buf]             \n"
       "vpcmpeqb    %%ymm5,%%ymm5,%%ymm5          \n"
@@ -3537,7 +3756,6 @@ void OMITFP I210AlphaToARGBRow_AVX2(const uint16_t* y_buf,
                                     const struct YuvConstants* yuvconstants,
                                     int width) {
   asm volatile(
-
       YUVTORGB_SETUP_AVX2(
       yuvconstants) "sub         %[u_buf],%[v_buf]             \n"
 
@@ -3574,7 +3792,6 @@ void OMITFP I410AlphaToARGBRow_AVX2(const uint16_t* y_buf,
                                     const struct YuvConstants* yuvconstants,
                                     int width) {
   asm volatile(
-
       YUVTORGB_SETUP_AVX2(
       yuvconstants) "sub         %[u_buf],%[v_buf]             \n"
 
@@ -3610,7 +3827,6 @@ void OMITFP I410ToAR30Row_AVX2(const uint16_t* y_buf,
                                const struct YuvConstants* yuvconstants,
                                int width) {
   asm volatile (
-
     YUVTORGB_SETUP_AVX2(yuvconstants)
       "sub         %[u_buf],%[v_buf]             \n"
       "vpcmpeqb    %%ymm5,%%ymm5,%%ymm5          \n"  // AR30 constants
@@ -3636,7 +3852,7 @@ void OMITFP I410ToAR30Row_AVX2(const uint16_t* y_buf,
     [width]"+rm"(width)    // %[width]
   : [yuvconstants]"r"(yuvconstants)  // %[yuvconstants]
   : "memory", "cc", YUVTORGB_REGS_AVX2
-      "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5"
+      "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
   );
 }
 #endif  // HAS_I410TOAR30ROW_AVX2
@@ -3975,7 +4191,6 @@ void OMITFP P210ToAR30Row_AVX2(const uint16_t* y_buf,
                                const struct YuvConstants* yuvconstants,
                                int width) {
   asm volatile (
-
     YUVTORGB_SETUP_AVX2(yuvconstants)
       "vpcmpeqb    %%ymm5,%%ymm5,%%ymm5          \n"  // AR30 constants
       "vpsrlw      $14,%%ymm5,%%ymm5             \n"
@@ -4013,7 +4228,6 @@ void OMITFP P410ToAR30Row_AVX2(const uint16_t* y_buf,
                                const struct YuvConstants* yuvconstants,
                                int width) {
   asm volatile (
-
     YUVTORGB_SETUP_AVX2(yuvconstants)
       "vpcmpeqb    %%ymm5,%%ymm5,%%ymm5          \n"  // AR30 constants
       "vpsrlw      $14,%%ymm5,%%ymm5             \n"
@@ -4048,8 +4262,8 @@ void I400ToARGBRow_SSE2(const uint8_t* y_buf,
                         const struct YuvConstants* yuvconstants,
                         int width) {
   asm volatile(
-      "movdqa      192(%3),%%xmm2                \n"  // yg = 18997 = 1.164
-      "movdqa      224(%3),%%xmm3                \n"  // ygb = 1160 = 1.164 * 16
+      "movdqa      96(%3),%%xmm2                 \n"  // yg = 18997 = 1.164
+      "movdqa      128(%3),%%xmm3                \n"  // ygb = 1160 = 1.164 * 16
       "pcmpeqb     %%xmm4,%%xmm4                 \n"  // 0xff000000
       "pslld       $0x18,%%xmm4                  \n"
 
@@ -4093,8 +4307,8 @@ void I400ToARGBRow_AVX2(const uint8_t* y_buf,
                         const struct YuvConstants* yuvconstants,
                         int width) {
   asm volatile(
-      "vmovdqa     192(%3),%%ymm2                \n"  // yg = 18997 = 1.164
-      "vmovdqa     224(%3),%%ymm3                \n"  // ygb = -1160 = 1.164*16
+      "vmovdqa     96(%3),%%ymm2                 \n"  // yg = 18997 = 1.164
+      "vmovdqa     128(%3),%%ymm3                \n"  // ygb = -1160 = 1.164*16
       "vpcmpeqb    %%ymm4,%%ymm4,%%ymm4          \n"  // 0xff000000
       "vpslld      $0x18,%%ymm4,%%ymm4           \n"
 
@@ -4512,8 +4726,6 @@ void MergeUVRow_16_AVX2(const uint16_t* src_u,
   // clang-format off
   asm volatile (
       "vmovd       %4,%%xmm3                     \n"
-      "vpunpcklwd  %%xmm3,%%xmm3,%%xmm3          \n"
-      "vbroadcastss %%xmm3,%%xmm3                \n"
       "sub         %0,%1                         \n"
 
     // 16 pixels per loop.
@@ -4545,7 +4757,7 @@ void MergeUVRow_16_AVX2(const uint16_t* src_u,
 }
 #endif  // HAS_MERGEUVROW_AVX2
 
-#ifdef HAS_MERGEUVROW_16_AVX2
+#ifdef HAS_SPLITUVROW_16_AVX2
 const uvec8 kSplitUVShuffle16 = {0, 1, 4, 5, 8,  9,  12, 13,
                                  2, 3, 6, 7, 10, 11, 14, 15};
 void SplitUVRow_16_AVX2(const uint16_t* src_uv,
@@ -4557,8 +4769,6 @@ void SplitUVRow_16_AVX2(const uint16_t* src_uv,
   // clang-format off
   asm volatile (
       "vmovd       %4,%%xmm3                     \n"
-      "vpunpcklwd  %%xmm3,%%xmm3,%%xmm3          \n"
-      "vbroadcastss %%xmm3,%%xmm3                \n"
       "vbroadcastf128 %5,%%ymm4                  \n"
       "sub         %1,%2                         \n"
 
@@ -4584,16 +4794,15 @@ void SplitUVRow_16_AVX2(const uint16_t* src_uv,
       "jg          1b                            \n"
       "vzeroupper                                \n"
   : "+r"(src_uv),   // %0
-    "+r"(dst_u),   // %1
-    "+r"(dst_v),  // %2
-    "+r"(width),    // %3
-    "+r"(depth)     // %4
-  :
+    "+r"(dst_u),    // %1
+    "+r"(dst_v),    // %2
+    "+r"(width)     // %3
+  : "r"(depth),     // %4
     "m"(kSplitUVShuffle16) // %5
   : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4");
   // clang-format on
 }
-#endif  // HAS_MERGEUVROW_AVX2
+#endif  // HAS_SPLITUVROW_16_AVX2
 
 // Use scale to convert lsb formats to msb, depending how many bits there are:
 // 128 = 9 bits
@@ -5051,7 +5260,9 @@ void MergeARGBRow_SSE2(const uint8_t* src_r,
       :
       : "memory", "cc", "xmm0", "xmm1", "xmm2");
 }
+#endif
 
+#ifdef HAS_MERGEXRGBROW_SSE2
 void MergeXRGBRow_SSE2(const uint8_t* src_r,
                        const uint8_t* src_g,
                        const uint8_t* src_b,
@@ -5135,7 +5346,9 @@ void MergeARGBRow_AVX2(const uint8_t* src_r,
       :
       : "memory", "cc", "xmm0", "xmm1", "xmm2");
 }
+#endif
 
+#ifdef HAS_MERGEXRGBROW_AVX2
 void MergeXRGBRow_AVX2(const uint8_t* src_r,
                        const uint8_t* src_g,
                        const uint8_t* src_b,
@@ -5229,7 +5442,9 @@ void SplitARGBRow_SSE2(const uint8_t* src_argb,
       :
       : "memory", "cc", "xmm0", "xmm1", "xmm2");
 }
+#endif
 
+#ifdef HAS_SPLITXRGBROW_SSE2
 void SplitXRGBRow_SSE2(const uint8_t* src_argb,
                        uint8_t* dst_r,
                        uint8_t* dst_g,
@@ -5325,7 +5540,9 @@ void SplitARGBRow_SSSE3(const uint8_t* src_argb,
       : "m"(kShuffleMaskARGBSplit)  // %6
       : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3");
 }
+#endif
 
+#ifdef HAS_SPLITXRGBROW_SSSE3
 void SplitXRGBRow_SSSE3(const uint8_t* src_argb,
                         uint8_t* dst_r,
                         uint8_t* dst_g,
@@ -5417,7 +5634,9 @@ void SplitARGBRow_AVX2(const uint8_t* src_argb,
         "m"(kShuffleMaskARGBPermute)  // %7
       : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4");
 }
+#endif
 
+#ifdef HAS_SPLITXRGBROW_AVX2
 void SplitXRGBRow_AVX2(const uint8_t* src_argb,
                        uint8_t* dst_r,
                        uint8_t* dst_g,
@@ -5459,7 +5678,318 @@ void SplitXRGBRow_AVX2(const uint8_t* src_argb,
         "+r"(width)                   // %4
       : "m"(kShuffleMaskARGBSplit),   // %5
         "m"(kShuffleMaskARGBPermute)  // %6
-      : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3");
+      : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4");
+}
+#endif
+
+#ifdef HAS_MERGEXR30ROW_AVX2
+void MergeXR30Row_AVX2(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint8_t* dst_ar30,
+                       int depth,
+                       int width) {
+  int shift = depth - 10;
+  asm volatile(
+
+      "sub         %0,%1                         \n"
+      "sub         %0,%2                         \n"
+      "vpcmpeqb    %%ymm5,%%ymm5,%%ymm5          \n"  // AR30 constants
+      "vpsrlw      $14,%%ymm5,%%ymm5             \n"
+      "vpsllw      $4,%%ymm5,%%ymm5              \n"  // 2 alpha bits
+      "vpcmpeqb    %%ymm6,%%ymm6,%%ymm6          \n"
+      "vpsrlw      $6,%%ymm6,%%ymm6              \n"
+      "vmovd       %5,%%xmm4                     \n"
+
+      LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"
+      "vmovdqu     (%0,%1),%%ymm1                \n"
+      "vmovdqu     (%0,%2),%%ymm2                \n"
+      "vpsrlw      %%xmm4,%%ymm0,%%ymm0          \n"
+      "vpsrlw      %%xmm4,%%ymm1,%%ymm1          \n"
+      "vpsrlw      %%xmm4,%%ymm2,%%ymm2          \n"
+      "vpminuw     %%ymm0,%%ymm6,%%ymm0          \n"
+      "vpminuw     %%ymm1,%%ymm6,%%ymm1          \n"
+      "vpminuw     %%ymm2,%%ymm6,%%ymm2          \n"
+      "vpermq      $0xd8,%%ymm0,%%ymm0           \n"
+      "vpermq      $0xd8,%%ymm1,%%ymm1           \n"
+      "vpermq      $0xd8,%%ymm2,%%ymm2           \n"
+      "vpsllw      $0x4,%%ymm0,%%ymm0            \n"  // Shift R to target bit
+      "vpunpckhwd  %%ymm0,%%ymm2,%%ymm3          \n"  // RB
+      "vpunpcklwd  %%ymm0,%%ymm2,%%ymm0          \n"
+      "vpunpckhwd  %%ymm5,%%ymm1,%%ymm2          \n"  // AG
+      "vpunpcklwd  %%ymm5,%%ymm1,%%ymm1          \n"
+      "vpslld      $0xa,%%ymm1,%%ymm1            \n"  // Shift AG to target bit
+      "vpslld      $0xa,%%ymm2,%%ymm2            \n"
+      "vpor        %%ymm1,%%ymm0,%%ymm0          \n"  // Combine
+      "vpor        %%ymm2,%%ymm3,%%ymm3          \n"
+      "vmovdqu     %%ymm0,(%3)                   \n"
+      "vmovdqu     %%ymm3,0x20(%3)               \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x40(%3),%3                   \n"
+      "sub         $0x10,%4                      \n"
+      "jg          1b                            \n"
+      "vzeroupper                                \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_ar30),  // %3
+        "+r"(width)      // %4
+#if defined(__i386__)
+      : "m"(shift)  // %5
+#else
+      : "rm"(shift)           // %5
+#endif
+      : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5");
+}
+#endif
+
+#ifdef HAS_MERGEAR64ROW_AVX2
+static const lvec32 MergeAR64Permute = {0, 4, 2, 6, 1, 5, 3, 7};
+void MergeAR64Row_AVX2(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       const uint16_t* src_a,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width) {
+  int shift = 16 - depth;
+  int mask = (1 << depth) - 1;
+  mask = (mask << 16) + mask;
+  asm volatile(
+
+      "sub         %0,%1                         \n"
+      "sub         %0,%2                         \n"
+      "sub         %0,%3                         \n"
+      "vmovdqa     %8,%%ymm5                     \n"
+      "vmovd       %6,%%xmm6                     \n"
+      "vbroadcastss %7,%%ymm7                    \n"
+
+      LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"  // R
+      "vmovdqu     (%0,%1),%%ymm1                \n"  // G
+      "vmovdqu     (%0,%2),%%ymm2                \n"  // B
+      "vmovdqu     (%0,%3),%%ymm3                \n"  // A
+      "vpminuw     %%ymm0,%%ymm7,%%ymm0          \n"
+      "vpminuw     %%ymm1,%%ymm7,%%ymm1          \n"
+      "vpminuw     %%ymm2,%%ymm7,%%ymm2          \n"
+      "vpminuw     %%ymm3,%%ymm7,%%ymm3          \n"
+      "vpsllw      %%xmm6,%%ymm0,%%ymm0          \n"
+      "vpsllw      %%xmm6,%%ymm1,%%ymm1          \n"
+      "vpsllw      %%xmm6,%%ymm2,%%ymm2          \n"
+      "vpsllw      %%xmm6,%%ymm3,%%ymm3          \n"
+      "vpermd      %%ymm0,%%ymm5,%%ymm0          \n"
+      "vpermd      %%ymm1,%%ymm5,%%ymm1          \n"
+      "vpermd      %%ymm2,%%ymm5,%%ymm2          \n"
+      "vpermd      %%ymm3,%%ymm5,%%ymm3          \n"
+      "vpunpcklwd  %%ymm1,%%ymm2,%%ymm4          \n"  // BG(low)
+      "vpunpckhwd  %%ymm1,%%ymm2,%%ymm1          \n"  // BG(hi)
+      "vpunpcklwd  %%ymm3,%%ymm0,%%ymm2          \n"  // RA(low)
+      "vpunpckhwd  %%ymm3,%%ymm0,%%ymm0          \n"  // RA(hi)
+      "vpunpckldq  %%ymm2,%%ymm4,%%ymm3          \n"  // BGRA(1)
+      "vpunpckhdq  %%ymm2,%%ymm4,%%ymm4          \n"  // BGRA(3)
+      "vpunpckldq  %%ymm0,%%ymm1,%%ymm2          \n"  // BGRA(2)
+      "vpunpckhdq  %%ymm0,%%ymm1,%%ymm1          \n"  // BGRA(4)
+      "vmovdqu     %%ymm3,(%4)                   \n"
+      "vmovdqu     %%ymm2,0x20(%4)               \n"
+      "vmovdqu     %%ymm4,0x40(%4)               \n"
+      "vmovdqu     %%ymm1,0x60(%4)               \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x80(%4),%4                   \n"
+      "subl        $0x10,%5                      \n"
+      "jg          1b                            \n"
+      "vzeroupper                                \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(src_a),     // %3
+        "+r"(dst_ar64),  // %4
+#if defined(__i386__)
+        "+m"(width)  // %5
+#else
+        "+rm"(width)          // %5
+#endif
+      : "m"(shift),            // %6
+        "m"(mask),             // %7
+        "m"(MergeAR64Permute)  // %8
+      : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6",
+        "xmm7");
+}
+#endif
+
+#ifdef HAS_MERGEXR64ROW_AVX2
+void MergeXR64Row_AVX2(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width) {
+  int shift = 16 - depth;
+  int mask = (1 << depth) - 1;
+  mask = (mask << 16) + mask;
+  asm volatile(
+
+      "sub         %0,%1                         \n"
+      "sub         %0,%2                         \n"
+      "vmovdqa     %7,%%ymm5                     \n"
+      "vmovd       %5,%%xmm6                     \n"
+      "vbroadcastss %6,%%ymm7                    \n"
+
+      LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"  // R
+      "vmovdqu     (%0,%1),%%ymm1                \n"  // G
+      "vmovdqu     (%0,%2),%%ymm2                \n"  // B
+      "vpminuw     %%ymm0,%%ymm7,%%ymm0          \n"
+      "vpminuw     %%ymm1,%%ymm7,%%ymm1          \n"
+      "vpminuw     %%ymm2,%%ymm7,%%ymm2          \n"
+      "vpsllw      %%xmm6,%%ymm0,%%ymm0          \n"
+      "vpsllw      %%xmm6,%%ymm1,%%ymm1          \n"
+      "vpsllw      %%xmm6,%%ymm2,%%ymm2          \n"
+      "vpermd      %%ymm0,%%ymm5,%%ymm0          \n"
+      "vpermd      %%ymm1,%%ymm5,%%ymm1          \n"
+      "vpermd      %%ymm2,%%ymm5,%%ymm2          \n"
+      "vpcmpeqb    %%ymm3,%%ymm3,%%ymm3          \n"  // A (0xffff)
+      "vpunpcklwd  %%ymm1,%%ymm2,%%ymm4          \n"  // BG(low)
+      "vpunpckhwd  %%ymm1,%%ymm2,%%ymm1          \n"  // BG(hi)
+      "vpunpcklwd  %%ymm3,%%ymm0,%%ymm2          \n"  // RA(low)
+      "vpunpckhwd  %%ymm3,%%ymm0,%%ymm0          \n"  // RA(hi)
+      "vpunpckldq  %%ymm2,%%ymm4,%%ymm3          \n"  // BGRA(1)
+      "vpunpckhdq  %%ymm2,%%ymm4,%%ymm4          \n"  // BGRA(3)
+      "vpunpckldq  %%ymm0,%%ymm1,%%ymm2          \n"  // BGRA(2)
+      "vpunpckhdq  %%ymm0,%%ymm1,%%ymm1          \n"  // BGRA(4)
+      "vmovdqu     %%ymm3,(%3)                   \n"
+      "vmovdqu     %%ymm2,0x20(%3)               \n"
+      "vmovdqu     %%ymm4,0x40(%3)               \n"
+      "vmovdqu     %%ymm1,0x60(%3)               \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x80(%3),%3                   \n"
+      "subl        $0x10,%4                      \n"
+      "jg          1b                            \n"
+      "vzeroupper                                \n"
+      : "+r"(src_r),           // %0
+        "+r"(src_g),           // %1
+        "+r"(src_b),           // %2
+        "+r"(dst_ar64),        // %3
+        "+r"(width)            // %4
+      : "m"(shift),            // %5
+        "m"(mask),             // %6
+        "m"(MergeAR64Permute)  // %7
+      : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6",
+        "xmm7");
+}
+#endif
+
+#ifdef HAS_MERGEARGB16TO8ROW_AVX2
+static const uvec8 MergeARGB16To8Shuffle = {0, 8,  1, 9,  2, 10, 3, 11,
+                                            4, 12, 5, 13, 6, 14, 7, 15};
+void MergeARGB16To8Row_AVX2(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            const uint16_t* src_a,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width) {
+  int shift = depth - 8;
+  asm volatile(
+
+      "sub         %0,%1                         \n"
+      "sub         %0,%2                         \n"
+      "sub         %0,%3                         \n"
+      "vbroadcastf128 %7,%%ymm5                  \n"
+      "vmovd       %6,%%xmm6                     \n"
+
+      LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"  // R
+      "vmovdqu     (%0,%1),%%ymm1                \n"  // G
+      "vmovdqu     (%0,%2),%%ymm2                \n"  // B
+      "vmovdqu     (%0,%3),%%ymm3                \n"  // A
+      "vpsrlw      %%xmm6,%%ymm0,%%ymm0          \n"
+      "vpsrlw      %%xmm6,%%ymm1,%%ymm1          \n"
+      "vpsrlw      %%xmm6,%%ymm2,%%ymm2          \n"
+      "vpsrlw      %%xmm6,%%ymm3,%%ymm3          \n"
+      "vpackuswb   %%ymm1,%%ymm2,%%ymm1          \n"  // BG (planar)
+      "vpackuswb   %%ymm3,%%ymm0,%%ymm0          \n"  // RA (planar)
+      "vpshufb     %%ymm5,%%ymm1,%%ymm1          \n"  // BG (interleave)
+      "vpshufb     %%ymm5,%%ymm0,%%ymm0          \n"  // RA (interleave)
+      "vpermq      $0xd8,%%ymm1,%%ymm1           \n"
+      "vpermq      $0xd8,%%ymm0,%%ymm0           \n"
+      "vpunpcklwd  %%ymm0,%%ymm1,%%ymm2          \n"  // BGRA (low)
+      "vpunpckhwd  %%ymm0,%%ymm1,%%ymm0          \n"  // BGRA (hi)
+      "vmovdqu     %%ymm2,(%4)                   \n"
+      "vmovdqu     %%ymm0,0x20(%4)               \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x40(%4),%4                   \n"
+      "subl        $0x10,%5                      \n"
+      "jg          1b                            \n"
+      "vzeroupper                                \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(src_a),     // %3
+        "+r"(dst_argb),  // %4
+#if defined(__i386__)
+        "+m"(width)  // %5
+#else
+        "+rm"(width)          // %5
+#endif
+      : "m"(shift),                 // %6
+        "m"(MergeARGB16To8Shuffle)  // %7
+      : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6");
+}
+#endif
+
+#ifdef HAS_MERGEXRGB16TO8ROW_AVX2
+void MergeXRGB16To8Row_AVX2(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width) {
+  int shift = depth - 8;
+  asm volatile(
+
+      "sub         %0,%1                         \n"
+      "sub         %0,%2                         \n"
+      "vbroadcastf128 %6,%%ymm5                  \n"
+      "vmovd       %5,%%xmm6                     \n"
+      "vpcmpeqb    %%ymm3,%%ymm3,%%ymm3          \n"
+      "vpsrlw      $8,%%ymm3,%%ymm3              \n"  // A (0xff)
+
+      LABELALIGN
+      "1:                                        \n"
+      "vmovdqu     (%0),%%ymm0                   \n"  // R
+      "vmovdqu     (%0,%1),%%ymm1                \n"  // G
+      "vmovdqu     (%0,%2),%%ymm2                \n"  // B
+      "vpsrlw      %%xmm6,%%ymm0,%%ymm0          \n"
+      "vpsrlw      %%xmm6,%%ymm1,%%ymm1          \n"
+      "vpsrlw      %%xmm6,%%ymm2,%%ymm2          \n"
+      "vpackuswb   %%ymm1,%%ymm2,%%ymm1          \n"  // BG (planar)
+      "vpackuswb   %%ymm3,%%ymm0,%%ymm0          \n"  // RA (planar)
+      "vpshufb     %%ymm5,%%ymm1,%%ymm1          \n"  // BG (interleave)
+      "vpshufb     %%ymm5,%%ymm0,%%ymm0          \n"  // RA (interleave)
+      "vpermq      $0xd8,%%ymm1,%%ymm1           \n"
+      "vpermq      $0xd8,%%ymm0,%%ymm0           \n"
+      "vpunpcklwd  %%ymm0,%%ymm1,%%ymm2          \n"  // BGRA (low)
+      "vpunpckhwd  %%ymm0,%%ymm1,%%ymm0          \n"  // BGRA (hi)
+      "vmovdqu     %%ymm2,(%3)                   \n"
+      "vmovdqu     %%ymm0,0x20(%3)               \n"
+      "lea         0x20(%0),%0                   \n"
+      "lea         0x40(%3),%3                   \n"
+      "subl        $0x10,%4                      \n"
+      "jg          1b                            \n"
+      "vzeroupper                                \n"
+      : "+r"(src_r),                // %0
+        "+r"(src_g),                // %1
+        "+r"(src_b),                // %2
+        "+r"(dst_argb),             // %3
+        "+r"(width)                 // %4
+      : "m"(shift),                 // %5
+        "m"(MergeARGB16To8Shuffle)  // %6
+      : "memory", "cc", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6");
 }
 #endif
 
@@ -6188,7 +6718,7 @@ static const uvec8 kShuffleAlpha = {3u,  0x80, 3u,  0x80, 7u,  0x80, 7u,  0x80,
                                     11u, 0x80, 11u, 0x80, 15u, 0x80, 15u, 0x80};
 
 // Blend 8 pixels at a time
-void ARGBBlendRow_SSSE3(const uint8_t* src_argb0,
+void ARGBBlendRow_SSSE3(const uint8_t* src_argb,
                         const uint8_t* src_argb1,
                         uint8_t* dst_argb,
                         int width) {
@@ -6259,7 +6789,7 @@ void ARGBBlendRow_SSSE3(const uint8_t* src_argb0,
       "sub         $0x1,%3                       \n"
       "jge         91b                           \n"
       "99:                                       \n"
-      : "+r"(src_argb0),    // %0
+      : "+r"(src_argb),     // %0
         "+r"(src_argb1),    // %1
         "+r"(dst_argb),     // %2
         "+r"(width)         // %3
@@ -6861,7 +7391,7 @@ void ARGBShadeRow_SSE2(const uint8_t* src_argb,
 
 #ifdef HAS_ARGBMULTIPLYROW_SSE2
 // Multiply 2 rows of ARGB pixels together, 4 pixels at a time.
-void ARGBMultiplyRow_SSE2(const uint8_t* src_argb0,
+void ARGBMultiplyRow_SSE2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -6889,7 +7419,7 @@ void ARGBMultiplyRow_SSE2(const uint8_t* src_argb0,
       "lea         0x10(%2),%2                   \n"
       "sub         $0x4,%3                       \n"
       "jg          1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -6900,7 +7430,7 @@ void ARGBMultiplyRow_SSE2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBMULTIPLYROW_AVX2
 // Multiply 2 rows of ARGB pixels together, 8 pixels at a time.
-void ARGBMultiplyRow_AVX2(const uint8_t* src_argb0,
+void ARGBMultiplyRow_AVX2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -6927,7 +7457,7 @@ void ARGBMultiplyRow_AVX2(const uint8_t* src_argb0,
       "sub         $0x8,%3                       \n"
       "jg          1b                            \n"
       "vzeroupper                                \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -6938,7 +7468,7 @@ void ARGBMultiplyRow_AVX2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBADDROW_SSE2
 // Add 2 rows of ARGB pixels together, 4 pixels at a time.
-void ARGBAddRow_SSE2(const uint8_t* src_argb0,
+void ARGBAddRow_SSE2(const uint8_t* src_argb,
                      const uint8_t* src_argb1,
                      uint8_t* dst_argb,
                      int width) {
@@ -6955,7 +7485,7 @@ void ARGBAddRow_SSE2(const uint8_t* src_argb0,
       "lea         0x10(%2),%2                   \n"
       "sub         $0x4,%3                       \n"
       "jg          1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -6966,7 +7496,7 @@ void ARGBAddRow_SSE2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBADDROW_AVX2
 // Add 2 rows of ARGB pixels together, 4 pixels at a time.
-void ARGBAddRow_AVX2(const uint8_t* src_argb0,
+void ARGBAddRow_AVX2(const uint8_t* src_argb,
                      const uint8_t* src_argb1,
                      uint8_t* dst_argb,
                      int width) {
@@ -6983,7 +7513,7 @@ void ARGBAddRow_AVX2(const uint8_t* src_argb0,
       "sub         $0x8,%3                       \n"
       "jg          1b                            \n"
       "vzeroupper                                \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -6994,7 +7524,7 @@ void ARGBAddRow_AVX2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBSUBTRACTROW_SSE2
 // Subtract 2 rows of ARGB pixels, 4 pixels at a time.
-void ARGBSubtractRow_SSE2(const uint8_t* src_argb0,
+void ARGBSubtractRow_SSE2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -7011,7 +7541,7 @@ void ARGBSubtractRow_SSE2(const uint8_t* src_argb0,
       "lea         0x10(%2),%2                   \n"
       "sub         $0x4,%3                       \n"
       "jg          1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -7022,7 +7552,7 @@ void ARGBSubtractRow_SSE2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBSUBTRACTROW_AVX2
 // Subtract 2 rows of ARGB pixels, 8 pixels at a time.
-void ARGBSubtractRow_AVX2(const uint8_t* src_argb0,
+void ARGBSubtractRow_AVX2(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -7039,7 +7569,7 @@ void ARGBSubtractRow_AVX2(const uint8_t* src_argb0,
       "sub         $0x8,%3                       \n"
       "jg          1b                            \n"
       "vzeroupper                                \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
diff --git a/source/row_mmi.cc b/source/row_mmi.cc
index 9a8e2cb2..362fd1cf 100644
--- a/source/row_mmi.cc
+++ b/source/row_mmi.cc
@@ -605,7 +605,7 @@ void ARGBToARGB4444Row_MMI(const uint8_t* src_argb,
       : "memory");
 }
 
-void ARGBToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void ARGBToYRow_MMI(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   uint64_t src, src_hi, src_lo;
   uint64_t dest0, dest1, dest2, dest3;
   const uint64_t value = 0x1080;
@@ -613,8 +613,8 @@ void ARGBToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
 
   __asm__ volatile(
       "1:                                                           \n\t"
-      "gsldlc1    %[src],          0x07(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x00(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x07(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x00(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -626,8 +626,8 @@ void ARGBToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest0],        %[dest0],          %[src]        \n\t"
       "psrlw      %[dest0],        %[dest0],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x0f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x08(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x0f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x08(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -639,8 +639,8 @@ void ARGBToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest1],        %[dest1],          %[src]        \n\t"
       "psrlw      %[dest1],        %[dest1],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x17(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x10(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x17(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x10(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -652,8 +652,8 @@ void ARGBToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest2],        %[dest2],          %[src]        \n\t"
       "psrlw      %[dest2],        %[dest2],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x1f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x18(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x1f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x18(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -671,20 +671,20 @@ void ARGBToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "gssdlc1    %[dest0],        0x07(%[dst_y])                   \n\t"
       "gssdrc1    %[dest0],        0x00(%[dst_y])                   \n\t"
 
-      "daddiu     %[src_argb0],    %[src_argb0],      0x20          \n\t"
+      "daddiu     %[src_argb],    %[src_argb],      0x20          \n\t"
       "daddiu     %[dst_y],        %[dst_y],          0x08          \n\t"
       "daddi      %[width],        %[width],         -0x08          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src] "=&f"(src), [src_hi] "=&f"(src_hi), [src_lo] "=&f"(src_lo),
         [dest0] "=&f"(dest0), [dest1] "=&f"(dest1), [dest2] "=&f"(dest2),
         [dest3] "=&f"(dest3)
-      : [src_argb0] "r"(src_argb0), [dst_y] "r"(dst_y), [width] "r"(width),
+      : [src_argb] "r"(src_argb), [dst_y] "r"(dst_y), [width] "r"(width),
         [mask] "f"(mask), [value] "f"(value), [eight] "f"(0x08),
         [zero] "f"(0x00)
       : "memory");
 }
 
-void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
+void ARGBToUVRow_MMI(const uint8_t* src_rgb,
                      int src_stride_rgb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
@@ -700,9 +700,9 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "dli        %[tmp0],         0x0001000100010001                   \n\t"
       "dmtc1      %[tmp0],         %[ftmp12]                            \n\t"
       "1:                                                               \n\t"
-      "daddu      %[src_rgb1],     %[src_rgb0],       %[src_stride_rgb] \n\t"
-      "gsldrc1    %[src0],         0x00(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x07(%[src_rgb0])                    \n\t"
+      "daddu      %[src_rgb1],     %[src_rgb],       %[src_stride_rgb] \n\t"
+      "gsldrc1    %[src0],         0x00(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x07(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x00(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x07(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -720,8 +720,8 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest0_u],      %[dest0_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest0_v],      %[dest0_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x08(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x0f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x08(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x0f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x08(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x0f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -748,8 +748,8 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest0_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest0_v],      %[dest0_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x10(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x17(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x10(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x17(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x10(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x17(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -767,8 +767,8 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest1_u],      %[dest1_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest1_v],      %[dest1_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x18(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x1f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x18(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x1f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x18(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x1f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -795,8 +795,8 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest1_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest1_v],      %[dest1_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x20(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x27(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x20(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x27(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x20(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x27(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -814,8 +814,8 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest2_u],      %[dest2_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest2_v],      %[dest2_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x28(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x2f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x28(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x2f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x28(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x2f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -842,8 +842,8 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest2_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest2_v],      %[dest2_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x30(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x37(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x30(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x37(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x30(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x37(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -861,8 +861,8 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest3_u],      %[dest3_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest3_v],      %[dest3_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x38(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x3f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x38(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x3f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x38(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x3f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -901,7 +901,7 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       "gssdlc1    %[dest0_v],      0x07(%[dst_v])                       \n\t"
       "gssdrc1    %[dest0_v],      0x00(%[dst_v])                       \n\t"
 
-      "daddiu     %[src_rgb0],     %[src_rgb0],       0x40              \n\t"
+      "daddiu     %[src_rgb],     %[src_rgb],       0x40              \n\t"
       "daddiu     %[dst_u],        %[dst_u],          0x08              \n\t"
       "daddiu     %[dst_v],        %[dst_v],          0x08              \n\t"
       "daddi      %[width],        %[width],         -0x10              \n\t"
@@ -913,7 +913,7 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
         [dest2_u] "=&f"(ftmp[8]), [dest2_v] "=&f"(ftmp[9]),
         [dest3_u] "=&f"(ftmp[10]), [dest3_v] "=&f"(ftmp[11]),
         [ftmp12] "=&f"(ftmp[12]), [tmp0] "=&r"(tmp[0])
-      : [src_rgb0] "r"(src_rgb0), [src_stride_rgb] "r"(src_stride_rgb),
+      : [src_rgb] "r"(src_rgb), [src_stride_rgb] "r"(src_stride_rgb),
         [dst_u] "r"(dst_u), [dst_v] "r"(dst_v), [width] "r"(width),
         [mask_u] "f"(mask_u), [mask_v] "f"(mask_v), [value] "f"(value),
         [zero] "f"(0x00), [eight] "f"(0x08), [one] "f"(0x01),
@@ -921,7 +921,7 @@ void ARGBToUVRow_MMI(const uint8_t* src_rgb0,
       : "memory");
 }
 
-void BGRAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void BGRAToYRow_MMI(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   uint64_t src, src_hi, src_lo;
   uint64_t dest0, dest1, dest2, dest3;
   const uint64_t value = 0x1080;
@@ -929,8 +929,8 @@ void BGRAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
 
   __asm__ volatile(
       "1:                                                           \n\t"
-      "gsldlc1    %[src],          0x07(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x00(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x07(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x00(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -942,8 +942,8 @@ void BGRAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest0],        %[dest0],          %[src]        \n\t"
       "psrlw      %[dest0],        %[dest0],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x0f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x08(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x0f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x08(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -955,8 +955,8 @@ void BGRAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest1],        %[dest1],          %[src]        \n\t"
       "psrlw      %[dest1],        %[dest1],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x17(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x10(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x17(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x10(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -968,8 +968,8 @@ void BGRAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest2],        %[dest2],          %[src]        \n\t"
       "psrlw      %[dest2],        %[dest2],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x1f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x18(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x1f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x18(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -987,20 +987,20 @@ void BGRAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "gssdlc1    %[dest0],        0x07(%[dst_y])                   \n\t"
       "gssdrc1    %[dest0],        0x00(%[dst_y])                   \n\t"
 
-      "daddiu     %[src_argb0],    %[src_argb0],      0x20          \n\t"
+      "daddiu     %[src_argb],    %[src_argb],      0x20          \n\t"
       "daddiu     %[dst_y],        %[dst_y],          0x08          \n\t"
       "daddi      %[width],        %[width],         -0x08          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src] "=&f"(src), [src_hi] "=&f"(src_hi), [src_lo] "=&f"(src_lo),
         [dest0] "=&f"(dest0), [dest1] "=&f"(dest1), [dest2] "=&f"(dest2),
         [dest3] "=&f"(dest3)
-      : [src_argb0] "r"(src_argb0), [dst_y] "r"(dst_y), [width] "r"(width),
+      : [src_argb] "r"(src_argb), [dst_y] "r"(dst_y), [width] "r"(width),
         [mask] "f"(mask), [value] "f"(value), [eight] "f"(0x08),
         [zero] "f"(0x00)
       : "memory");
 }
 
-void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
+void BGRAToUVRow_MMI(const uint8_t* src_rgb,
                      int src_stride_rgb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
@@ -1016,9 +1016,9 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "dli        %[tmp0],         0x0001000100010001                   \n\t"
       "dmtc1      %[tmp0],         %[ftmp12]                            \n\t"
       "1:                                                               \n\t"
-      "daddu      %[src_rgb1],     %[src_rgb0],       %[src_stride_rgb] \n\t"
-      "gsldrc1    %[src0],         0x00(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x07(%[src_rgb0])                    \n\t"
+      "daddu      %[src_rgb1],     %[src_rgb],       %[src_stride_rgb] \n\t"
+      "gsldrc1    %[src0],         0x00(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x07(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x00(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x07(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1036,8 +1036,8 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest0_u],      %[dest0_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest0_v],      %[dest0_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x08(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x0f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x08(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x0f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x08(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x0f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1064,8 +1064,8 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest0_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest0_v],      %[dest0_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x10(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x17(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x10(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x17(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x10(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x17(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1083,8 +1083,8 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest1_u],      %[dest1_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest1_v],      %[dest1_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x18(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x1f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x18(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x1f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x18(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x1f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1111,8 +1111,8 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest1_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest1_v],      %[dest1_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x20(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x27(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x20(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x27(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x20(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x27(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1130,8 +1130,8 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest2_u],      %[dest2_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest2_v],      %[dest2_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x28(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x2f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x28(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x2f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x28(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x2f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1158,8 +1158,8 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest2_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest2_v],      %[dest2_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x30(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x37(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x30(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x37(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x30(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x37(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1177,8 +1177,8 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest3_u],      %[dest3_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest3_v],      %[dest3_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x38(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x3f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x38(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x3f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x38(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x3f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1217,7 +1217,7 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       "gssdlc1    %[dest0_v],      0x07(%[dst_v])                       \n\t"
       "gssdrc1    %[dest0_v],      0x00(%[dst_v])                       \n\t"
 
-      "daddiu     %[src_rgb0],     %[src_rgb0],       0x40              \n\t"
+      "daddiu     %[src_rgb],     %[src_rgb],       0x40              \n\t"
       "daddiu     %[dst_u],        %[dst_u],          0x08              \n\t"
       "daddiu     %[dst_v],        %[dst_v],          0x08              \n\t"
       "daddi      %[width],        %[width],         -0x10              \n\t"
@@ -1229,7 +1229,7 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
         [dest2_u] "=&f"(ftmp[8]), [dest2_v] "=&f"(ftmp[9]),
         [dest3_u] "=&f"(ftmp[10]), [dest3_v] "=&f"(ftmp[11]),
         [ftmp12] "=&f"(ftmp[12]), [tmp0] "=&r"(tmp[0])
-      : [src_rgb0] "r"(src_rgb0), [src_stride_rgb] "r"(src_stride_rgb),
+      : [src_rgb] "r"(src_rgb), [src_stride_rgb] "r"(src_stride_rgb),
         [dst_u] "r"(dst_u), [dst_v] "r"(dst_v), [width] "r"(width),
         [mask_u] "f"(mask_u), [mask_v] "f"(mask_v), [value] "f"(value),
         [zero] "f"(0x00), [eight] "f"(0x08), [one] "f"(0x01),
@@ -1237,7 +1237,7 @@ void BGRAToUVRow_MMI(const uint8_t* src_rgb0,
       : "memory");
 }
 
-void ABGRToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void ABGRToYRow_MMI(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   uint64_t src, src_hi, src_lo;
   uint64_t dest0, dest1, dest2, dest3;
   const uint64_t value = 0x1080;
@@ -1245,8 +1245,8 @@ void ABGRToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
 
   __asm__ volatile(
       "1:                                                           \n\t"
-      "gsldlc1    %[src],          0x07(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x00(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x07(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x00(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1258,8 +1258,8 @@ void ABGRToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest0],        %[dest0],          %[src]        \n\t"
       "psrlw      %[dest0],        %[dest0],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x0f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x08(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x0f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x08(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1271,8 +1271,8 @@ void ABGRToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest1],        %[dest1],          %[src]        \n\t"
       "psrlw      %[dest1],        %[dest1],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x17(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x10(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x17(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x10(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1284,8 +1284,8 @@ void ABGRToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest2],        %[dest2],          %[src]        \n\t"
       "psrlw      %[dest2],        %[dest2],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x1f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x18(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x1f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x18(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1303,20 +1303,20 @@ void ABGRToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "gssdlc1    %[dest0],        0x07(%[dst_y])                   \n\t"
       "gssdrc1    %[dest0],        0x00(%[dst_y])                   \n\t"
 
-      "daddiu     %[src_argb0],    %[src_argb0],      0x20          \n\t"
+      "daddiu     %[src_argb],    %[src_argb],      0x20          \n\t"
       "daddiu     %[dst_y],        %[dst_y],          0x08          \n\t"
       "daddi      %[width],        %[width],         -0x08          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src] "=&f"(src), [src_hi] "=&f"(src_hi), [src_lo] "=&f"(src_lo),
         [dest0] "=&f"(dest0), [dest1] "=&f"(dest1), [dest2] "=&f"(dest2),
         [dest3] "=&f"(dest3)
-      : [src_argb0] "r"(src_argb0), [dst_y] "r"(dst_y), [width] "r"(width),
+      : [src_argb] "r"(src_argb), [dst_y] "r"(dst_y), [width] "r"(width),
         [mask] "f"(mask), [value] "f"(value), [eight] "f"(0x08),
         [zero] "f"(0x00)
       : "memory");
 }
 
-void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
+void ABGRToUVRow_MMI(const uint8_t* src_rgb,
                      int src_stride_rgb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
@@ -1332,9 +1332,9 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "dli        %[tmp0],         0x0001000100010001                   \n\t"
       "dmtc1      %[tmp0],         %[ftmp12]                            \n\t"
       "1:                                                               \n\t"
-      "daddu      %[src_rgb1],     %[src_rgb0],       %[src_stride_rgb] \n\t"
-      "gsldrc1    %[src0],         0x00(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x07(%[src_rgb0])                    \n\t"
+      "daddu      %[src_rgb1],     %[src_rgb],       %[src_stride_rgb] \n\t"
+      "gsldrc1    %[src0],         0x00(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x07(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x00(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x07(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1352,8 +1352,8 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest0_u],      %[dest0_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest0_v],      %[dest0_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x08(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x0f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x08(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x0f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x08(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x0f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1380,8 +1380,8 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest0_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest0_v],      %[dest0_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x10(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x17(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x10(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x17(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x10(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x17(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1399,8 +1399,8 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest1_u],      %[dest1_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest1_v],      %[dest1_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x18(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x1f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x18(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x1f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x18(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x1f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1427,8 +1427,8 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest1_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest1_v],      %[dest1_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x20(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x27(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x20(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x27(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x20(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x27(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1446,8 +1446,8 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest2_u],      %[dest2_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest2_v],      %[dest2_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x28(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x2f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x28(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x2f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x28(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x2f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1474,8 +1474,8 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest2_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest2_v],      %[dest2_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x30(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x37(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x30(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x37(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x30(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x37(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1493,8 +1493,8 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest3_u],      %[dest3_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest3_v],      %[dest3_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x38(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x3f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x38(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x3f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x38(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x3f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1533,7 +1533,7 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       "gssdlc1    %[dest0_v],      0x07(%[dst_v])                       \n\t"
       "gssdrc1    %[dest0_v],      0x00(%[dst_v])                       \n\t"
 
-      "daddiu     %[src_rgb0],     %[src_rgb0],       0x40              \n\t"
+      "daddiu     %[src_rgb],     %[src_rgb],       0x40              \n\t"
       "daddiu     %[dst_u],        %[dst_u],          0x08              \n\t"
       "daddiu     %[dst_v],        %[dst_v],          0x08              \n\t"
       "daddi      %[width],        %[width],         -0x10              \n\t"
@@ -1545,7 +1545,7 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
         [dest2_u] "=&f"(ftmp[8]), [dest2_v] "=&f"(ftmp[9]),
         [dest3_u] "=&f"(ftmp[10]), [dest3_v] "=&f"(ftmp[11]),
         [ftmp12] "=&f"(ftmp[12]), [tmp0] "=&r"(tmp[0])
-      : [src_rgb0] "r"(src_rgb0), [src_stride_rgb] "r"(src_stride_rgb),
+      : [src_rgb] "r"(src_rgb), [src_stride_rgb] "r"(src_stride_rgb),
         [dst_u] "r"(dst_u), [dst_v] "r"(dst_v), [width] "r"(width),
         [mask_u] "f"(mask_u), [mask_v] "f"(mask_v), [value] "f"(value),
         [zero] "f"(0x00), [eight] "f"(0x08), [one] "f"(0x01),
@@ -1553,7 +1553,7 @@ void ABGRToUVRow_MMI(const uint8_t* src_rgb0,
       : "memory");
 }
 
-void RGBAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void RGBAToYRow_MMI(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   uint64_t src, src_hi, src_lo;
   uint64_t dest0, dest1, dest2, dest3;
   const uint64_t value = 0x1080;
@@ -1561,8 +1561,8 @@ void RGBAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
 
   __asm__ volatile(
       "1:                                                           \n\t"
-      "gsldlc1    %[src],          0x07(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x00(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x07(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x00(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1574,8 +1574,8 @@ void RGBAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest0],        %[dest0],          %[src]        \n\t"
       "psrlw      %[dest0],        %[dest0],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x0f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x08(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x0f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x08(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1587,8 +1587,8 @@ void RGBAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest1],        %[dest1],          %[src]        \n\t"
       "psrlw      %[dest1],        %[dest1],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x17(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x10(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x17(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x10(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1600,8 +1600,8 @@ void RGBAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest2],        %[dest2],          %[src]        \n\t"
       "psrlw      %[dest2],        %[dest2],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x1f(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x18(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x1f(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x18(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_0   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1619,20 +1619,20 @@ void RGBAToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "gssdlc1    %[dest0],        0x07(%[dst_y])                   \n\t"
       "gssdrc1    %[dest0],        0x00(%[dst_y])                   \n\t"
 
-      "daddiu     %[src_argb0],    %[src_argb0],      0x20          \n\t"
+      "daddiu     %[src_argb],    %[src_argb],      0x20          \n\t"
       "daddiu     %[dst_y],        %[dst_y],          0x08          \n\t"
       "daddi      %[width],        %[width],         -0x08          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src] "=&f"(src), [src_hi] "=&f"(src_hi), [src_lo] "=&f"(src_lo),
         [dest0] "=&f"(dest0), [dest1] "=&f"(dest1), [dest2] "=&f"(dest2),
         [dest3] "=&f"(dest3)
-      : [src_argb0] "r"(src_argb0), [dst_y] "r"(dst_y), [width] "r"(width),
+      : [src_argb] "r"(src_argb), [dst_y] "r"(dst_y), [width] "r"(width),
         [mask] "f"(mask), [value] "f"(value), [eight] "f"(0x08),
         [zero] "f"(0x00)
       : "memory");
 }
 
-void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
+void RGBAToUVRow_MMI(const uint8_t* src_rgb,
                      int src_stride_rgb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
@@ -1648,9 +1648,9 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "dli        %[tmp0],         0x0001000100010001                   \n\t"
       "dmtc1      %[tmp0],         %[ftmp12]                            \n\t"
       "1:                                                               \n\t"
-      "daddu      %[src_rgb1],     %[src_rgb0],       %[src_stride_rgb] \n\t"
-      "gsldrc1    %[src0],         0x00(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x07(%[src_rgb0])                    \n\t"
+      "daddu      %[src_rgb1],     %[src_rgb],       %[src_stride_rgb] \n\t"
+      "gsldrc1    %[src0],         0x00(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x07(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x00(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x07(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1668,8 +1668,8 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest0_u],      %[dest0_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest0_v],      %[dest0_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x08(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x0f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x08(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x0f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x08(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x0f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1696,8 +1696,8 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest0_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest0_v],      %[dest0_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x10(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x17(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x10(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x17(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x10(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x17(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1715,8 +1715,8 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest1_u],      %[dest1_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest1_v],      %[dest1_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x18(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x1f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x18(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x1f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x18(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x1f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1743,8 +1743,8 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest1_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest1_v],      %[dest1_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x20(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x27(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x20(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x27(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x20(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x27(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1762,8 +1762,8 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest2_u],      %[dest2_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest2_v],      %[dest2_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x28(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x2f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x28(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x2f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x28(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x2f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1790,8 +1790,8 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest2_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest2_v],      %[dest2_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x30(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x37(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x30(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x37(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x30(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x37(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1809,8 +1809,8 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest3_u],      %[dest3_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest3_v],      %[dest3_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x38(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x3f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x38(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x3f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x38(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x3f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1849,7 +1849,7 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       "gssdlc1    %[dest0_v],      0x07(%[dst_v])                       \n\t"
       "gssdrc1    %[dest0_v],      0x00(%[dst_v])                       \n\t"
 
-      "daddiu     %[src_rgb0],     %[src_rgb0],       0x40              \n\t"
+      "daddiu     %[src_rgb],     %[src_rgb],       0x40              \n\t"
       "daddiu     %[dst_u],        %[dst_u],          0x08              \n\t"
       "daddiu     %[dst_v],        %[dst_v],          0x08              \n\t"
       "daddi      %[width],        %[width],         -0x10              \n\t"
@@ -1861,7 +1861,7 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
         [dest2_u] "=&f"(ftmp[8]), [dest2_v] "=&f"(ftmp[9]),
         [dest3_u] "=&f"(ftmp[10]), [dest3_v] "=&f"(ftmp[11]),
         [ftmp12] "=&f"(ftmp[12]), [tmp0] "=&r"(tmp[0])
-      : [src_rgb0] "r"(src_rgb0), [src_stride_rgb] "r"(src_stride_rgb),
+      : [src_rgb] "r"(src_rgb), [src_stride_rgb] "r"(src_stride_rgb),
         [dst_u] "r"(dst_u), [dst_v] "r"(dst_v), [width] "r"(width),
         [mask_u] "f"(mask_u), [mask_v] "f"(mask_v), [value] "f"(value),
         [zero] "f"(0x00), [eight] "f"(0x08), [one] "f"(0x01),
@@ -1869,7 +1869,7 @@ void RGBAToUVRow_MMI(const uint8_t* src_rgb0,
       : "memory");
 }
 
-void RGB24ToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void RGB24ToYRow_MMI(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   uint64_t src, src_hi, src_lo;
   uint64_t dest0, dest1, dest2, dest3;
   const uint64_t value = 0x1080;
@@ -1877,8 +1877,8 @@ void RGB24ToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
 
   __asm__ volatile(
       "1:                                                           \n\t"
-      "gsldlc1    %[src],          0x07(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x00(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x07(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x00(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1891,8 +1891,8 @@ void RGB24ToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest0],        %[dest0],          %[src]        \n\t"
       "psrlw      %[dest0],        %[dest0],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x0d(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x06(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x0d(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x06(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1905,8 +1905,8 @@ void RGB24ToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest1],        %[dest1],          %[src]        \n\t"
       "psrlw      %[dest1],        %[dest1],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x13(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x0c(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x13(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x0c(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1919,8 +1919,8 @@ void RGB24ToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest2],        %[dest2],          %[src]        \n\t"
       "psrlw      %[dest2],        %[dest2],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x19(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x12(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x19(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x12(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -1939,20 +1939,20 @@ void RGB24ToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "gssdlc1    %[dest0],        0x07(%[dst_y])                   \n\t"
       "gssdrc1    %[dest0],        0x00(%[dst_y])                   \n\t"
 
-      "daddiu     %[src_argb0],    %[src_argb0],      0x18          \n\t"
+      "daddiu     %[src_argb],    %[src_argb],      0x18          \n\t"
       "daddiu     %[dst_y],        %[dst_y],          0x08          \n\t"
       "daddi      %[width],        %[width],         -0x08          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src] "=&f"(src), [src_hi] "=&f"(src_hi), [src_lo] "=&f"(src_lo),
         [dest0] "=&f"(dest0), [dest1] "=&f"(dest1), [dest2] "=&f"(dest2),
         [dest3] "=&f"(dest3)
-      : [src_argb0] "r"(src_argb0), [dst_y] "r"(dst_y), [width] "r"(width),
+      : [src_argb] "r"(src_argb), [dst_y] "r"(dst_y), [width] "r"(width),
         [mask] "f"(mask), [value] "f"(value), [eight] "f"(0x08),
         [zero] "f"(0x00)
       : "memory");
 }
 
-void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
+void RGB24ToUVRow_MMI(const uint8_t* src_rgb,
                       int src_stride_rgb,
                       uint8_t* dst_u,
                       uint8_t* dst_v,
@@ -1968,9 +1968,9 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "dli        %[tmp0],         0x0001000100010001                   \n\t"
       "dmtc1      %[tmp0],         %[ftmp12]                            \n\t"
       "1:                                                               \n\t"
-      "daddu      %[src_rgb1],     %[src_rgb0],       %[src_stride_rgb] \n\t"
-      "gsldrc1    %[src0],         0x00(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x07(%[src_rgb0])                    \n\t"
+      "daddu      %[src_rgb1],     %[src_rgb],       %[src_stride_rgb] \n\t"
+      "gsldrc1    %[src0],         0x00(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x07(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x00(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x07(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -1990,8 +1990,8 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest0_u],      %[dest0_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest0_v],      %[dest0_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x06(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x0d(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x06(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x0d(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x06(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x0d(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2020,8 +2020,8 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest0_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest0_v],      %[dest0_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x0c(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x13(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x0c(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x13(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x0c(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x13(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2041,8 +2041,8 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest1_u],      %[dest1_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest1_v],      %[dest1_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x12(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x19(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x12(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x19(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x12(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x19(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2071,8 +2071,8 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest1_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest1_v],      %[dest1_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x18(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x1f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x18(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x1f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x18(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x1f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2092,8 +2092,8 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest2_u],      %[dest2_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest2_v],      %[dest2_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x1e(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x25(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x1e(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x25(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x1e(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x25(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2122,8 +2122,8 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest2_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest2_v],      %[dest2_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x24(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x2b(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x24(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x2b(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x24(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x2b(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2143,8 +2143,8 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest3_u],      %[dest3_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest3_v],      %[dest3_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x2a(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x31(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x2a(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x31(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x2a(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x31(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2185,7 +2185,7 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       "gssdlc1    %[dest0_v],      0x07(%[dst_v])                       \n\t"
       "gssdrc1    %[dest0_v],      0x00(%[dst_v])                       \n\t"
 
-      "daddiu     %[src_rgb0],     %[src_rgb0],       0x30              \n\t"
+      "daddiu     %[src_rgb],     %[src_rgb],       0x30              \n\t"
       "daddiu     %[dst_u],        %[dst_u],          0x08              \n\t"
       "daddiu     %[dst_v],        %[dst_v],          0x08              \n\t"
       "daddi      %[width],        %[width],         -0x10              \n\t"
@@ -2197,7 +2197,7 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
         [dest2_u] "=&f"(ftmp[8]), [dest2_v] "=&f"(ftmp[9]),
         [dest3_u] "=&f"(ftmp[10]), [dest3_v] "=&f"(ftmp[11]),
         [ftmp12] "=&f"(ftmp[12]), [tmp0] "=&r"(tmp[0])
-      : [src_rgb0] "r"(src_rgb0), [src_stride_rgb] "r"(src_stride_rgb),
+      : [src_rgb] "r"(src_rgb), [src_stride_rgb] "r"(src_stride_rgb),
         [dst_u] "r"(dst_u), [dst_v] "r"(dst_v), [width] "r"(width),
         [mask_u] "f"(mask_u), [mask_v] "f"(mask_v), [value] "f"(value),
         [zero] "f"(0x00), [eight] "f"(0x08), [one] "f"(0x01),
@@ -2205,7 +2205,7 @@ void RGB24ToUVRow_MMI(const uint8_t* src_rgb0,
       : "memory");
 }
 
-void RAWToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void RAWToYRow_MMI(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   uint64_t src, src_hi, src_lo;
   uint64_t dest0, dest1, dest2, dest3;
   const uint64_t value = 0x1080;
@@ -2213,8 +2213,8 @@ void RAWToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
 
   __asm__ volatile(
       "1:                                                           \n\t"
-      "gsldlc1    %[src],          0x07(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x00(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x07(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x00(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -2227,8 +2227,8 @@ void RAWToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest0],        %[dest0],          %[src]        \n\t"
       "psrlw      %[dest0],        %[dest0],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x0d(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x06(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x0d(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x06(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -2241,8 +2241,8 @@ void RAWToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest1],        %[dest1],          %[src]        \n\t"
       "psrlw      %[dest1],        %[dest1],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x13(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x0c(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x13(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x0c(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -2255,8 +2255,8 @@ void RAWToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "paddw      %[dest2],        %[dest2],          %[src]        \n\t"
       "psrlw      %[dest2],        %[dest2],          %[eight]      \n\t"
 
-      "gsldlc1    %[src],          0x19(%[src_argb0])               \n\t"
-      "gsldrc1    %[src],          0x12(%[src_argb0])               \n\t"
+      "gsldlc1    %[src],          0x19(%[src_argb])               \n\t"
+      "gsldrc1    %[src],          0x12(%[src_argb])               \n\t"
       "punpcklbh  %[src_lo],       %[src],            %[zero]       \n\t"
       "pinsrh_3   %[src_lo],       %[src_lo],         %[value]      \n\t"
       "pmaddhw    %[src_lo],       %[src_lo],         %[mask]       \n\t"
@@ -2275,20 +2275,20 @@ void RAWToYRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
       "gssdlc1    %[dest0],        0x07(%[dst_y])                   \n\t"
       "gssdrc1    %[dest0],        0x00(%[dst_y])                   \n\t"
 
-      "daddiu     %[src_argb0],    %[src_argb0],      0x18          \n\t"
+      "daddiu     %[src_argb],    %[src_argb],      0x18          \n\t"
       "daddiu     %[dst_y],        %[dst_y],          0x08          \n\t"
       "daddi      %[width],        %[width],         -0x08          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src] "=&f"(src), [src_hi] "=&f"(src_hi), [src_lo] "=&f"(src_lo),
         [dest0] "=&f"(dest0), [dest1] "=&f"(dest1), [dest2] "=&f"(dest2),
         [dest3] "=&f"(dest3)
-      : [src_argb0] "r"(src_argb0), [dst_y] "r"(dst_y), [width] "r"(width),
+      : [src_argb] "r"(src_argb), [dst_y] "r"(dst_y), [width] "r"(width),
         [mask] "f"(mask), [value] "f"(value), [eight] "f"(0x08),
         [zero] "f"(0x00)
       : "memory");
 }
 
-void RAWToUVRow_MMI(const uint8_t* src_rgb0,
+void RAWToUVRow_MMI(const uint8_t* src_rgb,
                     int src_stride_rgb,
                     uint8_t* dst_u,
                     uint8_t* dst_v,
@@ -2304,9 +2304,9 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "dli        %[tmp0],         0x0001000100010001                   \n\t"
       "dmtc1      %[tmp0],         %[ftmp12]                            \n\t"
       "1:                                                               \n\t"
-      "daddu      %[src_rgb1],     %[src_rgb0],       %[src_stride_rgb] \n\t"
-      "gsldrc1    %[src0],         0x00(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x07(%[src_rgb0])                    \n\t"
+      "daddu      %[src_rgb1],     %[src_rgb],       %[src_stride_rgb] \n\t"
+      "gsldrc1    %[src0],         0x00(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x07(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x00(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x07(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2326,8 +2326,8 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest0_u],      %[dest0_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest0_v],      %[dest0_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x06(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x0d(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x06(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x0d(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x06(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x0d(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2356,8 +2356,8 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest0_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest0_v],      %[dest0_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x0c(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x13(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x0c(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x13(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x0c(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x13(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2377,8 +2377,8 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest1_u],      %[dest1_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest1_v],      %[dest1_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x12(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x19(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x12(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x19(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x12(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x19(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2407,8 +2407,8 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest1_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest1_v],      %[dest1_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x18(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x1f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x18(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x1f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x18(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x1f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2428,8 +2428,8 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest2_u],      %[dest2_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest2_v],      %[dest2_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x1e(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x25(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x1e(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x25(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x1e(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x25(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2458,8 +2458,8 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest2_v],      %[src0],           %[src1]           \n\t"
       "psraw      %[dest2_v],      %[dest2_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x24(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x2b(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x24(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x2b(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x24(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x2b(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2479,8 +2479,8 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest3_u],      %[dest3_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest3_v],      %[dest3_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x2a(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x31(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x2a(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x31(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x2a(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x31(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2521,7 +2521,7 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       "gssdlc1    %[dest0_v],      0x07(%[dst_v])                       \n\t"
       "gssdrc1    %[dest0_v],      0x00(%[dst_v])                       \n\t"
 
-      "daddiu     %[src_rgb0],     %[src_rgb0],       0x30              \n\t"
+      "daddiu     %[src_rgb],     %[src_rgb],       0x30              \n\t"
       "daddiu     %[dst_u],        %[dst_u],          0x08              \n\t"
       "daddiu     %[dst_v],        %[dst_v],          0x08              \n\t"
       "daddi      %[width],        %[width],         -0x10              \n\t"
@@ -2533,7 +2533,7 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
         [dest2_u] "=&f"(ftmp[8]), [dest2_v] "=&f"(ftmp[9]),
         [dest3_u] "=&f"(ftmp[10]), [dest3_v] "=&f"(ftmp[11]),
         [ftmp12] "=&f"(ftmp[12]), [tmp0] "=&r"(tmp[0])
-      : [src_rgb0] "r"(src_rgb0), [src_stride_rgb] "r"(src_stride_rgb),
+      : [src_rgb] "r"(src_rgb), [src_stride_rgb] "r"(src_stride_rgb),
         [dst_u] "r"(dst_u), [dst_v] "r"(dst_v), [width] "r"(width),
         [mask_u] "f"(mask_u), [mask_v] "f"(mask_v), [value] "f"(value),
         [zero] "f"(0x00), [eight] "f"(0x08), [one] "f"(0x01),
@@ -2541,7 +2541,7 @@ void RAWToUVRow_MMI(const uint8_t* src_rgb0,
       : "memory");
 }
 
-void ARGBToYJRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void ARGBToYJRow_MMI(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   uint64_t src, src_hi, src_lo;
   uint64_t dest, dest0, dest1, dest2, dest3;
   uint64_t tmp0, tmp1;
@@ -2618,13 +2618,13 @@ void ARGBToYJRow_MMI(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
         [src_lo] "=&f"(src_lo), [dest0] "=&f"(dest0), [dest1] "=&f"(dest1),
         [dest2] "=&f"(dest2), [dest3] "=&f"(dest3), [tmp0] "=&f"(tmp0),
         [tmp1] "=&f"(tmp1)
-      : [src_ptr] "r"(src_argb0), [dst_ptr] "r"(dst_y), [mask0] "f"(mask0),
+      : [src_ptr] "r"(src_argb), [dst_ptr] "r"(dst_y), [mask0] "f"(mask0),
         [mask1] "f"(mask1), [shift] "f"(shift), [value] "f"(value),
         [width] "r"(width)
       : "memory");
 }
 
-void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
+void ARGBToUVJRow_MMI(const uint8_t* src_rgb,
                       int src_stride_rgb,
                       uint8_t* dst_u,
                       uint8_t* dst_v,
@@ -2637,9 +2637,9 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
 
   __asm__ volatile(
       "1:                                                               \n\t"
-      "daddu      %[src_rgb1],     %[src_rgb0],       %[src_stride_rgb] \n\t"
-      "gsldrc1    %[src0],         0x00(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x07(%[src_rgb0])                    \n\t"
+      "daddu      %[src_rgb1],     %[src_rgb],       %[src_stride_rgb] \n\t"
+      "gsldrc1    %[src0],         0x00(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x07(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x00(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x07(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2655,8 +2655,8 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest0_u],      %[dest0_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest0_v],      %[dest0_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x08(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x0f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x08(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x0f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x08(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x0f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2681,8 +2681,8 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest0_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest0_v],      %[dest0_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x10(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x17(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x10(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x17(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x10(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x17(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2698,8 +2698,8 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest1_u],      %[dest1_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest1_v],      %[dest1_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x18(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x1f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x18(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x1f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x18(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x1f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2724,8 +2724,8 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest1_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest1_v],      %[dest1_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x20(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x27(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x20(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x27(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x20(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x27(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2741,8 +2741,8 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest2_u],      %[dest2_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest2_v],      %[dest2_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x28(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x2f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x28(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x2f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x28(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x2f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2767,8 +2767,8 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "psubw      %[dest2_v],      %[src1],           %[src0]           \n\t"
       "psraw      %[dest2_v],      %[dest2_v],        %[eight]          \n\t"
 
-      "gsldrc1    %[src0],         0x30(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x37(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x30(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x37(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x30(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x37(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2784,8 +2784,8 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "pmaddhw    %[dest3_u],      %[dest3_u],        %[mask_u]         \n\t"
       "pmaddhw    %[dest3_v],      %[dest3_v],        %[mask_v]         \n\t"
 
-      "gsldrc1    %[src0],         0x38(%[src_rgb0])                    \n\t"
-      "gsldlc1    %[src0],         0x3f(%[src_rgb0])                    \n\t"
+      "gsldrc1    %[src0],         0x38(%[src_rgb])                    \n\t"
+      "gsldlc1    %[src0],         0x3f(%[src_rgb])                    \n\t"
       "gsldrc1    %[src1],         0x38(%[src_rgb1])                    \n\t"
       "gsldlc1    %[src1],         0x3f(%[src_rgb1])                    \n\t"
       "punpcklbh  %[src_lo],       %[src0],           %[zero]           \n\t"
@@ -2822,7 +2822,7 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
       "gssdlc1    %[dest0_v],      0x07(%[dst_v])                       \n\t"
       "gssdrc1    %[dest0_v],      0x00(%[dst_v])                       \n\t"
 
-      "daddiu     %[src_rgb0],     %[src_rgb0],       0x40              \n\t"
+      "daddiu     %[src_rgb],     %[src_rgb],       0x40              \n\t"
       "daddiu     %[dst_u],        %[dst_u],          0x08              \n\t"
       "daddiu     %[dst_v],        %[dst_v],          0x08              \n\t"
       "daddi      %[width],        %[width],         -0x10              \n\t"
@@ -2833,7 +2833,7 @@ void ARGBToUVJRow_MMI(const uint8_t* src_rgb0,
         [dest1_u] "=&f"(ftmp[6]), [dest1_v] "=&f"(ftmp[7]),
         [dest2_u] "=&f"(ftmp[8]), [dest2_v] "=&f"(ftmp[9]),
         [dest3_u] "=&f"(ftmp[10]), [dest3_v] "=&f"(ftmp[11])
-      : [src_rgb0] "r"(src_rgb0), [src_stride_rgb] "r"(src_stride_rgb),
+      : [src_rgb] "r"(src_rgb), [src_stride_rgb] "r"(src_stride_rgb),
         [dst_u] "r"(dst_u), [dst_v] "r"(dst_v), [width] "r"(width),
         [mask_u] "f"(mask_u), [mask_v] "f"(mask_v), [value] "f"(value),
         [zero] "f"(0x00), [eight] "f"(0x08),
@@ -4386,7 +4386,7 @@ void ARGBShadeRow_MMI(const uint8_t* src_argb,
       : "memory");
 }
 
-void ARGBMultiplyRow_MMI(const uint8_t* src_argb0,
+void ARGBMultiplyRow_MMI(const uint8_t* src_argb,
                          const uint8_t* src_argb1,
                          uint8_t* dst_argb,
                          int width) {
@@ -4422,12 +4422,12 @@ void ARGBMultiplyRow_MMI(const uint8_t* src_argb0,
         [src1_hi] "=&f"(src1_hi), [src1_lo] "=&f"(src1_lo),
         [dest_hi] "=&f"(dest_hi), [dest_lo] "=&f"(dest_lo), [src0] "=&f"(src0),
         [src1] "=&f"(src1), [dest] "=&f"(dest)
-      : [src0_ptr] "r"(src_argb0), [src1_ptr] "r"(src_argb1),
+      : [src0_ptr] "r"(src_argb), [src1_ptr] "r"(src_argb1),
         [dst_ptr] "r"(dst_argb), [width] "r"(width), [mask] "f"(mask)
       : "memory");
 }
 
-void ARGBAddRow_MMI(const uint8_t* src_argb0,
+void ARGBAddRow_MMI(const uint8_t* src_argb,
                     const uint8_t* src_argb1,
                     uint8_t* dst_argb,
                     int width) {
@@ -4449,12 +4449,12 @@ void ARGBAddRow_MMI(const uint8_t* src_argb0,
       "daddi      %[width],        %[width],         -0x02          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src0] "=&f"(src0), [src1] "=&f"(src1), [dest] "=&f"(dest)
-      : [src0_ptr] "r"(src_argb0), [src1_ptr] "r"(src_argb1),
+      : [src0_ptr] "r"(src_argb), [src1_ptr] "r"(src_argb1),
         [dst_ptr] "r"(dst_argb), [width] "r"(width)
       : "memory");
 }
 
-void ARGBSubtractRow_MMI(const uint8_t* src_argb0,
+void ARGBSubtractRow_MMI(const uint8_t* src_argb,
                          const uint8_t* src_argb1,
                          uint8_t* dst_argb,
                          int width) {
@@ -4476,7 +4476,7 @@ void ARGBSubtractRow_MMI(const uint8_t* src_argb0,
       "daddi      %[width],        %[width],         -0x02          \n\t"
       "bnez       %[width],        1b                               \n\t"
       : [src0] "=&f"(src0), [src1] "=&f"(src1), [dest] "=&f"(dest)
-      : [src0_ptr] "r"(src_argb0), [src1_ptr] "r"(src_argb1),
+      : [src0_ptr] "r"(src_argb), [src1_ptr] "r"(src_argb1),
         [dst_ptr] "r"(dst_argb), [width] "r"(width)
       : "memory");
 }
@@ -5552,10 +5552,10 @@ void UYVYToYRow_MMI(const uint8_t* src_uyvy, uint8_t* dst_y, int width) {
       : "memory");
 }
 
-// Blend src_argb0 over src_argb1 and store to dst_argb.
-// dst_argb may be src_argb0 or src_argb1.
+// Blend src_argb over src_argb1 and store to dst_argb.
+// dst_argb may be src_argb or src_argb1.
 // This code mimics the SSSE3 version for better testability.
-void ARGBBlendRow_MMI(const uint8_t* src_argb0,
+void ARGBBlendRow_MMI(const uint8_t* src_argb,
                       const uint8_t* src_argb1,
                       uint8_t* dst_argb,
                       int width) {
@@ -5608,7 +5608,7 @@ void ARGBBlendRow_MMI(const uint8_t* src_argb0,
         [dest] "=&f"(dest), [src0_hi] "=&f"(src0_hi), [src0_lo] "=&f"(src0_lo),
         [src1_hi] "=&f"(src1_hi), [src1_lo] "=&f"(src1_lo),
         [dest_hi] "=&f"(dest_hi), [dest_lo] "=&f"(dest_lo)
-      : [src0_ptr] "r"(src_argb0), [src1_ptr] "r"(src_argb1),
+      : [src0_ptr] "r"(src_argb), [src1_ptr] "r"(src_argb1),
         [dst_ptr] "r"(dst_argb), [mask0] "f"(mask0), [mask1] "f"(mask1),
         [mask2] "f"(mask2), [mask3] "f"(mask3), [mask4] "f"(mask4),
         [shift] "f"(shift), [width] "r"(width)
diff --git a/source/row_msa.cc b/source/row_msa.cc
index fe6df93a..c0b13b0f 100644
--- a/source/row_msa.cc
+++ b/source/row_msa.cc
@@ -781,7 +781,7 @@ void UYVYToUV422Row_MSA(const uint8_t* src_uyvy,
   }
 }
 
-void ARGBToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void ARGBToYRow_MSA(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   int x;
   v16u8 src0, src1, src2, src3, vec0, vec1, vec2, vec3, dst0;
   v8u16 reg0, reg1, reg2, reg3, reg4, reg5;
@@ -792,10 +792,10 @@ void ARGBToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
   v8u16 const_0x1080 = (v8u16)__msa_fill_h(0x1080);
 
   for (x = 0; x < width; x += 16) {
-    src0 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 32);
-    src3 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 48);
+    src0 = (v16u8)__msa_ld_b((v16u8*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((v16u8*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((v16u8*)src_argb, 32);
+    src3 = (v16u8)__msa_ld_b((v16u8*)src_argb, 48);
     vec0 = (v16u8)__msa_pckev_b((v16i8)src1, (v16i8)src0);
     vec1 = (v16u8)__msa_pckev_b((v16i8)src3, (v16i8)src2);
     vec2 = (v16u8)__msa_pckod_b((v16i8)src1, (v16i8)src0);
@@ -822,18 +822,18 @@ void ARGBToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
     reg1 = (v8u16)__msa_srai_h((v8i16)reg1, 8);
     dst0 = (v16u8)__msa_pckev_b((v16i8)reg1, (v16i8)reg0);
     ST_UB(dst0, dst_y);
-    src_argb0 += 64;
+    src_argb += 64;
     dst_y += 16;
   }
 }
 
-void ARGBToUVRow_MSA(const uint8_t* src_argb0,
+void ARGBToUVRow_MSA(const uint8_t* src_argb,
                      int src_stride_argb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
                      int width) {
   int x;
-  const uint8_t* src_argb0_next = src_argb0 + src_stride_argb;
+  const uint8_t* src_argb_next = src_argb + src_stride_argb;
   v16u8 src0, src1, src2, src3, src4, src5, src6, src7;
   v16u8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
   v8u16 reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7, reg8, reg9;
@@ -847,14 +847,14 @@ void ARGBToUVRow_MSA(const uint8_t* src_argb0,
   v8u16 const_0x0001 = (v8u16)__msa_fill_h(0x0001);
 
   for (x = 0; x < width; x += 32) {
-    src0 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 32);
-    src3 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 48);
-    src4 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 64);
-    src5 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 80);
-    src6 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 96);
-    src7 = (v16u8)__msa_ld_b((v16u8*)src_argb0, 112);
+    src0 = (v16u8)__msa_ld_b((v16u8*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((v16u8*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((v16u8*)src_argb, 32);
+    src3 = (v16u8)__msa_ld_b((v16u8*)src_argb, 48);
+    src4 = (v16u8)__msa_ld_b((v16u8*)src_argb, 64);
+    src5 = (v16u8)__msa_ld_b((v16u8*)src_argb, 80);
+    src6 = (v16u8)__msa_ld_b((v16u8*)src_argb, 96);
+    src7 = (v16u8)__msa_ld_b((v16u8*)src_argb, 112);
     vec0 = (v16u8)__msa_pckev_b((v16i8)src1, (v16i8)src0);
     vec1 = (v16u8)__msa_pckev_b((v16i8)src3, (v16i8)src2);
     vec2 = (v16u8)__msa_pckev_b((v16i8)src5, (v16i8)src4);
@@ -875,14 +875,14 @@ void ARGBToUVRow_MSA(const uint8_t* src_argb0,
     reg3 = __msa_hadd_u_h(vec5, vec5);
     reg4 = __msa_hadd_u_h(vec0, vec0);
     reg5 = __msa_hadd_u_h(vec1, vec1);
-    src0 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 0);
-    src1 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 16);
-    src2 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 32);
-    src3 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 48);
-    src4 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 64);
-    src5 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 80);
-    src6 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 96);
-    src7 = (v16u8)__msa_ld_b((v16u8*)src_argb0_next, 112);
+    src0 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 0);
+    src1 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 16);
+    src2 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 32);
+    src3 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 48);
+    src4 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 64);
+    src5 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 80);
+    src6 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 96);
+    src7 = (v16u8)__msa_ld_b((v16u8*)src_argb_next, 112);
     vec0 = (v16u8)__msa_pckev_b((v16i8)src1, (v16i8)src0);
     vec1 = (v16u8)__msa_pckev_b((v16i8)src3, (v16i8)src2);
     vec2 = (v16u8)__msa_pckev_b((v16i8)src5, (v16i8)src4);
@@ -945,8 +945,8 @@ void ARGBToUVRow_MSA(const uint8_t* src_argb0,
     dst1 = (v16u8)__msa_pckev_b((v16i8)reg5, (v16i8)reg4);
     ST_UB(dst0, dst_u);
     ST_UB(dst1, dst_v);
-    src_argb0 += 128;
-    src_argb0_next += 128;
+    src_argb += 128;
+    src_argb_next += 128;
     dst_u += 16;
     dst_v += 16;
   }
@@ -1173,7 +1173,7 @@ void ARGBToUV444Row_MSA(const uint8_t* src_argb,
   }
 }
 
-void ARGBMultiplyRow_MSA(const uint8_t* src_argb0,
+void ARGBMultiplyRow_MSA(const uint8_t* src_argb,
                          const uint8_t* src_argb1,
                          uint8_t* dst_argb,
                          int width) {
@@ -1184,7 +1184,7 @@ void ARGBMultiplyRow_MSA(const uint8_t* src_argb0,
   v8i16 zero = {0};
 
   for (x = 0; x < width; x += 4) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
     src1 = (v16u8)__msa_ld_b((void*)src_argb1, 0);
     vec0 = (v8u16)__msa_ilvr_b((v16i8)src0, (v16i8)src0);
     vec1 = (v8u16)__msa_ilvl_b((v16i8)src0, (v16i8)src0);
@@ -1206,13 +1206,13 @@ void ARGBMultiplyRow_MSA(const uint8_t* src_argb0,
     vec1 = (v8u16)__msa_pckev_h((v8i16)reg3, (v8i16)reg2);
     dst0 = (v16u8)__msa_pckev_b((v16i8)vec1, (v16i8)vec0);
     ST_UB(dst0, dst_argb);
-    src_argb0 += 16;
+    src_argb += 16;
     src_argb1 += 16;
     dst_argb += 16;
   }
 }
 
-void ARGBAddRow_MSA(const uint8_t* src_argb0,
+void ARGBAddRow_MSA(const uint8_t* src_argb,
                     const uint8_t* src_argb1,
                     uint8_t* dst_argb,
                     int width) {
@@ -1220,20 +1220,20 @@ void ARGBAddRow_MSA(const uint8_t* src_argb0,
   v16u8 src0, src1, src2, src3, dst0, dst1;
 
   for (x = 0; x < width; x += 8) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
     src2 = (v16u8)__msa_ld_b((void*)src_argb1, 0);
     src3 = (v16u8)__msa_ld_b((void*)src_argb1, 16);
     dst0 = __msa_adds_u_b(src0, src2);
     dst1 = __msa_adds_u_b(src1, src3);
     ST_UB2(dst0, dst1, dst_argb, 16);
-    src_argb0 += 32;
+    src_argb += 32;
     src_argb1 += 32;
     dst_argb += 32;
   }
 }
 
-void ARGBSubtractRow_MSA(const uint8_t* src_argb0,
+void ARGBSubtractRow_MSA(const uint8_t* src_argb,
                          const uint8_t* src_argb1,
                          uint8_t* dst_argb,
                          int width) {
@@ -1241,14 +1241,14 @@ void ARGBSubtractRow_MSA(const uint8_t* src_argb0,
   v16u8 src0, src1, src2, src3, dst0, dst1;
 
   for (x = 0; x < width; x += 8) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
     src2 = (v16u8)__msa_ld_b((void*)src_argb1, 0);
     src3 = (v16u8)__msa_ld_b((void*)src_argb1, 16);
     dst0 = __msa_subs_u_b(src0, src2);
     dst1 = __msa_subs_u_b(src1, src3);
     ST_UB2(dst0, dst1, dst_argb, 16);
-    src_argb0 += 32;
+    src_argb += 32;
     src_argb1 += 32;
     dst_argb += 32;
   }
@@ -1794,7 +1794,7 @@ void RGB565ToYRow_MSA(const uint8_t* src_rgb565, uint8_t* dst_y, int width) {
   }
 }
 
-void RGB24ToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void RGB24ToYRow_MSA(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   int x;
   v16u8 src0, src1, src2, reg0, reg1, reg2, reg3, dst0;
   v8u16 vec0, vec1, vec2, vec3;
@@ -1809,9 +1809,9 @@ void RGB24ToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
   v16i8 zero = {0};
 
   for (x = 0; x < width; x += 16) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((void*)src_argb0, 32);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((void*)src_argb, 32);
     reg0 = (v16u8)__msa_vshf_b(mask0, zero, (v16i8)src0);
     reg1 = (v16u8)__msa_vshf_b(mask1, (v16i8)src1, (v16i8)src0);
     reg2 = (v16u8)__msa_vshf_b(mask2, (v16i8)src2, (v16i8)src1);
@@ -1830,12 +1830,12 @@ void RGB24ToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
     vec1 = (v8u16)__msa_srai_h((v8i16)vec1, 8);
     dst0 = (v16u8)__msa_pckev_b((v16i8)vec1, (v16i8)vec0);
     ST_UB(dst0, dst_y);
-    src_argb0 += 48;
+    src_argb += 48;
     dst_y += 16;
   }
 }
 
-void RAWToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void RAWToYRow_MSA(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   int x;
   v16u8 src0, src1, src2, reg0, reg1, reg2, reg3, dst0;
   v8u16 vec0, vec1, vec2, vec3;
@@ -1850,9 +1850,9 @@ void RAWToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
   v16i8 zero = {0};
 
   for (x = 0; x < width; x += 16) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((void*)src_argb0, 32);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((void*)src_argb, 32);
     reg0 = (v16u8)__msa_vshf_b(mask0, zero, (v16i8)src0);
     reg1 = (v16u8)__msa_vshf_b(mask1, (v16i8)src1, (v16i8)src0);
     reg2 = (v16u8)__msa_vshf_b(mask2, (v16i8)src2, (v16i8)src1);
@@ -1871,7 +1871,7 @@ void RAWToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
     vec1 = (v8u16)__msa_srai_h((v8i16)vec1, 8);
     dst0 = (v16u8)__msa_pckev_b((v16i8)vec1, (v16i8)vec0);
     ST_UB(dst0, dst_y);
-    src_argb0 += 48;
+    src_argb += 48;
     dst_y += 16;
   }
 }
@@ -2037,14 +2037,14 @@ void RGB565ToUVRow_MSA(const uint8_t* src_rgb565,
   }
 }
 
-void RGB24ToUVRow_MSA(const uint8_t* src_rgb0,
+void RGB24ToUVRow_MSA(const uint8_t* src_rgb,
                       int src_stride_rgb,
                       uint8_t* dst_u,
                       uint8_t* dst_v,
                       int width) {
   int x;
-  const uint8_t* s = src_rgb0;
-  const uint8_t* t = src_rgb0 + src_stride_rgb;
+  const uint8_t* s = src_rgb;
+  const uint8_t* t = src_rgb + src_stride_rgb;
   int64_t res0, res1;
   v16u8 src0, src1, src2, src3, src4, src5, src6, src7;
   v16u8 inp0, inp1, inp2, inp3, inp4, inp5;
@@ -2147,14 +2147,14 @@ void RGB24ToUVRow_MSA(const uint8_t* src_rgb0,
   }
 }
 
-void RAWToUVRow_MSA(const uint8_t* src_rgb0,
+void RAWToUVRow_MSA(const uint8_t* src_rgb,
                     int src_stride_rgb,
                     uint8_t* dst_u,
                     uint8_t* dst_v,
                     int width) {
   int x;
-  const uint8_t* s = src_rgb0;
-  const uint8_t* t = src_rgb0 + src_stride_rgb;
+  const uint8_t* s = src_rgb;
+  const uint8_t* t = src_rgb + src_stride_rgb;
   int64_t res0, res1;
   v16u8 inp0, inp1, inp2, inp3, inp4, inp5;
   v16u8 src0, src1, src2, src3, src4, src5, src6, src7;
@@ -2446,7 +2446,7 @@ void SobelXYRow_MSA(const uint8_t* src_sobelx,
   }
 }
 
-void ARGBToYJRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void ARGBToYJRow_MSA(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   int x;
   v16u8 src0, src1, src2, src3, dst0;
   v16u8 const_0x961D = (v16u8)__msa_fill_h(0x961D);
@@ -2454,19 +2454,19 @@ void ARGBToYJRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
   v8u16 const_0x80 = (v8u16)__msa_fill_h(0x80);
 
   for (x = 0; x < width; x += 16) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((void*)src_argb0, 32);
-    src3 = (v16u8)__msa_ld_b((void*)src_argb0, 48);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((void*)src_argb, 32);
+    src3 = (v16u8)__msa_ld_b((void*)src_argb, 48);
     ARGBTOY(src0, src1, src2, src3, const_0x961D, const_0x4D, const_0x80, 8,
             dst0);
     ST_UB(dst0, dst_y);
-    src_argb0 += 64;
+    src_argb += 64;
     dst_y += 16;
   }
 }
 
-void BGRAToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void BGRAToYRow_MSA(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   int x;
   v16u8 src0, src1, src2, src3, dst0;
   v16u8 const_0x4200 = (v16u8)__msa_fill_h(0x4200);
@@ -2474,19 +2474,19 @@ void BGRAToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
   v8u16 const_0x1080 = (v8u16)__msa_fill_h(0x1080);
 
   for (x = 0; x < width; x += 16) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((void*)src_argb0, 32);
-    src3 = (v16u8)__msa_ld_b((void*)src_argb0, 48);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((void*)src_argb, 32);
+    src3 = (v16u8)__msa_ld_b((void*)src_argb, 48);
     ARGBTOY(src0, src1, src2, src3, const_0x4200, const_0x1981, const_0x1080, 8,
             dst0);
     ST_UB(dst0, dst_y);
-    src_argb0 += 64;
+    src_argb += 64;
     dst_y += 16;
   }
 }
 
-void ABGRToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void ABGRToYRow_MSA(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   int x;
   v16u8 src0, src1, src2, src3, dst0;
   v16u8 const_0x8142 = (v16u8)__msa_fill_h(0x8142);
@@ -2494,19 +2494,19 @@ void ABGRToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
   v8u16 const_0x1080 = (v8u16)__msa_fill_h(0x1080);
 
   for (x = 0; x < width; x += 16) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((void*)src_argb0, 32);
-    src3 = (v16u8)__msa_ld_b((void*)src_argb0, 48);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((void*)src_argb, 32);
+    src3 = (v16u8)__msa_ld_b((void*)src_argb, 48);
     ARGBTOY(src0, src1, src2, src3, const_0x8142, const_0x19, const_0x1080, 8,
             dst0);
     ST_UB(dst0, dst_y);
-    src_argb0 += 64;
+    src_argb += 64;
     dst_y += 16;
   }
 }
 
-void RGBAToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
+void RGBAToYRow_MSA(const uint8_t* src_argb, uint8_t* dst_y, int width) {
   int x;
   v16u8 src0, src1, src2, src3, dst0;
   v16u8 const_0x1900 = (v16u8)__msa_fill_h(0x1900);
@@ -2514,26 +2514,26 @@ void RGBAToYRow_MSA(const uint8_t* src_argb0, uint8_t* dst_y, int width) {
   v8u16 const_0x1080 = (v8u16)__msa_fill_h(0x1080);
 
   for (x = 0; x < width; x += 16) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
-    src2 = (v16u8)__msa_ld_b((void*)src_argb0, 32);
-    src3 = (v16u8)__msa_ld_b((void*)src_argb0, 48);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
+    src2 = (v16u8)__msa_ld_b((void*)src_argb, 32);
+    src3 = (v16u8)__msa_ld_b((void*)src_argb, 48);
     ARGBTOY(src0, src1, src2, src3, const_0x1900, const_0x4281, const_0x1080, 8,
             dst0);
     ST_UB(dst0, dst_y);
-    src_argb0 += 64;
+    src_argb += 64;
     dst_y += 16;
   }
 }
 
-void ARGBToUVJRow_MSA(const uint8_t* src_rgb0,
+void ARGBToUVJRow_MSA(const uint8_t* src_rgb,
                       int src_stride_rgb,
                       uint8_t* dst_u,
                       uint8_t* dst_v,
                       int width) {
   int x;
-  const uint8_t* s = src_rgb0;
-  const uint8_t* t = src_rgb0 + src_stride_rgb;
+  const uint8_t* s = src_rgb;
+  const uint8_t* t = src_rgb + src_stride_rgb;
   v8u16 src0, src1, src2, src3, src4, src5, src6, src7;
   v8u16 vec0, vec1, vec2, vec3;
   v8u16 dst0, dst1, dst2, dst3;
@@ -2658,14 +2658,14 @@ void ARGBToUVJRow_MSA(const uint8_t* src_rgb0,
   }
 }
 
-void BGRAToUVRow_MSA(const uint8_t* src_rgb0,
+void BGRAToUVRow_MSA(const uint8_t* src_rgb,
                      int src_stride_rgb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
                      int width) {
   int x;
-  const uint8_t* s = src_rgb0;
-  const uint8_t* t = src_rgb0 + src_stride_rgb;
+  const uint8_t* s = src_rgb;
+  const uint8_t* t = src_rgb + src_stride_rgb;
   const uint8_t unused = 0xf;
   v8u16 src0, src1, src2, src3;
   v16u8 dst0, dst1;
@@ -2693,14 +2693,14 @@ void BGRAToUVRow_MSA(const uint8_t* src_rgb0,
   }
 }
 
-void ABGRToUVRow_MSA(const uint8_t* src_rgb0,
+void ABGRToUVRow_MSA(const uint8_t* src_rgb,
                      int src_stride_rgb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
                      int width) {
   int x;
-  const uint8_t* s = src_rgb0;
-  const uint8_t* t = src_rgb0 + src_stride_rgb;
+  const uint8_t* s = src_rgb;
+  const uint8_t* t = src_rgb + src_stride_rgb;
   const uint8_t unused = 0xf;
   v8u16 src0, src1, src2, src3;
   v16u8 dst0, dst1;
@@ -2728,14 +2728,14 @@ void ABGRToUVRow_MSA(const uint8_t* src_rgb0,
   }
 }
 
-void RGBAToUVRow_MSA(const uint8_t* src_rgb0,
+void RGBAToUVRow_MSA(const uint8_t* src_rgb,
                      int src_stride_rgb,
                      uint8_t* dst_u,
                      uint8_t* dst_v,
                      int width) {
   int x;
-  const uint8_t* s = src_rgb0;
-  const uint8_t* t = src_rgb0 + src_stride_rgb;
+  const uint8_t* s = src_rgb;
+  const uint8_t* t = src_rgb + src_stride_rgb;
   const uint8_t unused = 0xf;
   v8u16 src0, src1, src2, src3;
   v16u8 dst0, dst1;
@@ -3109,7 +3109,7 @@ void ARGBExtractAlphaRow_MSA(const uint8_t* src_argb,
   }
 }
 
-void ARGBBlendRow_MSA(const uint8_t* src_argb0,
+void ARGBBlendRow_MSA(const uint8_t* src_argb,
                       const uint8_t* src_argb1,
                       uint8_t* dst_argb,
                       int width) {
@@ -3123,8 +3123,8 @@ void ARGBBlendRow_MSA(const uint8_t* src_argb0,
   v16i8 zero = {0};
 
   for (x = 0; x < width; x += 8) {
-    src0 = (v16u8)__msa_ld_b((void*)src_argb0, 0);
-    src1 = (v16u8)__msa_ld_b((void*)src_argb0, 16);
+    src0 = (v16u8)__msa_ld_b((void*)src_argb, 0);
+    src1 = (v16u8)__msa_ld_b((void*)src_argb, 16);
     src2 = (v16u8)__msa_ld_b((void*)src_argb1, 0);
     src3 = (v16u8)__msa_ld_b((void*)src_argb1, 16);
     vec0 = (v8u16)__msa_ilvr_b(zero, (v16i8)src0);
@@ -3168,7 +3168,7 @@ void ARGBBlendRow_MSA(const uint8_t* src_argb0,
     dst0 = __msa_bmnz_v(dst0, const_255, mask);
     dst1 = __msa_bmnz_v(dst1, const_255, mask);
     ST_UB2(dst0, dst1, dst_argb, 16);
-    src_argb0 += 32;
+    src_argb += 32;
     src_argb1 += 32;
     dst_argb += 32;
   }
diff --git a/source/row_neon.cc b/source/row_neon.cc
index 43a2cac7..e8dc1257 100644
--- a/source/row_neon.cc
+++ b/source/row_neon.cc
@@ -21,90 +21,115 @@ extern "C" {
 #if !defined(LIBYUV_DISABLE_NEON) && defined(__ARM_NEON__) && \
     !defined(__aarch64__)
 
+// q0: Y uint16x8_t
+// d2: U uint8x8_t
+// d3: V uint8x8_t
+
 // Read 8 Y, 4 U and 4 V from 422
 #define READYUV422                               \
-  "vld1.8     {d0}, [%0]!                    \n" \
-  "vld1.32    {d2[0]}, [%1]!                 \n" \
-  "vld1.32    {d2[1]}, [%2]!                 \n"
+  "vld1.8     {d0}, [%[src_y]]!              \n" \
+  "vld1.32    {d2[0]}, [%[src_u]]!           \n" \
+  "vld1.32    {d2[1]}, [%[src_v]]!           \n" \
+  "vmov.u8    d1, d0                         \n" \
+  "vmovl.u8   q1, d2                         \n" \
+  "vzip.u8    d0, d1                         \n" \
+  "vsli.u16   q1, q1, #8                     \n"
 
 // Read 8 Y, 8 U and 8 V from 444
 #define READYUV444                               \
-  "vld1.8     {d0}, [%0]!                    \n" \
-  "vld1.8     {d2}, [%1]!                    \n" \
-  "vld1.8     {d3}, [%2]!                    \n" \
-  "vpaddl.u8  q1, q1                         \n" \
-  "vrshrn.u16 d2, q1, #1                     \n"
+  "vld1.8     {d0}, [%[src_y]]!              \n" \
+  "vld1.8     {d2}, [%[src_u]]!              \n" \
+  "vmovl.u8   q0, d0                         \n" \
+  "vld1.8     {d3}, [%[src_v]]!              \n" \
+  "vsli.u16   q0, q0, #8                     \n"
 
 // Read 8 Y, and set 4 U and 4 V to 128
 #define READYUV400                               \
-  "vld1.8     {d0}, [%0]!                    \n" \
-  "vmov.u8    d2, #128                       \n"
+  "vld1.8     {d0}, [%[src_y]]!              \n" \
+  "vmov.u8    q1, #128                       \n" \
+  "vmovl.u8   q0, d0                         \n" \
+  "vsli.u16   q0, q0, #8                     \n"
 
 // Read 8 Y and 4 UV from NV12
-#define READNV12                                                               \
-  "vld1.8     {d0}, [%0]!                    \n"                               \
-  "vld1.8     {d2}, [%1]!                    \n"                               \
-  "vmov.u8    d3, d2                         \n" /* split odd/even uv apart */ \
-  "vuzp.u8    d2, d3                         \n"                               \
-  "vtrn.u32   d2, d3                         \n"
+#define READNV12                                                              \
+  "vld1.8     {d0}, [%[src_y]]!              \n"                              \
+  "vld1.8     {d2}, [%[src_uv]]!             \n"                              \
+  "vmov.u8    d1, d0                         \n"                              \
+  "vmov.u8    d3, d2                         \n"                              \
+  "vzip.u8    d0, d1                         \n"                              \
+  "vsli.u16   d2, d2, #8                     \n" /* Duplicate low byte (U) */ \
+  "vsri.u16   d3, d3, #8                     \n" /* Duplicate high byte (V) */
 
 // Read 8 Y and 4 VU from NV21
 #define READNV21                                                               \
-  "vld1.8     {d0}, [%0]!                    \n"                               \
-  "vld1.8     {d2}, [%1]!                    \n"                               \
-  "vmov.u8    d3, d2                         \n" /* split odd/even uv apart */ \
-  "vuzp.u8    d3, d2                         \n"                               \
-  "vtrn.u32   d2, d3                         \n"
+  "vld1.8     {d0}, [%[src_y]]!              \n"                               \
+  "vld1.8     {d2}, [%[src_vu]]!             \n"                               \
+  "vmov.u8    d1, d0                         \n"                               \
+  "vmov.u8    d3, d2                         \n"                               \
+  "vzip.u8    d0, d1                         \n"                               \
+  "vsri.u16   d2, d2, #8                     \n" /* Duplicate high byte (U) */ \
+  "vsli.u16   d3, d3, #8                     \n" /* Duplicate low byte (V) */
 
 // Read 8 YUY2
 #define READYUY2                                 \
-  "vld2.8     {d0, d2}, [%0]!                \n" \
+  "vld2.8     {d0, d2}, [%[src_yuy2]]!       \n" \
+  "vmovl.u8   q0, d0                         \n" \
   "vmov.u8    d3, d2                         \n" \
-  "vuzp.u8    d2, d3                         \n" \
-  "vtrn.u32   d2, d3                         \n"
+  "vsli.u16   q0, q0, #8                     \n" \
+  "vsli.u16   d2, d2, #8                     \n" \
+  "vsri.u16   d3, d3, #8                     \n"
 
 // Read 8 UYVY
 #define READUYVY                                 \
-  "vld2.8     {d2, d3}, [%0]!                \n" \
-  "vmov.u8    d0, d3                         \n" \
+  "vld2.8     {d2, d3}, [%[src_uyvy]]!       \n" \
+  "vmovl.u8   q0, d3                         \n" \
   "vmov.u8    d3, d2                         \n" \
-  "vuzp.u8    d2, d3                         \n" \
-  "vtrn.u32   d2, d3                         \n"
-
-#define YUVTORGB_SETUP                             \
-  "vld1.8     {d24}, [%[kUVToRB]]            \n"   \
-  "vld1.8     {d25}, [%[kUVToG]]             \n"   \
-  "vld1.16    {d26[], d27[]}, [%[kUVBiasBGR]]! \n" \
-  "vld1.16    {d8[], d9[]}, [%[kUVBiasBGR]]!   \n" \
-  "vld1.16    {d28[], d29[]}, [%[kUVBiasBGR]]  \n" \
-  "vld1.32    {d30[], d31[]}, [%[kYToRgb]]     \n"
-
-#define YUVTORGB                                                              \
-  "vmull.u8   q8, d2, d24                    \n" /* u/v B/R component      */ \
-  "vmull.u8   q9, d2, d25                    \n" /* u/v G component        */ \
-  "vmovl.u8   q0, d0                         \n" /* Y                      */ \
-  "vmovl.s16  q10, d1                        \n"                              \
-  "vmovl.s16  q0, d0                         \n"                              \
-  "vmul.s32   q10, q10, q15                  \n"                              \
-  "vmul.s32   q0, q0, q15                    \n"                              \
-  "vqshrun.s32 d0, q0, #16                   \n"                              \
-  "vqshrun.s32 d1, q10, #16                  \n" /* Y                      */ \
-  "vadd.s16   d18, d19                       \n"                              \
-  "vshll.u16  q1, d16, #16                   \n" /* Replicate u * UB       */ \
-  "vshll.u16  q10, d17, #16                  \n" /* Replicate v * VR       */ \
-  "vshll.u16  q3, d18, #16                   \n" /* Replicate (v*VG + u*UG)*/ \
-  "vaddw.u16  q1, q1, d16                    \n"                              \
-  "vaddw.u16  q10, q10, d17                  \n"                              \
-  "vaddw.u16  q3, q3, d18                    \n"                              \
-  "vqadd.s16  q8, q0, q13                    \n" /* B */                      \
-  "vqadd.s16  q9, q0, q14                    \n" /* R */                      \
-  "vqadd.s16  q0, q0, q4                     \n" /* G */                      \
-  "vqadd.s16  q8, q8, q1                     \n" /* B */                      \
-  "vqadd.s16  q9, q9, q10                    \n" /* R */                      \
-  "vqsub.s16  q0, q0, q3                     \n" /* G */                      \
-  "vqshrun.s16 d20, q8, #6                   \n" /* B */                      \
-  "vqshrun.s16 d22, q9, #6                   \n" /* R */                      \
-  "vqshrun.s16 d21, q0, #6                   \n" /* G */
+  "vsli.u16   q0, q0, #8                     \n" \
+  "vsli.u16   d2, d2, #8                     \n" \
+  "vsri.u16   d3, d3, #8                     \n"
+
+#define YUVTORGB_SETUP                                        \
+  "vld4.8     {d26[], d27[], d28[], d29[]}, [%[kUVCoeff]] \n" \
+  "vld1.16    {d31[]}, [%[kRGBCoeffBias]]!   \n"              \
+  "vld1.16    {d20[], d21[]}, [%[kRGBCoeffBias]]! \n"         \
+  "vld1.16    {d22[], d23[]}, [%[kRGBCoeffBias]]! \n"         \
+  "vld1.16    {d24[], d25[]}, [%[kRGBCoeffBias]] \n"
+
+// q0: B uint16x8_t
+// q1: G uint16x8_t
+// q2: R uint16x8_t
+
+// Convert from YUV to 2.14 fixed point RGB
+#define YUVTORGB                                           \
+  "vmull.u16  q2, d1, d31                    \n"           \
+  "vmull.u8   q8, d3, d29                    \n" /* DGV */ \
+  "vmull.u16  q0, d0, d31                    \n"           \
+  "vmlal.u8   q8, d2, d28                    \n" /* DG */  \
+  "vqshrn.u32 d0, q0, #16                    \n"           \
+  "vqshrn.u32 d1, q2, #16                    \n" /* Y */   \
+  "vmull.u8   q9, d2, d26                    \n" /* DB */  \
+  "vmull.u8   q2, d3, d27                    \n" /* DR */  \
+  "vadd.u16   q4, q0, q11                    \n" /* G */   \
+  "vadd.u16   q2, q0, q2                     \n" /* R */   \
+  "vadd.u16   q0, q0, q9                     \n" /* B */   \
+  "vqsub.u16  q1, q4, q8                     \n" /* G */   \
+  "vqsub.u16  q0, q0, q10                    \n" /* B */   \
+  "vqsub.u16  q2, q2, q12                    \n" /* R */
+
+// Convert from 2.14 fixed point RGB To 8 bit RGB
+#define RGBTORGB8                                        \
+  "vqshrn.u16 d4, q2, #6                     \n" /* R */ \
+  "vqshrn.u16 d2, q1, #6                     \n" /* G */ \
+  "vqshrn.u16 d0, q0, #6                     \n" /* B */
+
+#define YUVTORGB_REGS \
+  "q0", "q1", "q2", "q4", "q8", "q9", "q10", "q11", "q12", "q13", "q14", "d31"
+
+#define STORERGBA                                \
+  "vmov.u8    d1, d0                         \n" \
+  "vmov.u8    d3, d4                         \n" \
+  "vmov.u8    d0, d6                         \n" \
+  "vst4.8     {d0, d1, d2, d3}, [%[dst_rgba]]! \n"
 
 void I444ToARGBRow_NEON(const uint8_t* src_y,
                         const uint8_t* src_u,
@@ -114,22 +139,20 @@ void I444ToARGBRow_NEON(const uint8_t* src_y,
                         int width) {
   asm volatile(
       YUVTORGB_SETUP
-      "vmov.u8     d23, #255                     \n"
+      "vmov.u8     d6, #255                      \n"
       "1:                                        \n" READYUV444 YUVTORGB
-      "subs        %4, %4, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%3]!   \n"
+          RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
       "bgt         1b                            \n"
-      : "+r"(src_y),     // %0
-        "+r"(src_u),     // %1
-        "+r"(src_v),     // %2
-        "+r"(dst_argb),  // %3
-        "+r"(width)      // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void I422ToARGBRow_NEON(const uint8_t* src_y,
@@ -140,22 +163,20 @@ void I422ToARGBRow_NEON(const uint8_t* src_y,
                         int width) {
   asm volatile(
       YUVTORGB_SETUP
-      "vmov.u8     d23, #255                     \n"
+      "vmov.u8     d6, #255                      \n"
       "1:                                        \n" READYUV422 YUVTORGB
-      "subs        %4, %4, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%3]!   \n"
+          RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
       "bgt         1b                            \n"
-      : "+r"(src_y),     // %0
-        "+r"(src_u),     // %1
-        "+r"(src_v),     // %2
-        "+r"(dst_argb),  // %3
-        "+r"(width)      // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void I444AlphaToARGBRow_NEON(const uint8_t* src_y,
@@ -168,22 +189,20 @@ void I444AlphaToARGBRow_NEON(const uint8_t* src_y,
   asm volatile(
       YUVTORGB_SETUP
       "1:                                        \n" READYUV444 YUVTORGB
-      "vld1.8      {d23}, [%3]!                  \n"
-      "subs        %5, %5, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%4]!   \n"
-      "bgt         1b                            \n"
-      : "+r"(src_y),     // %0
-        "+r"(src_u),     // %1
-        "+r"(src_v),     // %2
-        "+r"(src_a),     // %3
-        "+r"(dst_argb),  // %4
-        "+r"(width)      // %5
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+          RGBTORGB8
+      "vld1.8      {d6}, [%[src_a]]!             \n"
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
+      "bgt         1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [src_a] "+r"(src_a),                               // %[src_a]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void I422AlphaToARGBRow_NEON(const uint8_t* src_y,
@@ -196,22 +215,20 @@ void I422AlphaToARGBRow_NEON(const uint8_t* src_y,
   asm volatile(
       YUVTORGB_SETUP
       "1:                                        \n" READYUV422 YUVTORGB
-      "subs        %5, %5, #8                    \n"
-      "vld1.8      {d23}, [%3]!                  \n"
-      "vst4.8      {d20, d21, d22, d23}, [%4]!   \n"
-      "bgt         1b                            \n"
-      : "+r"(src_y),     // %0
-        "+r"(src_u),     // %1
-        "+r"(src_v),     // %2
-        "+r"(src_a),     // %3
-        "+r"(dst_argb),  // %4
-        "+r"(width)      // %5
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+          RGBTORGB8
+      "vld1.8      {d6}, [%[src_a]]!             \n"
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
+      "bgt         1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [src_a] "+r"(src_a),                               // %[src_a]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void I422ToRGBARow_NEON(const uint8_t* src_y,
@@ -222,22 +239,18 @@ void I422ToRGBARow_NEON(const uint8_t* src_y,
                         int width) {
   asm volatile(
       YUVTORGB_SETUP
+      "vmov.u8     d6, #255                      \n"
       "1:                                        \n" READYUV422 YUVTORGB
-      "subs        %4, %4, #8                    \n"
-      "vmov.u8     d19, #255                     \n"  // YUVTORGB modified d19
-      "vst4.8      {d19, d20, d21, d22}, [%3]!   \n"
+          RGBTORGB8 "subs        %[width], %[width], #8        \n" STORERGBA
       "bgt         1b                            \n"
-      : "+r"(src_y),     // %0
-        "+r"(src_u),     // %1
-        "+r"(src_v),     // %2
-        "+r"(dst_rgba),  // %3
-        "+r"(width)      // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_rgba] "+r"(dst_rgba),                         // %[dst_rgba]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void I422ToRGB24Row_NEON(const uint8_t* src_y,
@@ -248,29 +261,28 @@ void I422ToRGB24Row_NEON(const uint8_t* src_y,
                          int width) {
   asm volatile(
       YUVTORGB_SETUP
+      "vmov.u8     d6, #255                      \n"
       "1:                                        \n" READYUV422 YUVTORGB
-      "subs        %4, %4, #8                    \n"
-      "vst3.8      {d20, d21, d22}, [%3]!        \n"
+          RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst3.8      {d0, d2, d4}, [%[dst_rgb24]]! \n"
       "bgt         1b                            \n"
-      : "+r"(src_y),      // %0
-        "+r"(src_u),      // %1
-        "+r"(src_v),      // %2
-        "+r"(dst_rgb24),  // %3
-        "+r"(width)       // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_rgb24] "+r"(dst_rgb24),                       // %[dst_rgb24]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS);
 }
 
 #define ARGBTORGB565                                                        \
-  "vshll.u8    q0, d22, #8                   \n" /* R                    */ \
-  "vshll.u8    q8, d21, #8                   \n" /* G                    */ \
-  "vshll.u8    q9, d20, #8                   \n" /* B                    */ \
-  "vsri.16     q0, q8, #5                    \n" /* RG                   */ \
-  "vsri.16     q0, q9, #11                   \n" /* RGB                  */
+  "vshll.u8    q2, d4, #8                    \n" /* R                    */ \
+  "vshll.u8    q1, d2, #8                    \n" /* G                    */ \
+  "vshll.u8    q0, d0, #8                    \n" /* B                    */ \
+  "vsri.16     q2, q1, #5                    \n" /* RG                   */ \
+  "vsri.16     q2, q0, #11                   \n" /* RGB                  */
 
 void I422ToRGB565Row_NEON(const uint8_t* src_y,
                           const uint8_t* src_u,
@@ -280,31 +292,29 @@ void I422ToRGB565Row_NEON(const uint8_t* src_y,
                           int width) {
   asm volatile(
       YUVTORGB_SETUP
+      "vmov.u8     d6, #255                      \n"
       "1:                                        \n" READYUV422 YUVTORGB
-      "subs        %4, %4, #8                    \n" ARGBTORGB565
-      "vst1.8      {q0}, [%3]!                   \n"  // store 8 pixels RGB565.
-      "bgt         1b                            \n"
-      : "+r"(src_y),       // %0
-        "+r"(src_u),       // %1
-        "+r"(src_v),       // %2
-        "+r"(dst_rgb565),  // %3
-        "+r"(width)        // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+          RGBTORGB8 "subs        %[width], %[width], #8        \n" ARGBTORGB565
+      "vst1.8      {q2}, [%[dst_rgb565]]!        \n"  // store 8 pixels RGB565.
+      "bgt         1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_rgb565] "+r"(dst_rgb565),                     // %[dst_rgb565]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS);
 }
 
 #define ARGBTOARGB1555                                                      \
-  "vshll.u8    q0, d23, #8                   \n" /* A                    */ \
-  "vshll.u8    q8, d22, #8                   \n" /* R                    */ \
-  "vshll.u8    q9, d21, #8                   \n" /* G                    */ \
-  "vshll.u8    q10, d20, #8                  \n" /* B                    */ \
-  "vsri.16     q0, q8, #1                    \n" /* AR                   */ \
-  "vsri.16     q0, q9, #6                    \n" /* ARG                  */ \
-  "vsri.16     q0, q10, #11                  \n" /* ARGB                 */
+  "vshll.u8    q3, d6, #8                    \n" /* A                    */ \
+  "vshll.u8    q2, d4, #8                    \n" /* R                    */ \
+  "vshll.u8    q1, d2, #8                    \n" /* G                    */ \
+  "vshll.u8    q0, d0, #8                    \n" /* B                    */ \
+  "vsri.16     q3, q2, #1                    \n" /* AR                   */ \
+  "vsri.16     q3, q1, #6                    \n" /* ARG                  */ \
+  "vsri.16     q3, q0, #11                   \n" /* ARGB                 */
 
 void I422ToARGB1555Row_NEON(const uint8_t* src_y,
                             const uint8_t* src_u,
@@ -315,30 +325,28 @@ void I422ToARGB1555Row_NEON(const uint8_t* src_y,
   asm volatile(
       YUVTORGB_SETUP
       "1:                                        \n" READYUV422 YUVTORGB
-      "subs        %4, %4, #8                    \n"
-      "vmov.u8     d23, #255                     \n" ARGBTOARGB1555
-      "vst1.8      {q0}, [%3]!                   \n"  // store 8 pixels
-      "bgt         1b                            \n"
-      : "+r"(src_y),         // %0
-        "+r"(src_u),         // %1
-        "+r"(src_v),         // %2
-        "+r"(dst_argb1555),  // %3
-        "+r"(width)          // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+          RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vmov.u8     d6, #0xff                     \n" ARGBTOARGB1555
+      "vst1.8      {q3}, [%[dst_argb1555]]!      \n"  // store 8 pixels RGB1555.
+      "bgt         1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb1555] "+r"(dst_argb1555),                 // %[dst_argb1555]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "q3");
 }
 
 #define ARGBTOARGB4444                                                      \
-  "vshr.u8    d20, d20, #4                   \n" /* B                    */ \
-  "vbic.32    d21, d21, d4                   \n" /* G                    */ \
-  "vshr.u8    d22, d22, #4                   \n" /* R                    */ \
-  "vbic.32    d23, d23, d4                   \n" /* A                    */ \
-  "vorr       d0, d20, d21                   \n" /* BG                   */ \
-  "vorr       d1, d22, d23                   \n" /* RA                   */ \
+  "vshr.u8    d0, d0, #4                     \n" /* B                    */ \
+  "vbic.32    d2, d2, d7                     \n" /* G                    */ \
+  "vshr.u8    d4, d4, #4                     \n" /* R                    */ \
+  "vbic.32    d6, d6, d7                     \n" /* A                    */ \
+  "vorr       d0, d0, d2                     \n" /* BG                   */ \
+  "vorr       d1, d4, d6                     \n" /* RA                   */ \
   "vzip.u8    d0, d1                         \n" /* BGRA                 */
 
 void I422ToARGB4444Row_NEON(const uint8_t* src_y,
@@ -349,25 +357,21 @@ void I422ToARGB4444Row_NEON(const uint8_t* src_y,
                             int width) {
   asm volatile(
       YUVTORGB_SETUP
-      "vmov.u8     d4, #0x0f                     \n"  // vbic bits to clear
-      "1:                                        \n"
-
-      READYUV422 YUVTORGB
-      "subs        %4, %4, #8                    \n"
-      "vmov.u8     d23, #255                     \n" ARGBTOARGB4444
-      "vst1.8      {q0}, [%3]!                   \n"  // store 8 pixels
-      "bgt         1b                            \n"
-      : "+r"(src_y),         // %0
-        "+r"(src_u),         // %1
-        "+r"(src_v),         // %2
-        "+r"(dst_argb4444),  // %3
-        "+r"(width)          // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      "vmov.u8     d6, #255                      \n"
+      "vmov.u8     d7, #0x0f                     \n"  // vbic bits to clear
+      "1:                                        \n" READYUV422 YUVTORGB
+          RGBTORGB8
+      "subs        %[width], %[width], #8        \n" ARGBTOARGB4444
+      "vst1.8      {q0}, [%[dst_argb4444]]!      \n"  // store 8 pixels
+      "bgt         1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb4444] "+r"(dst_argb4444),                 // %[dst_argb4444]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "q3");
 }
 
 void I400ToARGBRow_NEON(const uint8_t* src_y,
@@ -376,20 +380,18 @@ void I400ToARGBRow_NEON(const uint8_t* src_y,
                         int width) {
   asm volatile(
       YUVTORGB_SETUP
-      "vmov.u8     d23, #255                     \n"
+      "vmov.u8     d6, #255                      \n"
       "1:                                        \n" READYUV400 YUVTORGB
-      "subs        %2, %2, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%1]!   \n"
+          RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
       "bgt         1b                            \n"
-      : "+r"(src_y),     // %0
-        "+r"(dst_argb),  // %1
-        "+r"(width)      // %2
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void J400ToARGBRow_NEON(const uint8_t* src_y, uint8_t* dst_argb, int width) {
@@ -414,22 +416,20 @@ void NV12ToARGBRow_NEON(const uint8_t* src_y,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile(YUVTORGB_SETUP
-      "vmov.u8     d23, #255                     \n"
-      "1:                                        \n" READNV12 YUVTORGB
-      "subs        %3, %3, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%2]!   \n"
-      "bgt         1b                            \n"
-               : "+r"(src_y),     // %0
-                 "+r"(src_uv),    // %1
-                 "+r"(dst_argb),  // %2
-                 "+r"(width)      // %3
-               : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-                 [kUVToG] "r"(&yuvconstants->kUVToG),
-                 [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-                 [kYToRgb] "r"(&yuvconstants->kYToRgb)
-               : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9",
-                 "q10", "q11", "q12", "q13", "q14", "q15");
+  asm volatile(
+      YUVTORGB_SETUP
+      "vmov.u8     d6, #255                      \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
+      "bgt         1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_uv] "+r"(src_uv),                             // %[src_uv]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void NV21ToARGBRow_NEON(const uint8_t* src_y,
@@ -437,22 +437,20 @@ void NV21ToARGBRow_NEON(const uint8_t* src_y,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile(YUVTORGB_SETUP
-      "vmov.u8     d23, #255                     \n"
-      "1:                                        \n" READNV21 YUVTORGB
-      "subs        %3, %3, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%2]!   \n"
-      "bgt         1b                            \n"
-               : "+r"(src_y),     // %0
-                 "+r"(src_vu),    // %1
-                 "+r"(dst_argb),  // %2
-                 "+r"(width)      // %3
-               : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-                 [kUVToG] "r"(&yuvconstants->kUVToG),
-                 [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-                 [kYToRgb] "r"(&yuvconstants->kYToRgb)
-               : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9",
-                 "q10", "q11", "q12", "q13", "q14", "q15");
+  asm volatile(
+      YUVTORGB_SETUP
+      "vmov.u8     d6, #255                      \n"
+      "1:                                        \n" READNV21 YUVTORGB RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
+      "bgt         1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_vu] "+r"(src_vu),                             // %[src_vu]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void NV12ToRGB24Row_NEON(const uint8_t* src_y,
@@ -461,25 +459,19 @@ void NV12ToRGB24Row_NEON(const uint8_t* src_y,
                          const struct YuvConstants* yuvconstants,
                          int width) {
   asm volatile(
-
       YUVTORGB_SETUP
-
-      "1:                                        \n"
-
-      READNV12 YUVTORGB
-      "subs        %3, %3, #8                    \n"
-      "vst3.8      {d20, d21, d22}, [%2]!        \n"
+      "vmov.u8     d6, #255                      \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst3.8      {d0, d2, d4}, [%[dst_rgb24]]! \n"
       "bgt         1b                            \n"
-      : "+r"(src_y),      // %0
-        "+r"(src_uv),     // %1
-        "+r"(dst_rgb24),  // %2
-        "+r"(width)       // %3
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_uv] "+r"(src_uv),                             // %[src_uv]
+        [dst_rgb24] "+r"(dst_rgb24),                       // %[dst_rgb24]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS);
 }
 
 void NV21ToRGB24Row_NEON(const uint8_t* src_y,
@@ -488,25 +480,19 @@ void NV21ToRGB24Row_NEON(const uint8_t* src_y,
                          const struct YuvConstants* yuvconstants,
                          int width) {
   asm volatile(
-
       YUVTORGB_SETUP
-
-      "1:                                        \n"
-
-      READNV21 YUVTORGB
-      "subs        %3, %3, #8                    \n"
-      "vst3.8      {d20, d21, d22}, [%2]!        \n"
+      "vmov.u8     d6, #255                      \n"
+      "1:                                        \n" READNV21 YUVTORGB RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst3.8      {d0, d2, d4}, [%[dst_rgb24]]! \n"
       "bgt         1b                            \n"
-      : "+r"(src_y),      // %0
-        "+r"(src_vu),     // %1
-        "+r"(dst_rgb24),  // %2
-        "+r"(width)       // %3
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_vu] "+r"(src_vu),                             // %[src_vu]
+        [dst_rgb24] "+r"(dst_rgb24),                       // %[dst_rgb24]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS);
 }
 
 void NV12ToRGB565Row_NEON(const uint8_t* src_y,
@@ -516,62 +502,56 @@ void NV12ToRGB565Row_NEON(const uint8_t* src_y,
                           int width) {
   asm volatile(
       YUVTORGB_SETUP
-      "1:                                        \n" READNV12 YUVTORGB
-      "subs        %3, %3, #8                    \n" ARGBTORGB565
-      "vst1.8      {q0}, [%2]!                   \n"  // store 8 pixels RGB565.
+      "vmov.u8     d6, #255                      \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %[width], %[width], #8        \n" ARGBTORGB565
+      "vst1.8      {q2}, [%[dst_rgb565]]!        \n"  // store 8 pixels RGB565.
       "bgt         1b                            \n"
-      : "+r"(src_y),       // %0
-        "+r"(src_uv),      // %1
-        "+r"(dst_rgb565),  // %2
-        "+r"(width)        // %3
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9", "q10", "q11",
-        "q12", "q13", "q14", "q15");
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_uv] "+r"(src_uv),                             // %[src_uv]
+        [dst_rgb565] "+r"(dst_rgb565),                     // %[dst_rgb565]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS);
 }
 
 void YUY2ToARGBRow_NEON(const uint8_t* src_yuy2,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile(YUVTORGB_SETUP
-      "vmov.u8     d23, #255                     \n"
-      "1:                                        \n" READYUY2 YUVTORGB
-      "subs        %2, %2, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%1]!   \n"
+  asm volatile(
+      YUVTORGB_SETUP
+      "vmov.u8     d6, #255                      \n"
+      "1:                                        \n" READYUY2 YUVTORGB RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
       "bgt         1b                            \n"
-               : "+r"(src_yuy2),  // %0
-                 "+r"(dst_argb),  // %1
-                 "+r"(width)      // %2
-               : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-                 [kUVToG] "r"(&yuvconstants->kUVToG),
-                 [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-                 [kYToRgb] "r"(&yuvconstants->kYToRgb)
-               : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9",
-                 "q10", "q11", "q12", "q13", "q14", "q15");
+      : [src_yuy2] "+r"(src_yuy2),                         // %[src_yuy2]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 void UYVYToARGBRow_NEON(const uint8_t* src_uyvy,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile(YUVTORGB_SETUP
-      "vmov.u8     d23, #255                     \n"
-      "1:                                        \n" READUYVY YUVTORGB
-      "subs        %2, %2, #8                    \n"
-      "vst4.8      {d20, d21, d22, d23}, [%1]!   \n"
+  asm volatile(
+      YUVTORGB_SETUP
+      "vmov.u8     d6, #255                      \n"
+      "1:                                        \n" READUYVY YUVTORGB RGBTORGB8
+      "subs        %[width], %[width], #8        \n"
+      "vst4.8      {d0, d2, d4, d6}, [%[dst_argb]]! \n"
       "bgt         1b                            \n"
-               : "+r"(src_uyvy),  // %0
-                 "+r"(dst_argb),  // %1
-                 "+r"(width)      // %2
-               : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-                 [kUVToG] "r"(&yuvconstants->kUVToG),
-                 [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-                 [kYToRgb] "r"(&yuvconstants->kYToRgb)
-               : "cc", "memory", "q0", "q1", "q2", "q3", "q4", "q8", "q9",
-                 "q10", "q11", "q12", "q13", "q14", "q15");
+      : [src_uyvy] "+r"(src_uyvy),                         // %[src_uyvy]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "d6");
 }
 
 // Reads 16 pairs of UV and write even values to dst_u and odd to dst_v.
@@ -760,8 +740,8 @@ void MergeXRGBRow_NEON(const uint8_t* src_r,
       "vld1.8      {q1}, [%1]!                   \n"  // load G
       "vld1.8      {q0}, [%2]!                   \n"  // load B
       "subs        %4, %4, #16                   \n"  // 16 processed per loop
-      "vst4.8      {d0, d2, d4, d6}, [%4]!       \n"  // store 8 ARGB
-      "vst4.8      {d1, d3, d5, d7}, [%4]!       \n"  // next 8 ARGB
+      "vst4.8      {d0, d2, d4, d6}, [%3]!       \n"  // store 8 ARGB
+      "vst4.8      {d1, d3, d5, d7}, [%3]!       \n"  // next 8 ARGB
       "bgt         1b                            \n"
       : "+r"(src_r),                            // %0
         "+r"(src_g),                            // %1
@@ -773,6 +753,226 @@ void MergeXRGBRow_NEON(const uint8_t* src_r,
   );
 }
 
+void MergeXR30Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint8_t* dst_ar30,
+                       int depth,
+                       int width) {
+  int shift = 10 - depth;
+  asm volatile(
+      "vmov.u32    q14, #1023                    \n"
+      "vdup.32     q15, %5                       \n"
+      "1:                                        \n"
+      "vld1.16     {d4}, [%2]!                   \n"  // B
+      "vld1.16     {d2}, [%1]!                   \n"  // G
+      "vld1.16     {d0}, [%0]!                   \n"  // R
+      "vmovl.u16   q2, d4                        \n"  // B
+      "vmovl.u16   q1, d2                        \n"  // G
+      "vmovl.u16   q0, d0                        \n"  // R
+      "vshl.u32    q2, q2, q15                   \n"  // 000B
+      "vshl.u32    q1, q1, q15                   \n"
+      "vshl.u32    q0, q0, q15                   \n"
+      "vmin.u32    q2, q2, q14                   \n"
+      "vmin.u32    q1, q1, q14                   \n"
+      "vmin.u32    q0, q0, q14                   \n"
+      "vsli.u32    q2, q1, #10                   \n"  // 00GB
+      "vsli.u32    q2, q0, #20                   \n"  // 0RGB
+      "vorr.u32    q2, #0xc0000000               \n"  // ARGB (AR30)
+      "subs        %4, %4, #4                    \n"
+      "vst1.8      {q2}, [%3]!                   \n"
+      "bgt         1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_ar30),  // %3
+        "+r"(width)      // %4
+      : "r"(shift)       // %5
+      : "memory", "cc", "q0", "q1", "q2", "q14", "q15");
+}
+
+void MergeXR30Row_10_NEON(const uint16_t* src_r,
+                          const uint16_t* src_g,
+                          const uint16_t* src_b,
+                          uint8_t* dst_ar30,
+                          int /* depth */,
+                          int width) {
+  asm volatile(
+      "vmov.u32    q14, #1023                    \n"
+      "1:                                        \n"
+      "vld1.16     {d4}, [%2]!                   \n"  // B
+      "vld1.16     {d2}, [%1]!                   \n"  // G
+      "vld1.16     {d0}, [%0]!                   \n"  // R
+      "vmovl.u16   q2, d4                        \n"  // 000B
+      "vmovl.u16   q1, d2                        \n"  // G
+      "vmovl.u16   q0, d0                        \n"  // R
+      "vmin.u32    q2, q2, q14                   \n"
+      "vmin.u32    q1, q1, q14                   \n"
+      "vmin.u32    q0, q0, q14                   \n"
+      "vsli.u32    q2, q1, #10                   \n"  // 00GB
+      "vsli.u32    q2, q0, #20                   \n"  // 0RGB
+      "vorr.u32    q2, #0xc0000000               \n"  // ARGB (AR30)
+      "subs        %4, %4, #4                    \n"
+      "vst1.8      {q2}, [%3]!                   \n"
+      "bgt         1b                            \n"
+      "3:                                        \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_ar30),  // %3
+        "+r"(width)      // %4
+      :
+      : "memory", "cc", "q0", "q1", "q2", "q14");
+}
+
+void MergeAR64Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       const uint16_t* src_a,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width) {
+  int shift = 16 - depth;
+  int mask = (1 << depth) - 1;
+  asm volatile(
+
+      "vdup.u16    q15, %6                       \n"
+      "vdup.u16    q14, %7                       \n"
+      "1:                                        \n"
+      "vld1.16     {q2}, [%0]!                   \n"  // R
+      "vld1.16     {q1}, [%1]!                   \n"  // G
+      "vld1.16     {q0}, [%2]!                   \n"  // B
+      "vld1.16     {q3}, [%3]!                   \n"  // A
+      "vmin.u16    q2, q2, q14                   \n"
+      "vmin.u16    q1, q1, q14                   \n"
+      "vmin.u16    q0, q0, q14                   \n"
+      "vmin.u16    q3, q3, q14                   \n"
+      "vshl.u16    q2, q2, q15                   \n"
+      "vshl.u16    q1, q1, q15                   \n"
+      "vshl.u16    q0, q0, q15                   \n"
+      "vshl.u16    q3, q3, q15                   \n"
+      "subs        %5, %5, #8                    \n"
+      "vst4.16     {d0, d2, d4, d6}, [%4]!       \n"
+      "vst4.16     {d1, d3, d5, d7}, [%4]!       \n"
+      "bgt         1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(src_a),     // %3
+        "+r"(dst_ar64),  // %4
+        "+r"(width)      // %5
+      : "r"(shift),      // %6
+        "r"(mask)        // %7
+      : "memory", "cc", "q0", "q1", "q2", "q3", "q15");
+}
+
+void MergeXR64Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width) {
+  int shift = 16 - depth;
+  int mask = (1 << depth) - 1;
+  asm volatile(
+
+      "vmov.u8     q3, #0xff                     \n"  // A (0xffff)
+      "vdup.u16    q15, %5                       \n"
+      "vdup.u16    q14, %6                       \n"
+      "1:                                        \n"
+      "vld1.16     {q2}, [%0]!                   \n"  // R
+      "vld1.16     {q1}, [%1]!                   \n"  // G
+      "vld1.16     {q0}, [%2]!                   \n"  // B
+      "vmin.u16    q2, q2, q14                   \n"
+      "vmin.u16    q1, q1, q14                   \n"
+      "vmin.u16    q0, q0, q14                   \n"
+      "vshl.u16    q2, q2, q15                   \n"
+      "vshl.u16    q1, q1, q15                   \n"
+      "vshl.u16    q0, q0, q15                   \n"
+      "subs        %4, %4, #8                    \n"
+      "vst4.16     {d0, d2, d4, d6}, [%3]!       \n"
+      "vst4.16     {d1, d3, d5, d7}, [%3]!       \n"
+      "bgt         1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_ar64),  // %3
+        "+r"(width)      // %4
+      : "r"(shift),      // %5
+        "r"(mask)        // %6
+      : "memory", "cc", "q0", "q1", "q2", "q3", "q15");
+}
+
+void MergeARGB16To8Row_NEON(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            const uint16_t* src_a,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width) {
+  int shift = 8 - depth;
+  asm volatile(
+
+      "vdup.16     q15, %6                       \n"
+      "1:                                        \n"
+      "vld1.16     {q2}, [%0]!                   \n"  // R
+      "vld1.16     {q1}, [%1]!                   \n"  // G
+      "vld1.16     {q0}, [%2]!                   \n"  // B
+      "vld1.16     {q3}, [%3]!                   \n"  // A
+      "vshl.u16    q2, q2, q15                   \n"
+      "vshl.u16    q1, q1, q15                   \n"
+      "vshl.u16    q0, q0, q15                   \n"
+      "vshl.u16    q3, q3, q15                   \n"
+      "vqmovn.u16  d0, q0                        \n"
+      "vqmovn.u16  d1, q1                        \n"
+      "vqmovn.u16  d2, q2                        \n"
+      "vqmovn.u16  d3, q3                        \n"
+      "subs        %5, %5, #8                    \n"
+      "vst4.8      {d0, d1, d2, d3}, [%4]!       \n"
+      "bgt         1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(src_a),     // %3
+        "+r"(dst_argb),  // %4
+        "+r"(width)      // %5
+      : "r"(shift)       // %6
+      : "memory", "cc", "q0", "q1", "q2", "q3", "q15");
+}
+
+void MergeXRGB16To8Row_NEON(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width) {
+  int shift = 8 - depth;
+  asm volatile(
+
+      "vdup.16     q15, %5                       \n"
+      "vmov.u8     d6, #0xff                     \n"  // A (0xff)
+      "1:                                        \n"
+      "vld1.16     {q2}, [%0]!                   \n"  // R
+      "vld1.16     {q1}, [%1]!                   \n"  // G
+      "vld1.16     {q0}, [%2]!                   \n"  // B
+      "vshl.u16    q2, q2, q15                   \n"
+      "vshl.u16    q1, q1, q15                   \n"
+      "vshl.u16    q0, q0, q15                   \n"
+      "vqmovn.u16  d5, q2                        \n"
+      "vqmovn.u16  d4, q1                        \n"
+      "vqmovn.u16  d3, q0                        \n"
+      "subs        %4, %4, #8                    \n"
+      "vst4.u8     {d3, d4, d5, d6}, [%3]!       \n"
+      "bgt         1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_argb),  // %3
+        "+r"(width)      // %4
+      : "r"(shift)       // %5
+      : "memory", "cc", "q0", "q1", "q2", "d6", "q15");
+}
+
 // Copy multiple of 32.  vld4.8  allow unaligned and is fastest on a15.
 void CopyRow_NEON(const uint8_t* src, uint8_t* dst, int width) {
   asm volatile(
@@ -1104,16 +1304,17 @@ void ARGBToRGB24Row_NEON(const uint8_t* src_argb,
                          int width) {
   asm volatile(
       "1:                                        \n"
-      "vld4.8      {d1, d2, d3, d4}, [%0]!       \n"  // load 8 pixels of ARGB.
-      "subs        %2, %2, #8                    \n"  // 8 processed per loop.
-      "vst3.8      {d1, d2, d3}, [%1]!           \n"  // store 8 pixels of
-                                                      // RGB24.
+      "vld4.8      {d0, d2, d4, d6}, [%0]!       \n"  // load 16 pixels of ARGB.
+      "vld4.8      {d1, d3, d5, d7}, [%0]!       \n"
+      "subs        %2, %2, #16                   \n"  // 16 processed per loop.
+      "vst3.8      {d0, d2, d4}, [%1]!           \n"  // store 16 RGB24 pixels.
+      "vst3.8      {d1, d3, d5}, [%1]!           \n"
       "bgt         1b                            \n"
       : "+r"(src_argb),   // %0
         "+r"(dst_rgb24),  // %1
         "+r"(width)       // %2
       :
-      : "cc", "memory", "d1", "d2", "d3", "d4"  // Clobber List
+      : "cc", "memory", "q0", "q1", "q2", "q3"  // Clobber List
   );
 }
 
@@ -1328,16 +1529,16 @@ void ARGBToRGB565Row_NEON(const uint8_t* src_argb,
                           int width) {
   asm volatile(
       "1:                                        \n"
-      "vld4.8      {d20, d21, d22, d23}, [%0]!   \n"  // load 8 pixels of ARGB.
+      "vld4.8      {d0, d2, d4, d6}, [%0]!       \n"  // load 8 pixels of ARGB.
       "subs        %2, %2, #8                    \n"  // 8 processed per loop.
       ARGBTORGB565
-      "vst1.8      {q0}, [%1]!                   \n"  // store 8 pixels RGB565.
+      "vst1.8      {q2}, [%1]!                   \n"  // store 8 pixels RGB565.
       "bgt         1b                            \n"
       : "+r"(src_argb),    // %0
         "+r"(dst_rgb565),  // %1
         "+r"(width)        // %2
       :
-      : "cc", "memory", "q0", "q8", "q9", "q10", "q11");
+      : "cc", "memory", "q0", "q1", "q2", "d6");
 }
 
 void ARGBToRGB565DitherRow_NEON(const uint8_t* src_argb,
@@ -1345,21 +1546,21 @@ void ARGBToRGB565DitherRow_NEON(const uint8_t* src_argb,
                                 const uint32_t dither4,
                                 int width) {
   asm volatile(
-      "vdup.32     d2, %2                        \n"  // dither4
+      "vdup.32     d7, %2                        \n"  // dither4
       "1:                                        \n"
-      "vld4.8      {d20, d21, d22, d23}, [%1]!   \n"  // load 8 pixels of ARGB.
+      "vld4.8      {d0, d2, d4, d6}, [%1]!       \n"  // load 8 pixels of ARGB.
       "subs        %3, %3, #8                    \n"  // 8 processed per loop.
-      "vqadd.u8    d20, d20, d2                  \n"
-      "vqadd.u8    d21, d21, d2                  \n"
-      "vqadd.u8    d22, d22, d2                  \n"  // add for dither
+      "vqadd.u8    d0, d0, d7                    \n"
+      "vqadd.u8    d2, d2, d7                    \n"
+      "vqadd.u8    d4, d4, d7                    \n"  // add for dither
       ARGBTORGB565
-      "vst1.8      {q0}, [%0]!                   \n"  // store 8 RGB565.
+      "vst1.8      {q2}, [%0]!                   \n"  // store 8 RGB565.
       "bgt         1b                            \n"
       : "+r"(dst_rgb)   // %0
       : "r"(src_argb),  // %1
         "r"(dither4),   // %2
         "r"(width)      // %3
-      : "cc", "memory", "q0", "q1", "q8", "q9", "q10", "q11");
+      : "cc", "memory", "q0", "q1", "q2", "q3");
 }
 
 void ARGBToARGB1555Row_NEON(const uint8_t* src_argb,
@@ -1367,26 +1568,26 @@ void ARGBToARGB1555Row_NEON(const uint8_t* src_argb,
                             int width) {
   asm volatile(
       "1:                                        \n"
-      "vld4.8      {d20, d21, d22, d23}, [%0]!   \n"  // load 8 pixels of ARGB.
+      "vld4.8      {d0, d2, d4, d6}, [%0]!       \n"  // load 8 pixels of ARGB.
       "subs        %2, %2, #8                    \n"  // 8 processed per loop.
       ARGBTOARGB1555
-      "vst1.8      {q0}, [%1]!                   \n"  // store 8 ARGB1555.
+      "vst1.8      {q3}, [%1]!                   \n"  // store 8 ARGB1555.
       "bgt         1b                            \n"
       : "+r"(src_argb),      // %0
         "+r"(dst_argb1555),  // %1
         "+r"(width)          // %2
       :
-      : "cc", "memory", "q0", "q8", "q9", "q10", "q11");
+      : "cc", "memory", "q0", "q1", "q2", "q3");
 }
 
 void ARGBToARGB4444Row_NEON(const uint8_t* src_argb,
                             uint8_t* dst_argb4444,
                             int width) {
   asm volatile(
-      "vmov.u8     d4, #0x0f                     \n"  // bits to clear with
+      "vmov.u8     d7, #0x0f                     \n"  // bits to clear with
                                                       // vbic.
       "1:                                        \n"
-      "vld4.8      {d20, d21, d22, d23}, [%0]!   \n"  // load 8 pixels of ARGB.
+      "vld4.8      {d0, d2, d4, d6}, [%0]!       \n"  // load 8 pixels of ARGB.
       "subs        %2, %2, #8                    \n"  // 8 processed per loop.
       ARGBTOARGB4444
       "vst1.8      {q0}, [%1]!                   \n"  // store 8 ARGB4444.
@@ -1395,7 +1596,7 @@ void ARGBToARGB4444Row_NEON(const uint8_t* src_argb,
         "+r"(dst_argb4444),  // %1
         "+r"(width)          // %2
       :
-      : "cc", "memory", "q0", "q8", "q9", "q10", "q11");
+      : "cc", "memory", "q0", "q1", "q2", "q3");
 }
 
 void ARGBToYRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
@@ -1460,7 +1661,7 @@ void ARGBToYJRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
       : "cc", "memory", "q0", "q1", "q2", "q12", "q13");
 }
 
-void RGBAToYJRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
+void RGBAToYJRow_NEON(const uint8_t* src_rgba, uint8_t* dst_y, int width) {
   asm volatile(
       "vmov.u8     d24, #29                      \n"  // B * 0.1140 coefficient
       "vmov.u8     d25, #150                     \n"  // G * 0.5870 coefficient
@@ -1474,7 +1675,7 @@ void RGBAToYJRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
       "vqrshrn.u16 d0, q2, #8                    \n"  // 16 bit to 8 bit Y
       "vst1.8      {d0}, [%1]!                   \n"  // store 8 pixels Y.
       "bgt         1b                            \n"
-      : "+r"(src_argb),  // %0
+      : "+r"(src_rgba),  // %0
         "+r"(dst_y),     // %1
         "+r"(width)      // %2
       :
@@ -2119,6 +2320,107 @@ void ARGB4444ToYRow_NEON(const uint8_t* src_argb4444,
       : "cc", "memory", "q0", "q1", "q2", "q3", "q12", "q13");
 }
 
+void ARGBToAR64Row_NEON(const uint8_t* src_argb,
+                        uint16_t* dst_ar64,
+                        int width) {
+  asm volatile(
+      "1:                                        \n"
+      "vld1.8      {q0}, [%0]!                   \n"
+      "vld1.8      {q2}, [%0]!                   \n"
+      "vmov.u8     q1, q0                        \n"
+      "vmov.u8     q3, q2                        \n"
+      "subs        %2, %2, #8                    \n"  // 8 processed per loop.
+      "vst2.8      {q0, q1}, [%1]!               \n"  // store 4 pixels
+      "vst2.8      {q2, q3}, [%1]!               \n"  // store 4 pixels
+      "bgt         1b                            \n"
+      : "+r"(src_argb),  // %0
+        "+r"(dst_ar64),  // %1
+        "+r"(width)      // %2
+      :
+      : "cc", "memory", "q0", "q1", "q2", "q3");
+}
+
+static const uvec8 kShuffleARGBToABGR = {2,  1, 0, 3,  6,  5,  4,  7,
+                                         10, 9, 8, 11, 14, 13, 12, 15};
+
+void ARGBToAB64Row_NEON(const uint8_t* src_argb,
+                        uint16_t* dst_ab64,
+                        int width) {
+  asm volatile(
+      "vld1.8      {q4}, [%3]                    \n"  // shuffler
+
+      "1:                                        \n"
+      "vld1.8      {q0}, [%0]!                   \n"
+      "vld1.8      {q2}, [%0]!                   \n"
+      "vtbl.8      d2, {d0, d1}, d8              \n"
+      "vtbl.8      d3, {d0, d1}, d9              \n"
+      "vtbl.8      d6, {d4, d5}, d8              \n"
+      "vtbl.8      d7, {d4, d5}, d9              \n"
+      "vmov.u8     q0, q1                        \n"
+      "vmov.u8     q2, q3                        \n"
+      "subs        %2, %2, #8                    \n"  // 8 processed per loop.
+      "vst2.8      {q0, q1}, [%1]!               \n"  // store 4 pixels
+      "vst2.8      {q2, q3}, [%1]!               \n"  // store 4 pixels
+      "bgt         1b                            \n"
+      : "+r"(src_argb),           // %0
+        "+r"(dst_ab64),           // %1
+        "+r"(width)               // %2
+      : "r"(&kShuffleARGBToABGR)  // %3
+      : "cc", "memory", "q0", "q1", "q2", "q3", "q4");
+}
+
+void AR64ToARGBRow_NEON(const uint16_t* src_ar64,
+                        uint8_t* dst_argb,
+                        int width) {
+  asm volatile(
+      "1:                                        \n"
+      "vld1.16     {q0}, [%0]!                   \n"
+      "vld1.16     {q1}, [%0]!                   \n"
+      "vld1.16     {q2}, [%0]!                   \n"
+      "vld1.16     {q3}, [%0]!                   \n"
+      "vshrn.u16   d0, q0, #8                    \n"
+      "vshrn.u16   d1, q1, #8                    \n"
+      "vshrn.u16   d4, q2, #8                    \n"
+      "vshrn.u16   d5, q3, #8                    \n"
+      "subs        %2, %2, #8                    \n"  // 8 processed per loop.
+      "vst1.8      {q0}, [%1]!                   \n"  // store 4 pixels
+      "vst1.8      {q2}, [%1]!                   \n"  // store 4 pixels
+      "bgt         1b                            \n"
+      : "+r"(src_ar64),  // %0
+        "+r"(dst_argb),  // %1
+        "+r"(width)      // %2
+      :
+      : "cc", "memory", "q0", "q1", "q2", "q3");
+}
+
+static const uvec8 kShuffleAB64ToARGB = {5, 3, 1, 7, 13, 11, 9, 15};
+
+void AB64ToARGBRow_NEON(const uint16_t* src_ab64,
+                        uint8_t* dst_argb,
+                        int width) {
+  asm volatile(
+      "vld1.8      {d8}, [%3]                    \n"  // shuffler
+
+      "1:                                        \n"
+      "vld1.16     {q0}, [%0]!                   \n"
+      "vld1.16     {q1}, [%0]!                   \n"
+      "vld1.16     {q2}, [%0]!                   \n"
+      "vld1.16     {q3}, [%0]!                   \n"
+      "vtbl.8      d0, {d0, d1}, d8              \n"
+      "vtbl.8      d1, {d2, d3}, d8              \n"
+      "vtbl.8      d4, {d4, d5}, d8              \n"
+      "vtbl.8      d5, {d6, d7}, d8              \n"
+      "subs        %2, %2, #8                    \n"  // 8 processed per loop.
+      "vst1.8      {q0}, [%1]!                   \n"  // store 4 pixels
+      "vst1.8      {q2}, [%1]!                   \n"  // store 4 pixels
+      "bgt         1b                            \n"
+      : "+r"(src_ab64),           // %0
+        "+r"(dst_argb),           // %1
+        "+r"(width)               // %2
+      : "r"(&kShuffleAB64ToARGB)  // %3
+      : "cc", "memory", "q0", "q1", "q2", "q3", "q4");
+}
+
 void BGRAToYRow_NEON(const uint8_t* src_bgra, uint8_t* dst_y, int width) {
   asm volatile(
       "vmov.u8     d6, #25                       \n"  // B * 0.1016 coefficient
@@ -2263,9 +2565,9 @@ void RAWToYJRow_NEON(const uint8_t* src_raw, uint8_t* dst_yj, int width) {
       "1:                                        \n"
       "vld3.8      {d0, d1, d2}, [%0]!           \n"  // load 8 pixels of RAW.
       "subs        %2, %2, #8                    \n"  // 8 processed per loop.
-      "vmull.u8    q4, d0, d4                    \n"  // B
+      "vmull.u8    q4, d0, d4                    \n"  // R
       "vmlal.u8    q4, d1, d5                    \n"  // G
-      "vmlal.u8    q4, d2, d6                    \n"  // R
+      "vmlal.u8    q4, d2, d6                    \n"  // B
       "vqrshrn.u16 d0, q4, #8                    \n"  // 16 bit to 8 bit Y
       "vst1.8      {d0}, [%1]!                   \n"  // store 8 pixels Y.
       "bgt         1b                            \n"
@@ -2336,7 +2638,7 @@ void InterpolateRow_NEON(uint8_t* dst_ptr,
 }
 
 // dr * (256 - sa) / 256 + sr = dr - dr * sa / 256 + sr
-void ARGBBlendRow_NEON(const uint8_t* src_argb0,
+void ARGBBlendRow_NEON(const uint8_t* src_argb,
                        const uint8_t* src_argb1,
                        uint8_t* dst_argb,
                        int width) {
@@ -2387,7 +2689,7 @@ void ARGBBlendRow_NEON(const uint8_t* src_argb0,
 
       "99:                                       \n"
 
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -2625,7 +2927,7 @@ void ARGBColorMatrixRow_NEON(const uint8_t* src_argb,
 }
 
 // Multiply 2 rows of ARGB pixels together, 8 pixels at a time.
-void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
+void ARGBMultiplyRow_NEON(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -2645,7 +2947,7 @@ void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
       "vrshrn.u16  d3, q3, #8                    \n"  // 16 bit to 8 bit A
       "vst4.8      {d0, d1, d2, d3}, [%2]!       \n"  // store 8 ARGB pixels.
       "bgt         1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -2654,7 +2956,7 @@ void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
 }
 
 // Add 2 rows of ARGB pixels together, 8 pixels at a time.
-void ARGBAddRow_NEON(const uint8_t* src_argb0,
+void ARGBAddRow_NEON(const uint8_t* src_argb,
                      const uint8_t* src_argb1,
                      uint8_t* dst_argb,
                      int width) {
@@ -2668,7 +2970,7 @@ void ARGBAddRow_NEON(const uint8_t* src_argb0,
       "vqadd.u8    q1, q1, q3                    \n"  // add R, A
       "vst4.8      {d0, d1, d2, d3}, [%2]!       \n"  // store 8 ARGB pixels.
       "bgt         1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -2677,7 +2979,7 @@ void ARGBAddRow_NEON(const uint8_t* src_argb0,
 }
 
 // Subtract 2 rows of ARGB pixels, 8 pixels at a time.
-void ARGBSubtractRow_NEON(const uint8_t* src_argb0,
+void ARGBSubtractRow_NEON(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -2691,7 +2993,7 @@ void ARGBSubtractRow_NEON(const uint8_t* src_argb0,
       "vqsub.u8    q1, q1, q3                    \n"  // subtract R, A
       "vst4.8      {d0, d1, d2, d3}, [%2]!       \n"  // store 8 ARGB pixels.
       "bgt         1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -3171,32 +3473,22 @@ void SplitUVRow_16_NEON(const uint16_t* src_uv,
                         uint16_t* dst_v,
                         int depth,
                         int width) {
+  int shift = depth - 16;  // Negative for right shift.
   asm volatile(
-      "vdup.32     q0, %3                        \n"
+      "vdup.16     q2, %4                        \n"
       "1:                                        \n"
-      "vld2.16     {q1, q2}, [%0]!               \n"  // load 8 UV
-      "vmovl.u16   q3, d2                        \n"
-      "vmovl.u16   q4, d3                        \n"
-      "vshl.u32    q3, q3, q0                    \n"
-      "vshl.u32    q4, q4, q0                    \n"
-      "vmovn.u32   d2, q3                        \n"
-      "vmovn.u32   d3, q4                        \n"
-      "vmovl.u16   q3, d4                        \n"
-      "vmovl.u16   q4, d5                        \n"
-      "vshl.u32    q3, q3, q0                    \n"
-      "vshl.u32    q4, q4, q0                    \n"
-      "vmovn.u32   d4, q3                        \n"
-      "vmovn.u32   d5, q4                        \n"
-      "subs        %4, %4, #8                    \n"  // 8 src pixels per loop
-      "vst1.16     {q1}, [%1]!                   \n"  // store 8 U pixels
-      "vst1.16     {q2}, [%2]!                   \n"  // store 8 V pixels
+      "vld2.16     {q0, q1}, [%0]!               \n"  // load 8 UV
+      "vshl.u16    q0, q0, q2                    \n"
+      "vshl.u16    q1, q1, q2                    \n"
+      "subs        %3, %3, #8                    \n"  // 8 src pixels per loop
+      "vst1.16     {q0}, [%1]!                   \n"  // store 8 U pixels
+      "vst1.16     {q1}, [%2]!                   \n"  // store 8 V pixels
       "bgt         1b                            \n"
       : "+r"(src_uv),  // %0
         "+r"(dst_u),   // %1
         "+r"(dst_v),   // %2
-        "+r"(depth),   // %3
-        "+r"(width)    // %4
-      :
+        "+r"(width)    // %3
+      : "r"(shift)     // %4
       : "cc", "memory", "q0", "q1", "q2", "q3", "q4");
 }
 
@@ -3207,21 +3499,20 @@ void MergeUVRow_16_NEON(const uint16_t* src_u,
                         int width) {
   int shift = 16 - depth;
   asm volatile(
-      "vdup.16     q2, %3                        \n"
+      "vdup.16     q2, %4                        \n"
       "1:                                        \n"
       "vld1.16     {q0}, [%0]!                   \n"  // load 8 U
       "vld1.16     {q1}, [%1]!                   \n"  // load 8 V
       "vshl.u16    q0, q0, q2                    \n"
       "vshl.u16    q1, q1, q2                    \n"
-      "subs        %4, %4, #8                    \n"  // 8 src pixels per loop
+      "subs        %3, %3, #8                    \n"  // 8 src pixels per loop
       "vst2.16     {q0, q1}, [%2]!               \n"  // store 8 UV pixels
       "bgt         1b                            \n"
       : "+r"(src_u),   // %0
         "+r"(src_v),   // %1
         "+r"(dst_uv),  // %2
-        "+r"(shift),   // %3
-        "+r"(width)    // %4
-      :
+        "+r"(width)    // %3
+      : "r"(shift)     // %4
       : "cc", "memory", "q0", "q1", "q2");
 }
 
diff --git a/source/row_neon64.cc b/source/row_neon64.cc
index 941c9b98..b550ee98 100644
--- a/source/row_neon64.cc
+++ b/source/row_neon64.cc
@@ -18,93 +18,101 @@ extern "C" {
 // This module is for GCC Neon armv8 64 bit.
 #if !defined(LIBYUV_DISABLE_NEON) && defined(__aarch64__)
 
+// v0.8h: Y
+// v1.16b: 8U, 8V
+
 // Read 8 Y, 4 U and 4 V from 422
 #define READYUV422                               \
-  "ld1        {v0.8b}, [%0], #8              \n" \
-  "ld1        {v1.s}[0], [%1], #4            \n" \
-  "ld1        {v1.s}[1], [%2], #4            \n"
+  "ldr        d0, [%[src_y]], #8             \n" \
+  "ld1        {v1.s}[0], [%[src_u]], #4      \n" \
+  "ld1        {v1.s}[1], [%[src_v]], #4      \n" \
+  "zip1       v0.16b, v0.16b, v0.16b         \n" \
+  "prfm       pldl1keep, [%[src_y], 448]     \n" \
+  "zip1       v1.16b, v1.16b, v1.16b         \n" \
+  "prfm       pldl1keep, [%[src_u], 128]     \n" \
+  "prfm       pldl1keep, [%[src_v], 128]     \n"
 
 // Read 8 Y, 8 U and 8 V from 444
 #define READYUV444                               \
-  "ld1        {v0.8b}, [%0], #8              \n" \
-  "ld1        {v1.d}[0], [%1], #8            \n" \
-  "ld1        {v1.d}[1], [%2], #8            \n" \
-  "uaddlp     v1.8h, v1.16b                  \n" \
-  "rshrn      v1.8b, v1.8h, #1               \n"
+  "ldr        d0, [%[src_y]], #8             \n" \
+  "ld1        {v1.d}[0], [%[src_u]], #8      \n" \
+  "prfm       pldl1keep, [%[src_y], 448]     \n" \
+  "ld1        {v1.d}[1], [%[src_v]], #8      \n" \
+  "prfm       pldl1keep, [%[src_u], 448]     \n" \
+  "zip1       v0.16b, v0.16b, v0.16b         \n" \
+  "prfm       pldl1keep, [%[src_v], 448]     \n"
 
 // Read 8 Y, and set 4 U and 4 V to 128
 #define READYUV400                               \
-  "ld1        {v0.8b}, [%0], #8              \n" \
-  "movi       v1.8b , #128                   \n"
+  "ldr        d0, [%[src_y]], #8             \n" \
+  "movi       v1.16b, #128                   \n" \
+  "prfm       pldl1keep, [%[src_y], 448]     \n" \
+  "zip1       v0.16b, v0.16b, v0.16b         \n"
+
+static const uvec8 kNV12Table = {0, 0, 2, 2, 4, 4, 6, 6,
+                                 1, 1, 3, 3, 5, 5, 7, 7};
+static const uvec8 kNV21Table = {1, 1, 3, 3, 5, 5, 7, 7,
+                                 0, 0, 2, 2, 4, 4, 6, 6};
 
-// Read 8 Y and 4 UV from NV12
+// Read 8 Y and 4 UV from NV12 or NV21
 #define READNV12                                 \
-  "ld1        {v0.8b}, [%0], #8              \n" \
-  "ld1        {v2.8b}, [%1], #8              \n" \
-  "uzp1       v1.8b, v2.8b, v2.8b            \n" \
-  "uzp2       v3.8b, v2.8b, v2.8b            \n" \
-  "ins        v1.s[1], v3.s[0]               \n"
-
-// Read 8 Y and 4 VU from NV21
-#define READNV21                                 \
-  "ld1        {v0.8b}, [%0], #8              \n" \
-  "ld1        {v2.8b}, [%1], #8              \n" \
-  "uzp1       v3.8b, v2.8b, v2.8b            \n" \
-  "uzp2       v1.8b, v2.8b, v2.8b            \n" \
-  "ins        v1.s[1], v3.s[0]               \n"
+  "ldr        d0, [%[src_y]], #8             \n" \
+  "ldr        d1, [%[src_uv]], #8            \n" \
+  "zip1       v0.16b, v0.16b, v0.16b         \n" \
+  "prfm       pldl1keep, [%[src_y], 448]     \n" \
+  "tbl        v1.16b, {v1.16b}, v2.16b       \n" \
+  "prfm       pldl1keep, [%[src_uv], 448]    \n"
 
 // Read 8 YUY2
-#define READYUY2                                 \
-  "ld2        {v0.8b, v1.8b}, [%0], #16      \n" \
-  "uzp2       v3.8b, v1.8b, v1.8b            \n" \
-  "uzp1       v1.8b, v1.8b, v1.8b            \n" \
-  "ins        v1.s[1], v3.s[0]               \n"
+#define READYUY2                                     \
+  "ld2        {v0.8b, v1.8b}, [%[src_yuy2]], #16 \n" \
+  "zip1       v0.16b, v0.16b, v0.16b         \n"     \
+  "prfm       pldl1keep, [%[src_yuy2], 448]  \n"     \
+  "tbl        v1.16b, {v1.16b}, v2.16b       \n"
 
 // Read 8 UYVY
-#define READUYVY                                 \
-  "ld2        {v2.8b, v3.8b}, [%0], #16      \n" \
-  "orr        v0.8b, v3.8b, v3.8b            \n" \
-  "uzp1       v1.8b, v2.8b, v2.8b            \n" \
-  "uzp2       v3.8b, v2.8b, v2.8b            \n" \
-  "ins        v1.s[1], v3.s[0]               \n"
-
-#define YUVTORGB_SETUP                                      \
-  "ld3r       {v24.8h, v25.8h, v26.8h}, [%[kUVBiasBGR]] \n" \
-  "ld1r       {v31.4s}, [%[kYToRgb]]                    \n" \
-  "ld2        {v27.8h, v28.8h}, [%[kUVToRB]]            \n" \
-  "ld2        {v29.8h, v30.8h}, [%[kUVToG]]             \n"
-
-// clang-format off
-
-#define YUVTORGB(vR, vG, vB)                                        \
-  "uxtl       v0.8h, v0.8b                   \n" /* Extract Y    */ \
-  "shll       v2.8h, v1.8b, #8               \n" /* Replicate UV */ \
-  "ushll2     v3.4s, v0.8h, #0               \n" /* Y */            \
-  "ushll      v0.4s, v0.4h, #0               \n"                    \
-  "mul        v3.4s, v3.4s, v31.4s           \n"                    \
-  "mul        v0.4s, v0.4s, v31.4s           \n"                    \
-  "sqshrun    v0.4h, v0.4s, #16              \n"                    \
-  "sqshrun2   v0.8h, v3.4s, #16              \n" /* Y */            \
-  "uaddw      v1.8h, v2.8h, v1.8b            \n" /* Replicate UV */ \
-  "mov        v2.d[0], v1.d[1]               \n" /* Extract V */    \
-  "uxtl       v2.8h, v2.8b                   \n"                    \
-  "uxtl       v1.8h, v1.8b                   \n" /* Extract U */    \
-  "mul        v3.8h, v27.8h, v1.8h           \n"                    \
-  "mul        v5.8h, v29.8h, v1.8h           \n"                    \
-  "mul        v6.8h, v30.8h, v2.8h           \n"                    \
-  "mul        v7.8h, v28.8h, v2.8h           \n"                    \
-  "sqadd      v6.8h, v6.8h, v5.8h            \n"                    \
-  "sqadd      " #vB ".8h, v24.8h, v0.8h      \n" /* B */            \
-  "sqadd      " #vG ".8h, v25.8h, v0.8h      \n" /* G */            \
-  "sqadd      " #vR ".8h, v26.8h, v0.8h      \n" /* R */            \
-  "sqadd      " #vB ".8h, " #vB ".8h, v3.8h  \n" /* B */            \
-  "sqsub      " #vG ".8h, " #vG ".8h, v6.8h  \n" /* G */            \
-  "sqadd      " #vR ".8h, " #vR ".8h, v7.8h  \n" /* R */            \
-  "sqshrun    " #vB ".8b, " #vB ".8h, #6     \n" /* B */            \
-  "sqshrun    " #vG ".8b, " #vG ".8h, #6     \n" /* G */            \
-  "sqshrun    " #vR ".8b, " #vR ".8h, #6     \n" /* R */
-
-// clang-format on
+#define READUYVY                                     \
+  "ld2        {v3.8b, v4.8b}, [%[src_uyvy]], #16 \n" \
+  "zip1       v0.16b, v4.16b, v4.16b         \n"     \
+  "prfm       pldl1keep, [%[src_uyvy], 448]  \n"     \
+  "tbl        v1.16b, {v3.16b}, v2.16b       \n"
+
+// UB VR UG VG
+// YG BB BG BR
+#define YUVTORGB_SETUP                                                \
+  "ld4r       {v28.16b, v29.16b, v30.16b, v31.16b}, [%[kUVCoeff]] \n" \
+  "ld4r       {v24.8h, v25.8h, v26.8h, v27.8h}, [%[kRGBCoeffBias]] \n"
+
+// v16.8h: B
+// v17.8h: G
+// v18.8h: R
+
+// Convert from YUV to 2.14 fixed point RGB
+#define YUVTORGB                                          \
+  "umull2     v3.4s, v0.8h, v24.8h           \n"          \
+  "umull      v6.8h, v1.8b, v30.8b           \n"          \
+  "umull      v0.4s, v0.4h, v24.4h           \n"          \
+  "umlal2     v6.8h, v1.16b, v31.16b         \n" /* DG */ \
+  "uqshrn     v0.4h, v0.4s, #16              \n"          \
+  "uqshrn2    v0.8h, v3.4s, #16              \n" /* Y */  \
+  "umull      v4.8h, v1.8b, v28.8b           \n" /* DB */ \
+  "umull2     v5.8h, v1.16b, v29.16b         \n" /* DR */ \
+  "add        v17.8h, v0.8h, v26.8h          \n" /* G */  \
+  "add        v16.8h, v0.8h, v4.8h           \n" /* B */  \
+  "add        v18.8h, v0.8h, v5.8h           \n" /* R */  \
+  "uqsub      v17.8h, v17.8h, v6.8h          \n" /* G */  \
+  "uqsub      v16.8h, v16.8h, v25.8h         \n" /* B */  \
+  "uqsub      v18.8h, v18.8h, v27.8h         \n" /* R */
+
+// Convert from 2.14 fixed point RGB To 8 bit RGB
+#define RGBTORGB8                                \
+  "uqshrn     v17.8b, v17.8h, #6             \n" \
+  "uqshrn     v16.8b, v16.8h, #6             \n" \
+  "uqshrn     v18.8b, v18.8h, #6             \n"
+
+#define YUVTORGB_REGS                                                          \
+  "v0", "v1", "v3", "v4", "v5", "v6", "v7", "v16", "v17", "v18", "v24", "v25", \
+      "v26", "v27", "v28", "v29", "v30", "v31"
 
 void I444ToARGBRow_NEON(const uint8_t* src_y,
                         const uint8_t* src_u,
@@ -112,30 +120,22 @@ void I444ToARGBRow_NEON(const uint8_t* src_y,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n" /* A */
-      "1:                                        \n"
-    READYUV444
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%1, 448]          \n"
-      "prfm        pldl1keep, [%2, 448]          \n"
-      "subs        %w4, %w4, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%3], #32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_u),     // %1
-      "+r"(src_v),     // %2
-      "+r"(dst_argb),  // %3
-      "+r"(width)      // %4
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n" /* A */
+      "1:                                        \n" READYUV444 YUVTORGB
+          RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v19");
 }
 
 void I422ToARGBRow_NEON(const uint8_t* src_y,
@@ -144,31 +144,22 @@ void I422ToARGBRow_NEON(const uint8_t* src_y,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n" /* A */
-
-      "1:                                        \n"
-    READYUV422
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "subs        %w4, %w4, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%3], #32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_u),     // %1
-      "+r"(src_v),     // %2
-      "+r"(dst_argb),  // %3
-      "+r"(width)      // %4
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n" /* A */
+      "1:                                        \n" READYUV422 YUVTORGB
+          RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v19");
 }
 
 void I444AlphaToARGBRow_NEON(const uint8_t* src_y,
@@ -178,32 +169,23 @@ void I444AlphaToARGBRow_NEON(const uint8_t* src_y,
                              uint8_t* dst_argb,
                              const struct YuvConstants* yuvconstants,
                              int width) {
-  asm volatile (
-    YUVTORGB_SETUP
+  asm volatile(
+      YUVTORGB_SETUP
       "1:                                        \n"
-    READYUV444
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "ld1         {v23.8b}, [%3], #8            \n"
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "prfm        pldl1keep, [%3, 448]          \n"
-      "subs        %w5, %w5, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%4], #32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_u),     // %1
-      "+r"(src_v),     // %2
-      "+r"(src_a),     // %3
-      "+r"(dst_argb),  // %4
-      "+r"(width)      // %5
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+      "ld1         {v19.8b}, [%[src_a]], #8      \n" READYUV444
+      "prfm        pldl1keep, [%[src_a], 448]    \n" YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [src_a] "+r"(src_a),                               // %[src_a]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v19");
 }
 
 void I422AlphaToARGBRow_NEON(const uint8_t* src_y,
@@ -213,32 +195,23 @@ void I422AlphaToARGBRow_NEON(const uint8_t* src_y,
                              uint8_t* dst_argb,
                              const struct YuvConstants* yuvconstants,
                              int width) {
-  asm volatile (
-    YUVTORGB_SETUP
+  asm volatile(
+      YUVTORGB_SETUP
       "1:                                        \n"
-    READYUV422
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "ld1         {v23.8b}, [%3], #8            \n"
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "prfm        pldl1keep, [%3, 448]          \n"
-      "subs        %w5, %w5, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%4], #32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_u),     // %1
-      "+r"(src_v),     // %2
-      "+r"(src_a),     // %3
-      "+r"(dst_argb),  // %4
-      "+r"(width)      // %5
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+      "ld1         {v19.8b}, [%[src_a]], #8      \n" READYUV422
+      "prfm        pldl1keep, [%[src_a], 448]    \n" YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [src_a] "+r"(src_a),                               // %[src_a]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v19");
 }
 
 void I422ToRGBARow_NEON(const uint8_t* src_y,
@@ -247,30 +220,22 @@ void I422ToRGBARow_NEON(const uint8_t* src_y,
                         uint8_t* dst_rgba,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v20.8b, #255                  \n" /* A */
-      "1:                                        \n"
-    READYUV422
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v23, v22, v21)
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "subs        %w4, %w4, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%3], #32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_u),     // %1
-      "+r"(src_v),     // %2
-      "+r"(dst_rgba),  // %3
-      "+r"(width)      // %4
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v15.8b, #255                  \n" /* A */
+      "1:                                        \n" READYUV422 YUVTORGB
+          RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v15.8b,v16.8b,v17.8b,v18.8b}, [%[dst_rgba]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_rgba] "+r"(dst_rgba),                         // %[dst_rgba]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v15");
 }
 
 void I422ToRGB24Row_NEON(const uint8_t* src_y,
@@ -279,39 +244,29 @@ void I422ToRGB24Row_NEON(const uint8_t* src_y,
                          uint8_t* dst_rgb24,
                          const struct YuvConstants* yuvconstants,
                          int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "1:                                        \n"
-    READYUV422
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "subs        %w4, %w4, #8                  \n"
-      "st3         {v20.8b,v21.8b,v22.8b}, [%3], #24 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_u),     // %1
-      "+r"(src_v),     // %2
-      "+r"(dst_rgb24), // %3
-      "+r"(width)      // %4
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "1:                                        \n" READYUV422 YUVTORGB
+          RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st3         {v16.8b,v17.8b,v18.8b}, [%[dst_rgb24]], #24 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_rgb24] "+r"(dst_rgb24),                       // %[dst_rgb24]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS);
 }
 
 #define ARGBTORGB565                                                        \
-  "shll       v0.8h,  v22.8b, #8             \n" /* R                    */ \
-  "shll       v21.8h, v21.8b, #8             \n" /* G                    */ \
-  "shll       v20.8h, v20.8b, #8             \n" /* B                    */ \
-  "sri        v0.8h,  v21.8h, #5             \n" /* RG                   */ \
-  "sri        v0.8h,  v20.8h, #11            \n" /* RGB                  */
-
-// clang-format off
+  "shll       v18.8h, v18.8b, #8             \n" /* R                    */ \
+  "shll       v17.8h, v17.8b, #8             \n" /* G                    */ \
+  "shll       v16.8h, v16.8b, #8             \n" /* B                    */ \
+  "sri        v18.8h, v17.8h, #5             \n" /* RG                   */ \
+  "sri        v18.8h, v16.8h, #11            \n" /* RGB                  */
 
 void I422ToRGB565Row_NEON(const uint8_t* src_y,
                           const uint8_t* src_u,
@@ -320,38 +275,29 @@ void I422ToRGB565Row_NEON(const uint8_t* src_y,
                           const struct YuvConstants* yuvconstants,
                           int width) {
   asm volatile(
-    YUVTORGB_SETUP
-      "1:                                        \n"
-    READYUV422
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w4, %w4, #8                  \n"
-    ARGBTORGB565
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "st1         {v0.8h}, [%3], #16            \n"  // store 8 pixels RGB565.
-      "b.gt        1b                            \n"
-      : "+r"(src_y),       // %0
-        "+r"(src_u),       // %1
-        "+r"(src_v),       // %2
-        "+r"(dst_rgb565),  // %3
-        "+r"(width)        // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-        "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30");
+      YUVTORGB_SETUP
+      "1:                                        \n" READYUV422 YUVTORGB
+          RGBTORGB8 "subs        %w[width], %w[width], #8      \n" ARGBTORGB565
+      "st1         {v18.8h}, [%[dst_rgb565]], #16 \n"  // store 8 pixels RGB565.
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_rgb565] "+r"(dst_rgb565),                     // %[dst_rgb565]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS);
 }
 
 #define ARGBTOARGB1555                                                      \
-  "shll       v0.8h,  v23.8b, #8             \n" /* A                    */ \
-  "shll       v22.8h, v22.8b, #8             \n" /* R                    */ \
-  "shll       v21.8h, v21.8b, #8             \n" /* G                    */ \
-  "shll       v20.8h, v20.8b, #8             \n" /* B                    */ \
-  "sri        v0.8h,  v22.8h, #1             \n" /* AR                   */ \
-  "sri        v0.8h,  v21.8h, #6             \n" /* ARG                  */ \
-  "sri        v0.8h,  v20.8h, #11            \n" /* ARGB                 */
+  "shll       v0.8h,  v19.8b, #8             \n" /* A                    */ \
+  "shll       v18.8h, v18.8b, #8             \n" /* R                    */ \
+  "shll       v17.8h, v17.8b, #8             \n" /* G                    */ \
+  "shll       v16.8h, v16.8b, #8             \n" /* B                    */ \
+  "sri        v0.8h,  v18.8h, #1             \n" /* AR                   */ \
+  "sri        v0.8h,  v17.8h, #6             \n" /* ARG                  */ \
+  "sri        v0.8h,  v16.8h, #11            \n" /* ARGB                 */
 
 void I422ToARGB1555Row_NEON(const uint8_t* src_y,
                             const uint8_t* src_u,
@@ -360,40 +306,32 @@ void I422ToARGB1555Row_NEON(const uint8_t* src_y,
                             const struct YuvConstants* yuvconstants,
                             int width) {
   asm volatile(
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n"
-      "1:                                        \n"
-    READYUV422
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w4, %w4, #8                  \n"
-    ARGBTOARGB1555
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "st1         {v0.8h}, [%3], #16            \n"  // store 8 pixels RGB565.
-      "b.gt        1b                            \n"
-      : "+r"(src_y),         // %0
-        "+r"(src_u),         // %1
-        "+r"(src_v),         // %2
-        "+r"(dst_argb1555),  // %3
-        "+r"(width)          // %4
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-        "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30");
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n"
+      "1:                                        \n" READYUV422 YUVTORGB
+          RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n" ARGBTOARGB1555
+      "st1         {v0.8h}, [%[dst_argb1555]], #16 \n"  // store 8 pixels
+                                                        // RGB565.
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb1555] "+r"(dst_argb1555),                 // %[dst_argb1555]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v19");
 }
-// clang-format on
 
 #define ARGBTOARGB4444                                                       \
-  /* Input v20.8b<=B, v21.8b<=G, v22.8b<=R, v23.8b<=A, v4.8b<=0x0f        */ \
-  "ushr       v20.8b, v20.8b, #4             \n" /* B                    */  \
-  "bic        v21.8b, v21.8b, v4.8b          \n" /* G                    */  \
-  "ushr       v22.8b, v22.8b, #4             \n" /* R                    */  \
-  "bic        v23.8b, v23.8b, v4.8b          \n" /* A                    */  \
-  "orr        v0.8b,  v20.8b, v21.8b         \n" /* BG                   */  \
-  "orr        v1.8b,  v22.8b, v23.8b         \n" /* RA                   */  \
+  /* Input v16.8b<=B, v17.8b<=G, v18.8b<=R, v19.8b<=A, v23.8b<=0x0f       */ \
+  "ushr       v16.8b, v16.8b, #4             \n" /* B                    */  \
+  "bic        v17.8b, v17.8b, v23.8b         \n" /* G                    */  \
+  "ushr       v18.8b, v18.8b, #4             \n" /* R                    */  \
+  "bic        v19.8b, v19.8b, v23.8b         \n" /* A                    */  \
+  "orr        v0.8b,  v16.8b, v17.8b         \n" /* BG                   */  \
+  "orr        v1.8b,  v18.8b, v19.8b         \n" /* RA                   */  \
   "zip1       v0.16b, v0.16b, v1.16b         \n" /* BGRA                 */
 
 void I422ToARGB4444Row_NEON(const uint8_t* src_y,
@@ -402,58 +340,46 @@ void I422ToARGB4444Row_NEON(const uint8_t* src_y,
                             uint8_t* dst_argb4444,
                             const struct YuvConstants* yuvconstants,
                             int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v4.16b, #0x0f                 \n"  // bits to clear with vbic.
-      "1:                                        \n"
-    READYUV422
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w4, %w4, #8                  \n"
-      "movi        v23.8b, #255                  \n"
-    ARGBTOARGB4444
-      "prfm        pldl1keep, [%1, 128]          \n"
-      "prfm        pldl1keep, [%2, 128]          \n"
-      "st1         {v0.8h}, [%3], #16            \n"  // store 8 pixels ARGB4444.
-      "b.gt        1b                            \n"
-    : "+r"(src_y),    // %0
-      "+r"(src_u),    // %1
-      "+r"(src_v),    // %2
-      "+r"(dst_argb4444),  // %3
-      "+r"(width)     // %4
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v23.16b, #0x0f                \n"  // bits to clear with
+                                                      // vbic.
+      "1:                                        \n" READYUV422 YUVTORGB
+          RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "movi        v19.8b, #255                  \n" ARGBTOARGB4444
+      "st1         {v0.8h}, [%[dst_argb4444]], #16 \n"  // store 8
+                                                        // pixels
+                                                        // ARGB4444.
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [src_u] "+r"(src_u),                               // %[src_u]
+        [src_v] "+r"(src_v),                               // %[src_v]
+        [dst_argb4444] "+r"(dst_argb4444),                 // %[dst_argb4444]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v19", "v23");
 }
 
 void I400ToARGBRow_NEON(const uint8_t* src_y,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n"
-      "1:                                        \n"
-    READYUV400
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%1], #32 \n"
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n"
+      "1:                                        \n" READYUV400 YUVTORGB
+          RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
       "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(dst_argb),  // %1
-      "+r"(width)      // %2
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+      : [src_y] "+r"(src_y),                               // %[src_y]
+        [dst_argb] "+r"(dst_argb),                         // %[dst_argb]
+        [width] "+r"(width)                                // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),           // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias)  // %[kRGBCoeffBias]
+      : "cc", "memory", YUVTORGB_REGS, "v19");
 }
 
 void J400ToARGBRow_NEON(const uint8_t* src_y, uint8_t* dst_argb, int width) {
@@ -479,28 +405,22 @@ void NV12ToARGBRow_NEON(const uint8_t* src_y,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n"
-      "1:                                        \n"
-    READNV12
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%1, 256]          \n"
-      "subs        %w3, %w3, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%2], #32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_uv),    // %1
-      "+r"(dst_argb),  // %2
-      "+r"(width)      // %3
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n"
+      "ldr         q2, [%[kNV12Table]]           \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                                // %[src_y]
+        [src_uv] "+r"(src_uv),                              // %[src_uv]
+        [dst_argb] "+r"(dst_argb),                          // %[dst_argb]
+        [width] "+r"(width)                                 // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),            // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias),  // %[kRGBCoeffBias]
+        [kNV12Table] "r"(&kNV12Table)
+      : "cc", "memory", YUVTORGB_REGS, "v2", "v19");
 }
 
 void NV21ToARGBRow_NEON(const uint8_t* src_y,
@@ -508,28 +428,22 @@ void NV21ToARGBRow_NEON(const uint8_t* src_y,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n"
-      "1:                                        \n"
-    READNV21
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%1, 256]          \n"
-      "subs        %w3, %w3, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%2], #32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_vu),    // %1
-      "+r"(dst_argb),  // %2
-      "+r"(width)      // %3
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n"
+      "ldr         q2, [%[kNV12Table]]           \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                                // %[src_y]
+        [src_uv] "+r"(src_vu),                              // %[src_uv]
+        [dst_argb] "+r"(dst_argb),                          // %[dst_argb]
+        [width] "+r"(width)                                 // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),            // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias),  // %[kRGBCoeffBias]
+        [kNV12Table] "r"(&kNV21Table)
+      : "cc", "memory", YUVTORGB_REGS, "v2", "v19");
 }
 
 void NV12ToRGB24Row_NEON(const uint8_t* src_y,
@@ -537,27 +451,21 @@ void NV12ToRGB24Row_NEON(const uint8_t* src_y,
                          uint8_t* dst_rgb24,
                          const struct YuvConstants* yuvconstants,
                          int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "1:                                        \n"
-    READNV12
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%1, 256]          \n"
-      "subs        %w3, %w3, #8                  \n"
-      "st3         {v20.8b,v21.8b,v22.8b}, [%2], #24 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_uv),    // %1
-      "+r"(dst_rgb24),  // %2
-      "+r"(width)      // %3
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "ldr         q2, [%[kNV12Table]]           \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st3         {v16.8b,v17.8b,v18.8b}, [%[dst_rgb24]], #24 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                                // %[src_y]
+        [src_uv] "+r"(src_uv),                              // %[src_uv]
+        [dst_rgb24] "+r"(dst_rgb24),                        // %[dst_rgb24]
+        [width] "+r"(width)                                 // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),            // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias),  // %[kRGBCoeffBias]
+        [kNV12Table] "r"(&kNV12Table)
+      : "cc", "memory", YUVTORGB_REGS, "v2");
 }
 
 void NV21ToRGB24Row_NEON(const uint8_t* src_y,
@@ -565,27 +473,21 @@ void NV21ToRGB24Row_NEON(const uint8_t* src_y,
                          uint8_t* dst_rgb24,
                          const struct YuvConstants* yuvconstants,
                          int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "1:                                        \n"
-    READNV21
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%1, 256]          \n"
-      "subs        %w3, %w3, #8                  \n"
-      "st3         {v20.8b,v21.8b,v22.8b}, [%2], #24 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_y),     // %0
-      "+r"(src_vu),    // %1
-      "+r"(dst_rgb24),  // %2
-      "+r"(width)      // %3
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "ldr         q2, [%[kNV12Table]]           \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st3         {v16.8b,v17.8b,v18.8b}, [%[dst_rgb24]], #24 \n"
+      "b.gt        1b                            \n"
+      : [src_y] "+r"(src_y),                                // %[src_y]
+        [src_uv] "+r"(src_vu),                              // %[src_uv]
+        [dst_rgb24] "+r"(dst_rgb24),                        // %[dst_rgb24]
+        [width] "+r"(width)                                 // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),            // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias),  // %[kRGBCoeffBias]
+        [kNV12Table] "r"(&kNV21Table)
+      : "cc", "memory", YUVTORGB_REGS, "v2");
 }
 
 void NV12ToRGB565Row_NEON(const uint8_t* src_y,
@@ -594,75 +496,64 @@ void NV12ToRGB565Row_NEON(const uint8_t* src_y,
                           const struct YuvConstants* yuvconstants,
                           int width) {
   asm volatile(
-      YUVTORGB_SETUP "1:                                        \n" READNV12
-      "prfm        pldl1keep, [%0, 448]          \n" YUVTORGB(
-                         v22, v21, v20) ARGBTORGB565
-      "prfm        pldl1keep, [%1, 256]          \n"
-      "subs        %w3, %w3, #8                  \n"
-      "st1         {v0.8h}, [%2], 16             \n"  // store 8 pixels
+      YUVTORGB_SETUP
+      "ldr         q2, [%[kNV12Table]]           \n"
+      "1:                                        \n" READNV12 YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n" ARGBTORGB565
+      "st1         {v18.8h}, [%[dst_rgb565]], #16 \n"  // store 8
+                                                       // pixels
+                                                       // RGB565.
       "b.gt        1b                            \n"
-      : "+r"(src_y),       // %0
-        "+r"(src_uv),      // %1
-        "+r"(dst_rgb565),  // %2
-        "+r"(width)        // %3
-      : [kUVToRB] "r"(&yuvconstants->kUVToRB),
-        [kUVToG] "r"(&yuvconstants->kUVToG),
-        [kUVBiasBGR] "r"(&yuvconstants->kUVBiasBGR),
-        [kYToRgb] "r"(&yuvconstants->kYToRgb)
-      : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-        "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30");
+      : [src_y] "+r"(src_y),                                // %[src_y]
+        [src_uv] "+r"(src_uv),                              // %[src_uv]
+        [dst_rgb565] "+r"(dst_rgb565),                      // %[dst_rgb565]
+        [width] "+r"(width)                                 // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),            // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias),  // %[kRGBCoeffBias]
+        [kNV12Table] "r"(&kNV12Table)
+      : "cc", "memory", YUVTORGB_REGS, "v2");
 }
 
 void YUY2ToARGBRow_NEON(const uint8_t* src_yuy2,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n"
-      "1:                                        \n"
-    READYUY2
-      "prfm        pldl1keep, [%0, 448]          \n"
-    YUVTORGB(v22, v21, v20)
-      "subs        %w2, %w2, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%1], #32 \n"
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n"
+      "ldr         q2, [%[kNV12Table]]           \n"
+      "1:                                        \n" READYUY2 YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
       "b.gt        1b                            \n"
-    : "+r"(src_yuy2),  // %0
-      "+r"(dst_argb),  // %1
-      "+r"(width)      // %2
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+      : [src_yuy2] "+r"(src_yuy2),                          // %[src_yuy2]
+        [dst_argb] "+r"(dst_argb),                          // %[dst_argb]
+        [width] "+r"(width)                                 // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),            // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias),  // %[kRGBCoeffBias]
+        [kNV12Table] "r"(&kNV12Table)
+      : "cc", "memory", YUVTORGB_REGS, "v2", "v19");
 }
 
 void UYVYToARGBRow_NEON(const uint8_t* src_uyvy,
                         uint8_t* dst_argb,
                         const struct YuvConstants* yuvconstants,
                         int width) {
-  asm volatile (
-    YUVTORGB_SETUP
-      "movi        v23.8b, #255                  \n"
-      "1:                                        \n"
-    READUYVY
-    YUVTORGB(v22, v21, v20)
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"
-      "st4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%1], 32 \n"
-      "b.gt        1b                            \n"
-    : "+r"(src_uyvy),  // %0
-      "+r"(dst_argb),  // %1
-      "+r"(width)      // %2
-    : [kUVToRB]"r"(&yuvconstants->kUVToRB),
-      [kUVToG]"r"(&yuvconstants->kUVToG),
-      [kUVBiasBGR]"r"(&yuvconstants->kUVBiasBGR),
-      [kYToRgb]"r"(&yuvconstants->kYToRgb)
-    : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v20",
-      "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30"
-  );
+  asm volatile(
+      YUVTORGB_SETUP
+      "movi        v19.8b, #255                  \n"
+      "ldr         q2, [%[kNV12Table]]           \n"
+      "1:                                        \n" READUYVY YUVTORGB RGBTORGB8
+      "subs        %w[width], %w[width], #8      \n"
+      "st4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%[dst_argb]], #32 \n"
+      "b.gt        1b                            \n"
+      : [src_uyvy] "+r"(src_uyvy),                          // %[src_yuy2]
+        [dst_argb] "+r"(dst_argb),                          // %[dst_argb]
+        [width] "+r"(width)                                 // %[width]
+      : [kUVCoeff] "r"(&yuvconstants->kUVCoeff),            // %[kUVCoeff]
+        [kRGBCoeffBias] "r"(&yuvconstants->kRGBCoeffBias),  // %[kRGBCoeffBias]
+        [kNV12Table] "r"(&kNV12Table)
+      : "cc", "memory", YUVTORGB_REGS, "v2", "v19");
 }
 
 // Reads 16 pairs of UV and write even values to dst_u and odd to dst_v.
@@ -673,8 +564,8 @@ void SplitUVRow_NEON(const uint8_t* src_uv,
   asm volatile(
       "1:                                        \n"
       "ld2         {v0.16b,v1.16b}, [%0], #32    \n"  // load 16 pairs of UV
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w3, %w3, #16                 \n"  // 16 processed per loop
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v0.16b}, [%1], #16           \n"  // store U
       "st1         {v1.16b}, [%2], #16           \n"  // store V
       "b.gt        1b                            \n"
@@ -696,9 +587,9 @@ void MergeUVRow_NEON(const uint8_t* src_u,
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load U
       "ld1         {v1.16b}, [%1], #16           \n"  // load V
+      "subs        %w3, %w3, #16                 \n"  // 16 processed per loop
       "prfm        pldl1keep, [%0, 448]          \n"
       "prfm        pldl1keep, [%1, 448]          \n"
-      "subs        %w3, %w3, #16                 \n"  // 16 processed per loop
       "st2         {v0.16b,v1.16b}, [%2], #32    \n"  // store 16 pairs of UV
       "b.gt        1b                            \n"
       : "+r"(src_u),                // %0
@@ -719,8 +610,8 @@ void SplitRGBRow_NEON(const uint8_t* src_rgb,
   asm volatile(
       "1:                                        \n"
       "ld3         {v0.16b,v1.16b,v2.16b}, [%0], #48 \n"  // load 16 RGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w4, %w4, #16                 \n"  // 16 processed per loop
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v0.16b}, [%1], #16           \n"  // store R
       "st1         {v1.16b}, [%2], #16           \n"  // store G
       "st1         {v2.16b}, [%3], #16           \n"  // store B
@@ -746,12 +637,11 @@ void MergeRGBRow_NEON(const uint8_t* src_r,
       "ld1         {v0.16b}, [%0], #16           \n"  // load R
       "ld1         {v1.16b}, [%1], #16           \n"  // load G
       "ld1         {v2.16b}, [%2], #16           \n"  // load B
+      "subs        %w4, %w4, #16                 \n"  // 16 processed per loop
       "prfm        pldl1keep, [%0, 448]          \n"
       "prfm        pldl1keep, [%1, 448]          \n"
       "prfm        pldl1keep, [%2, 448]          \n"
-      "subs        %w4, %w4, #16                 \n"  // 16 processed per loop
       "st3         {v0.16b,v1.16b,v2.16b}, [%3], #48 \n"  // store 16 RGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "b.gt        1b                            \n"
       : "+r"(src_r),                      // %0
         "+r"(src_g),                      // %1
@@ -773,8 +663,8 @@ void SplitARGBRow_NEON(const uint8_t* src_rgba,
   asm volatile(
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w5, %w5, #16                 \n"  // 16 processed per loop
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v0.16b}, [%3], #16           \n"  // store B
       "st1         {v1.16b}, [%2], #16           \n"  // store G
       "st1         {v2.16b}, [%1], #16           \n"  // store R
@@ -804,11 +694,11 @@ void MergeARGBRow_NEON(const uint8_t* src_r,
       "ld1         {v1.16b}, [%1], #16           \n"  // load G
       "ld1         {v0.16b}, [%2], #16           \n"  // load B
       "ld1         {v3.16b}, [%3], #16           \n"  // load A
+      "subs        %w5, %w5, #16                 \n"  // 16 processed per loop
       "prfm        pldl1keep, [%0, 448]          \n"
       "prfm        pldl1keep, [%1, 448]          \n"
       "prfm        pldl1keep, [%2, 448]          \n"
       "prfm        pldl1keep, [%3, 448]          \n"
-      "subs        %w5, %w5, #16                 \n"  // 16 processed per loop
       "st4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%4], #64 \n"  // store 16ARGB
       "b.gt        1b                            \n"
       : "+r"(src_r),                            // %0
@@ -831,8 +721,8 @@ void SplitXRGBRow_NEON(const uint8_t* src_rgba,
   asm volatile(
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w4, %w4, #16                 \n"  // 16 processed per loop
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v0.16b}, [%3], #16           \n"  // store B
       "st1         {v1.16b}, [%2], #16           \n"  // store G
       "st1         {v2.16b}, [%1], #16           \n"  // store R
@@ -859,10 +749,10 @@ void MergeXRGBRow_NEON(const uint8_t* src_r,
       "ld1         {v2.16b}, [%0], #16           \n"  // load R
       "ld1         {v1.16b}, [%1], #16           \n"  // load G
       "ld1         {v0.16b}, [%2], #16           \n"  // load B
+      "subs        %w4, %w4, #16                 \n"  // 16 processed per loop
       "prfm        pldl1keep, [%0, 448]          \n"
       "prfm        pldl1keep, [%1, 448]          \n"
       "prfm        pldl1keep, [%2, 448]          \n"
-      "subs        %w4, %w4, #16                 \n"  // 16 processed per loop
       "st4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%3], #64 \n"  // store 16ARGB
       "b.gt        1b                            \n"
       : "+r"(src_r),                            // %0
@@ -875,6 +765,240 @@ void MergeXRGBRow_NEON(const uint8_t* src_r,
   );
 }
 
+void MergeXR30Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint8_t* dst_ar30,
+                       int depth,
+                       int width) {
+  int shift = 10 - depth;
+  asm volatile(
+      "movi        v30.16b, #255                 \n"
+      "ushr        v30.4s, v30.4s, #22           \n"  // 1023
+      "dup         v31.4s, %w5                   \n"
+      "1:                                        \n"
+      "ldr         d2, [%2], #8                  \n"  // B
+      "ldr         d1, [%1], #8                  \n"  // G
+      "ldr         d0, [%0], #8                  \n"  // R
+      "ushll       v2.4s, v2.4h, #0              \n"  // B
+      "ushll       v1.4s, v1.4h, #0              \n"  // G
+      "ushll       v0.4s, v0.4h, #0              \n"  // R
+      "ushl        v2.4s, v2.4s, v31.4s          \n"  // 000B
+      "ushl        v1.4s, v1.4s, v31.4s          \n"  // G
+      "ushl        v0.4s, v0.4s, v31.4s          \n"  // R
+      "umin        v2.4s, v2.4s, v30.4s          \n"
+      "umin        v1.4s, v1.4s, v30.4s          \n"
+      "umin        v0.4s, v0.4s, v30.4s          \n"
+      "sli         v2.4s, v1.4s, #10             \n"  // 00GB
+      "sli         v2.4s, v0.4s, #20             \n"  // 0RGB
+      "orr         v2.4s, #0xc0, lsl #24         \n"  // ARGB (AR30)
+      "subs        %w4, %w4, #4                  \n"
+      "str         q2, [%3], #16                 \n"
+      "b.gt        1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_ar30),  // %3
+        "+r"(width)      // %4
+      : "r"(shift)       // %5
+      : "memory", "cc", "v0", "v1", "v2", "v30", "v31");
+}
+
+void MergeXR30Row_10_NEON(const uint16_t* src_r,
+                          const uint16_t* src_g,
+                          const uint16_t* src_b,
+                          uint8_t* dst_ar30,
+                          int /* depth */,
+                          int width) {
+  asm volatile(
+      "movi        v30.16b, #255                 \n"
+      "ushr        v30.4s, v30.4s, #22           \n"  // 1023
+      "1:                                        \n"
+      "ldr         d2, [%2], #8                  \n"  // B
+      "ldr         d1, [%1], #8                  \n"  // G
+      "ldr         d0, [%0], #8                  \n"  // R
+      "ushll       v2.4s, v2.4h, #0              \n"  // 000B
+      "ushll       v1.4s, v1.4h, #0              \n"  // G
+      "ushll       v0.4s, v0.4h, #0              \n"  // R
+      "umin        v2.4s, v2.4s, v30.4s          \n"
+      "umin        v1.4s, v1.4s, v30.4s          \n"
+      "umin        v0.4s, v0.4s, v30.4s          \n"
+      "sli         v2.4s, v1.4s, #10             \n"  // 00GB
+      "sli         v2.4s, v0.4s, #20             \n"  // 0RGB
+      "orr         v2.4s, #0xc0, lsl #24         \n"  // ARGB (AR30)
+      "subs        %w4, %w4, #4                  \n"
+      "str         q2, [%3], #16                 \n"
+      "b.gt        1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_ar30),  // %3
+        "+r"(width)      // %4
+      :
+      : "memory", "cc", "v0", "v1", "v2", "v30");
+}
+
+void MergeAR64Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       const uint16_t* src_a,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width) {
+  int shift = 16 - depth;
+  int mask = (1 << depth) - 1;
+  asm volatile(
+
+      "dup         v30.8h, %w7                   \n"
+      "dup         v31.8h, %w6                   \n"
+      "1:                                        \n"
+      "ldr         q2, [%0], #16                 \n"  // R
+      "ldr         q1, [%1], #16                 \n"  // G
+      "ldr         q0, [%2], #16                 \n"  // B
+      "ldr         q3, [%3], #16                 \n"  // A
+      "umin        v2.8h, v2.8h, v30.8h          \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "umin        v1.8h, v1.8h, v30.8h          \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
+      "umin        v0.8h, v0.8h, v30.8h          \n"
+      "prfm        pldl1keep, [%2, 448]          \n"
+      "umin        v3.8h, v3.8h, v30.8h          \n"
+      "prfm        pldl1keep, [%3, 448]          \n"
+      "ushl        v2.8h, v2.8h, v31.8h          \n"
+      "ushl        v1.8h, v1.8h, v31.8h          \n"
+      "ushl        v0.8h, v0.8h, v31.8h          \n"
+      "ushl        v3.8h, v3.8h, v31.8h          \n"
+      "subs        %w5, %w5, #8                  \n"
+      "st4         {v0.8h, v1.8h, v2.8h, v3.8h}, [%4], #64 \n"
+      "b.gt        1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(src_a),     // %3
+        "+r"(dst_ar64),  // %4
+        "+r"(width)      // %5
+      : "r"(shift),      // %6
+        "r"(mask)        // %7
+      : "memory", "cc", "v0", "v1", "v2", "v3", "v31");
+}
+
+void MergeXR64Row_NEON(const uint16_t* src_r,
+                       const uint16_t* src_g,
+                       const uint16_t* src_b,
+                       uint16_t* dst_ar64,
+                       int depth,
+                       int width) {
+  int shift = 16 - depth;
+  int mask = (1 << depth) - 1;
+  asm volatile(
+
+      "movi        v3.16b, #0xff                 \n"  // A (0xffff)
+      "dup         v30.8h, %w6                   \n"
+      "dup         v31.8h, %w5                   \n"
+
+      "1:                                        \n"
+      "ldr         q2, [%0], #16                 \n"  // R
+      "ldr         q1, [%1], #16                 \n"  // G
+      "ldr         q0, [%2], #16                 \n"  // B
+      "umin        v2.8h, v2.8h, v30.8h          \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "umin        v1.8h, v1.8h, v30.8h          \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
+      "umin        v0.8h, v0.8h, v30.8h          \n"
+      "prfm        pldl1keep, [%2, 448]          \n"
+      "ushl        v2.8h, v2.8h, v31.8h          \n"
+      "ushl        v1.8h, v1.8h, v31.8h          \n"
+      "ushl        v0.8h, v0.8h, v31.8h          \n"
+      "subs        %w4, %w4, #8                  \n"
+      "st4         {v0.8h, v1.8h, v2.8h, v3.8h}, [%3], #64 \n"
+      "b.gt        1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_ar64),  // %3
+        "+r"(width)      // %4
+      : "r"(shift),      // %5
+        "r"(mask)        // %6
+      : "memory", "cc", "v0", "v1", "v2", "v3", "v31");
+}
+
+void MergeARGB16To8Row_NEON(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            const uint16_t* src_a,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width) {
+  int shift = 8 - depth;
+  asm volatile(
+
+      "dup         v31.8h, %w6                   \n"
+      "1:                                        \n"
+      "ldr         q2, [%0], #16                 \n"  // R
+      "ldr         q1, [%1], #16                 \n"  // G
+      "ldr         q0, [%2], #16                 \n"  // B
+      "ldr         q3, [%3], #16                 \n"  // A
+      "ushl        v2.8h, v2.8h, v31.8h          \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "ushl        v1.8h, v1.8h, v31.8h          \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
+      "ushl        v0.8h, v0.8h, v31.8h          \n"
+      "prfm        pldl1keep, [%2, 448]          \n"
+      "ushl        v3.8h, v3.8h, v31.8h          \n"
+      "prfm        pldl1keep, [%3, 448]          \n"
+      "uqxtn       v2.8b, v2.8h                  \n"
+      "uqxtn       v1.8b, v1.8h                  \n"
+      "uqxtn       v0.8b, v0.8h                  \n"
+      "uqxtn       v3.8b, v3.8h                  \n"
+      "subs        %w5, %w5, #8                  \n"
+      "st4         {v0.8b, v1.8b, v2.8b, v3.8b}, [%4], #32 \n"
+      "b.gt        1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(src_a),     // %3
+        "+r"(dst_argb),  // %4
+        "+r"(width)      // %5
+      : "r"(shift)       // %6
+      : "memory", "cc", "v0", "v1", "v2", "v3", "v31");
+}
+
+void MergeXRGB16To8Row_NEON(const uint16_t* src_r,
+                            const uint16_t* src_g,
+                            const uint16_t* src_b,
+                            uint8_t* dst_argb,
+                            int depth,
+                            int width) {
+  int shift = 8 - depth;
+  asm volatile(
+
+      "dup         v31.8h, %w5                   \n"
+      "movi        v3.8b, #0xff                  \n"  // A (0xff)
+      "1:                                        \n"
+      "ldr         q2, [%0], #16                 \n"  // R
+      "ldr         q1, [%1], #16                 \n"  // G
+      "ldr         q0, [%2], #16                 \n"  // B
+      "ushl        v2.8h, v2.8h, v31.8h          \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "ushl        v1.8h, v1.8h, v31.8h          \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
+      "ushl        v0.8h, v0.8h, v31.8h          \n"
+      "prfm        pldl1keep, [%2, 448]          \n"
+      "uqxtn       v2.8b, v2.8h                  \n"
+      "uqxtn       v1.8b, v1.8h                  \n"
+      "uqxtn       v0.8b, v0.8h                  \n"
+      "subs        %w4, %w4, #8                  \n"
+      "st4         {v0.8b, v1.8b, v2.8b, v3.8b}, [%3], #32 \n"
+      "b.gt        1b                            \n"
+      : "+r"(src_r),     // %0
+        "+r"(src_g),     // %1
+        "+r"(src_b),     // %2
+        "+r"(dst_argb),  // %3
+        "+r"(width)      // %4
+      : "r"(shift)       // %5
+      : "memory", "cc", "v0", "v1", "v2", "v3", "v31");
+}
+
 // Copy multiple of 32.
 void CopyRow_NEON(const uint8_t* src, uint8_t* dst, int width) {
   asm volatile(
@@ -1072,10 +1196,10 @@ void RAWToARGBRow_NEON(const uint8_t* src_raw, uint8_t* dst_argb, int width) {
       "movi        v5.8b, #255                   \n"  // Alpha
       "1:                                        \n"
       "ld3         {v0.8b,v1.8b,v2.8b}, [%0], #24 \n"  // read r g b
+      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
+      "orr         v3.8b, v1.8b, v1.8b           \n"   // move g
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      "orr         v3.8b, v1.8b, v1.8b           \n"  // move g
-      "orr         v4.8b, v0.8b, v0.8b           \n"  // move r
+      "orr         v4.8b, v0.8b, v0.8b           \n"         // move r
       "st4         {v2.8b,v3.8b,v4.8b,v5.8b}, [%1], #32 \n"  // store b g r a
       "b.gt        1b                            \n"
       : "+r"(src_raw),   // %0
@@ -1091,10 +1215,10 @@ void RAWToRGBARow_NEON(const uint8_t* src_raw, uint8_t* dst_rgba, int width) {
       "movi        v0.8b, #255                   \n"  // Alpha
       "1:                                        \n"
       "ld3         {v3.8b,v4.8b,v5.8b}, [%0], #24 \n"  // read r g b
+      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
+      "orr         v2.8b, v4.8b, v4.8b           \n"   // move g
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      "orr         v2.8b, v4.8b, v4.8b           \n"  // move g
-      "orr         v1.8b, v5.8b, v5.8b           \n"  // move r
+      "orr         v1.8b, v5.8b, v5.8b           \n"         // move r
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%1], #32 \n"  // store a b g r
       "b.gt        1b                            \n"
       : "+r"(src_raw),   // %0
@@ -1109,9 +1233,9 @@ void RAWToRGB24Row_NEON(const uint8_t* src_raw, uint8_t* dst_rgb24, int width) {
   asm volatile(
       "1:                                        \n"
       "ld3         {v0.8b,v1.8b,v2.8b}, [%0], #24 \n"  // read r g b
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
       "orr         v3.8b, v1.8b, v1.8b           \n"   // move g
+      "prfm        pldl1keep, [%0, 448]          \n"
       "orr         v4.8b, v0.8b, v0.8b           \n"   // move r
       "st3         {v2.8b,v3.8b,v4.8b}, [%1], #24 \n"  // store b g r
       "b.gt        1b                            \n"
@@ -1143,9 +1267,8 @@ void RGB565ToARGBRow_NEON(const uint8_t* src_rgb565,
       "movi        v3.8b, #255                   \n"  // Alpha
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 RGB565 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      RGB565TOARGB
+      "prfm        pldl1keep, [%0, 448]          \n" RGB565TOARGB
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%1], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
       : "+r"(src_rgb565),  // %0
@@ -1233,9 +1356,8 @@ void ARGB4444ToARGBRow_NEON(const uint8_t* src_argb4444,
   asm volatile(
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 ARGB4444 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      ARGB4444TOARGB
+      "prfm        pldl1keep, [%0, 448]          \n" ARGB4444TOARGB
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%1], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
       : "+r"(src_argb4444),  // %0
@@ -1251,17 +1373,16 @@ void ARGBToRGB24Row_NEON(const uint8_t* src_argb,
                          int width) {
   asm volatile(
       "1:                                        \n"
-      "ld4         {v1.8b,v2.8b,v3.8b,v4.8b}, [%0], #32 \n"  // load 8 ARGB
+      "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64\n"  // load 16 ARGB
+      "subs        %w2, %w2, #16                 \n"     // 16 pixels per loop.
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
-      "st3         {v1.8b,v2.8b,v3.8b}, [%1], #24 \n"  // store 8 pixels of
-                                                       // RGB24
+      "st3         {v0.16b,v1.16b,v2.16b}, [%1], #48\n"  // store 8 RGB24
       "b.gt        1b                            \n"
       : "+r"(src_argb),   // %0
         "+r"(dst_rgb24),  // %1
         "+r"(width)       // %2
       :
-      : "cc", "memory", "v1", "v2", "v3", "v4"  // Clobber List
+      : "cc", "memory", "v0", "v1", "v2", "v3"  // Clobber List
   );
 }
 
@@ -1269,9 +1390,9 @@ void ARGBToRAWRow_NEON(const uint8_t* src_argb, uint8_t* dst_raw, int width) {
   asm volatile(
       "1:                                        \n"
       "ld4         {v1.8b,v2.8b,v3.8b,v4.8b}, [%0], #32 \n"  // load b g r a
+      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
+      "orr         v4.8b, v2.8b, v2.8b           \n"  // mov g
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
-      "orr         v4.8b, v2.8b, v2.8b           \n"   // mov g
       "orr         v5.8b, v1.8b, v1.8b           \n"   // mov b
       "st3         {v3.8b,v4.8b,v5.8b}, [%1], #24 \n"  // store r g b
       "b.gt        1b                            \n"
@@ -1287,8 +1408,8 @@ void YUY2ToYRow_NEON(const uint8_t* src_yuy2, uint8_t* dst_y, int width) {
   asm volatile(
       "1:                                        \n"
       "ld2         {v0.16b,v1.16b}, [%0], #32    \n"  // load 16 pixels of YUY2.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #16                 \n"  // 16 processed per loop.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v0.16b}, [%1], #16           \n"  // store 16 pixels of Y.
       "b.gt        1b                            \n"
       : "+r"(src_yuy2),  // %0
@@ -1303,8 +1424,8 @@ void UYVYToYRow_NEON(const uint8_t* src_uyvy, uint8_t* dst_y, int width) {
   asm volatile(
       "1:                                        \n"
       "ld2         {v0.16b,v1.16b}, [%0], #32    \n"  // load 16 pixels of UYVY.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #16                 \n"  // 16 processed per loop.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v1.16b}, [%1], #16           \n"  // store 16 pixels of Y.
       "b.gt        1b                            \n"
       : "+r"(src_uyvy),  // %0
@@ -1322,8 +1443,8 @@ void YUY2ToUV422Row_NEON(const uint8_t* src_yuy2,
   asm volatile(
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 16 YUY2
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w3, %w3, #16                 \n"  // 16 pixels = 8 UVs.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v1.8b}, [%1], #8             \n"  // store 8 U.
       "st1         {v3.8b}, [%2], #8             \n"  // store 8 V.
       "b.gt        1b                            \n"
@@ -1343,8 +1464,8 @@ void UYVYToUV422Row_NEON(const uint8_t* src_uyvy,
   asm volatile(
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 16 UYVY
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w3, %w3, #16                 \n"  // 16 pixels = 8 UVs.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v0.8b}, [%1], #8             \n"  // store 8 U.
       "st1         {v2.8b}, [%2], #8             \n"  // store 8 V.
       "b.gt        1b                            \n"
@@ -1366,10 +1487,10 @@ void YUY2ToUVRow_NEON(const uint8_t* src_yuy2,
   asm volatile(
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 16 pixels
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w4, %w4, #16                 \n"  // 16 pixels = 8 UVs.
       "ld4         {v4.8b,v5.8b,v6.8b,v7.8b}, [%1], #32 \n"  // load next row
       "urhadd      v1.8b, v1.8b, v5.8b           \n"  // average rows of U
+      "prfm        pldl1keep, [%0, 448]          \n"
       "urhadd      v3.8b, v3.8b, v7.8b           \n"  // average rows of V
       "st1         {v1.8b}, [%2], #8             \n"  // store 8 U.
       "st1         {v3.8b}, [%3], #8             \n"  // store 8 V.
@@ -1394,10 +1515,10 @@ void UYVYToUVRow_NEON(const uint8_t* src_uyvy,
   asm volatile(
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 16 pixels
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w4, %w4, #16                 \n"  // 16 pixels = 8 UVs.
       "ld4         {v4.8b,v5.8b,v6.8b,v7.8b}, [%1], #32 \n"  // load next row
       "urhadd      v0.8b, v0.8b, v4.8b           \n"  // average rows of U
+      "prfm        pldl1keep, [%0, 448]          \n"
       "urhadd      v2.8b, v2.8b, v6.8b           \n"  // average rows of V
       "st1         {v0.8b}, [%2], #8             \n"  // store 8 U.
       "st1         {v2.8b}, [%3], #8             \n"  // store 8 V.
@@ -1422,8 +1543,8 @@ void ARGBShuffleRow_NEON(const uint8_t* src_argb,
       "ld1         {v2.16b}, [%3]                \n"  // shuffler
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 4 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #4                  \n"  // 4 processed per loop
+      "prfm        pldl1keep, [%0, 448]          \n"
       "tbl         v1.16b, {v0.16b}, v2.16b      \n"  // look up 4 pixels
       "st1         {v1.16b}, [%1], #16           \n"  // store 4.
       "b.gt        1b                            \n"
@@ -1443,11 +1564,11 @@ void I422ToYUY2Row_NEON(const uint8_t* src_y,
   asm volatile(
       "1:                                        \n"
       "ld2         {v0.8b, v1.8b}, [%0], #16     \n"  // load 16 Ys
-      "prfm        pldl1keep, [%0, 448]          \n"
+      "subs        %w4, %w4, #16                 \n"  // 16 pixels
       "orr         v2.8b, v1.8b, v1.8b           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "ld1         {v1.8b}, [%1], #8             \n"         // load 8 Us
       "ld1         {v3.8b}, [%2], #8             \n"         // load 8 Vs
-      "subs        %w4, %w4, #16                 \n"         // 16 pixels
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%3], #32 \n"  // Store 16 pixels.
       "b.gt        1b                            \n"
       : "+r"(src_y),     // %0
@@ -1467,8 +1588,8 @@ void I422ToUYVYRow_NEON(const uint8_t* src_y,
   asm volatile(
       "1:                                        \n"
       "ld2         {v1.8b,v2.8b}, [%0], #16      \n"  // load 16 Ys
-      "prfm        pldl1keep, [%0, 448]          \n"
       "orr         v3.8b, v2.8b, v2.8b           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "ld1         {v0.8b}, [%1], #8             \n"         // load 8 Us
       "ld1         {v2.8b}, [%2], #8             \n"         // load 8 Vs
       "subs        %w4, %w4, #16                 \n"         // 16 pixels
@@ -1488,18 +1609,17 @@ void ARGBToRGB565Row_NEON(const uint8_t* src_argb,
                           int width) {
   asm volatile(
       "1:                                        \n"
-      "ld4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%0], #32 \n"  // load 8
+      "ld4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%0], #32 \n"  // load 8
                                                                  // pixels
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      ARGBTORGB565
-      "st1         {v0.16b}, [%1], #16           \n"  // store 8 pixels RGB565.
+      "prfm        pldl1keep, [%0, 448]          \n" ARGBTORGB565
+      "st1         {v18.16b}, [%1], #16          \n"  // store 8 pixels RGB565.
       "b.gt        1b                            \n"
       : "+r"(src_argb),    // %0
         "+r"(dst_rgb565),  // %1
         "+r"(width)        // %2
       :
-      : "cc", "memory", "v0", "v20", "v21", "v22", "v23");
+      : "cc", "memory", "v16", "v17", "v18", "v19");
 }
 
 void ARGBToRGB565DitherRow_NEON(const uint8_t* src_argb,
@@ -1509,20 +1629,20 @@ void ARGBToRGB565DitherRow_NEON(const uint8_t* src_argb,
   asm volatile(
       "dup         v1.4s, %w2                    \n"  // dither4
       "1:                                        \n"
-      "ld4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%1], #32 \n"  // load 8
+      "ld4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%1], #32 \n"  // load 8
                                                                  // pixels
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
-      "uqadd       v20.8b, v20.8b, v1.8b         \n"
-      "uqadd       v21.8b, v21.8b, v1.8b         \n"
-      "uqadd       v22.8b, v22.8b, v1.8b         \n" ARGBTORGB565
-      "st1         {v0.16b}, [%0], #16           \n"  // store 8 pixels RGB565.
+      "uqadd       v16.8b, v16.8b, v1.8b         \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "uqadd       v17.8b, v17.8b, v1.8b         \n"
+      "uqadd       v18.8b, v18.8b, v1.8b         \n" ARGBTORGB565
+      "st1         {v18.16b}, [%0], #16          \n"  // store 8 pixels RGB565.
       "b.gt        1b                            \n"
       : "+r"(dst_rgb)   // %0
       : "r"(src_argb),  // %1
         "r"(dither4),   // %2
         "r"(width)      // %3
-      : "cc", "memory", "v0", "v1", "v20", "v21", "v22", "v23");
+      : "cc", "memory", "v1", "v16", "v17", "v18", "v19");
 }
 
 void ARGBToARGB1555Row_NEON(const uint8_t* src_argb,
@@ -1530,39 +1650,132 @@ void ARGBToARGB1555Row_NEON(const uint8_t* src_argb,
                             int width) {
   asm volatile(
       "1:                                        \n"
-      "ld4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%0], #32 \n"  // load 8
+      "ld4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%0], #32 \n"  // load 8
                                                                  // pixels
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      ARGBTOARGB1555
+      "prfm        pldl1keep, [%0, 448]          \n" ARGBTOARGB1555
       "st1         {v0.16b}, [%1], #16           \n"  // store 8 pixels
       "b.gt        1b                            \n"
       : "+r"(src_argb),      // %0
         "+r"(dst_argb1555),  // %1
         "+r"(width)          // %2
       :
-      : "cc", "memory", "v0", "v20", "v21", "v22", "v23");
+      : "cc", "memory", "v0", "v16", "v17", "v18", "v19");
 }
 
 void ARGBToARGB4444Row_NEON(const uint8_t* src_argb,
                             uint8_t* dst_argb4444,
                             int width) {
   asm volatile(
-      "movi        v4.16b, #0x0f                 \n"  // bits to clear with
+      "movi        v23.16b, #0x0f                \n"  // bits to clear with
                                                       // vbic.
       "1:                                        \n"
-      "ld4         {v20.8b,v21.8b,v22.8b,v23.8b}, [%0], #32 \n"  // load 8
+      "ld4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%0], #32 \n"  // load 8
                                                                  // pixels
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      ARGBTOARGB4444
+      "prfm        pldl1keep, [%0, 448]          \n" ARGBTOARGB4444
       "st1         {v0.16b}, [%1], #16           \n"  // store 8 pixels
       "b.gt        1b                            \n"
       : "+r"(src_argb),      // %0
         "+r"(dst_argb4444),  // %1
         "+r"(width)          // %2
       :
-      : "cc", "memory", "v0", "v1", "v4", "v20", "v21", "v22", "v23");
+      : "cc", "memory", "v0", "v1", "v16", "v17", "v18", "v19", "v23");
+}
+
+
+void ARGBToAR64Row_NEON(const uint8_t* src_argb,
+                        uint16_t* dst_ar64,
+                        int width) {
+  asm volatile(
+      "1:                                        \n"
+      "ldp         q0, q2, [%0], #32             \n"  // load 8 pixels
+      "mov         v1.16b, v0.16b                \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "mov         v3.16b, v2.16b                \n"
+      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
+      "st2         {v0.16b, v1.16b}, [%1], #32   \n"  // store 4 pixels
+      "st2         {v2.16b, v3.16b}, [%1], #32   \n"  // store 4 pixels
+      "b.gt        1b                            \n"
+      : "+r"(src_argb),  // %0
+        "+r"(dst_ar64),  // %1
+        "+r"(width)      // %2
+      :
+      : "cc", "memory", "v0", "v1", "v2", "v3");
+}
+
+static const uvec8 kShuffleARGBToABGR = {2,  1, 0, 3,  6,  5,  4,  7,
+                                         10, 9, 8, 11, 14, 13, 12, 15};
+
+void ARGBToAB64Row_NEON(const uint8_t* src_argb,
+                        uint16_t* dst_ab64,
+                        int width) {
+  asm volatile(
+      "ldr         q4, [%3]                      \n"  // shuffler
+      "1:                                        \n"
+      "ldp         q0, q2, [%0], #32             \n"  // load 8 pixels
+      "tbl         v0.16b, {v0.16b}, v4.16b      \n"
+      "tbl         v2.16b, {v2.16b}, v4.16b      \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "mov         v1.16b, v0.16b                \n"
+      "mov         v3.16b, v2.16b                \n"
+      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
+      "st2         {v0.16b, v1.16b}, [%1], #32   \n"  // store 4 pixels
+      "st2         {v2.16b, v3.16b}, [%1], #32   \n"  // store 4 pixels
+      "b.gt        1b                            \n"
+      : "+r"(src_argb),           // %0
+        "+r"(dst_ab64),           // %1
+        "+r"(width)               // %2
+      : "r"(&kShuffleARGBToABGR)  // %3
+      : "cc", "memory", "v0", "v1", "v2", "v3", "v4");
+}
+
+static const uvec8 kShuffleAR64ToARGB = {1,  3,  5,  7,  9,  11, 13, 15,
+                                         17, 19, 21, 23, 25, 27, 29, 31};
+
+void AR64ToARGBRow_NEON(const uint16_t* src_ar64,
+                        uint8_t* dst_argb,
+                        int width) {
+  asm volatile(
+      "ldr         q4, [%3]                      \n"  // shuffler
+      "1:                                        \n"
+      "ldp         q0, q1, [%0], #32             \n"  // load 4 pixels
+      "ldp         q2, q3, [%0], #32             \n"  // load 4 pixels
+      "tbl         v0.16b, {v0.16b, v1.16b}, v4.16b \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "tbl         v2.16b, {v2.16b, v3.16b}, v4.16b \n"
+      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
+      "stp         q0, q2, [%1], #32             \n"  // store 8 pixels
+      "b.gt        1b                            \n"
+      : "+r"(src_ar64),           // %0
+        "+r"(dst_argb),           // %1
+        "+r"(width)               // %2
+      : "r"(&kShuffleAR64ToARGB)  // %3
+      : "cc", "memory", "v0", "v1", "v2", "v3", "v4");
+}
+
+static const uvec8 kShuffleAB64ToARGB = {5,  3,  1,  7,  13, 11, 9,  15,
+                                         21, 19, 17, 23, 29, 27, 25, 31};
+
+void AB64ToARGBRow_NEON(const uint16_t* src_ab64,
+                        uint8_t* dst_argb,
+                        int width) {
+  asm volatile(
+      "ldr         q4, [%3]                      \n"  // shuffler
+      "1:                                        \n"
+      "ldp         q0, q1, [%0], #32             \n"  // load 4 pixels
+      "ldp         q2, q3, [%0], #32             \n"  // load 4 pixels
+      "tbl         v0.16b, {v0.16b, v1.16b}, v4.16b \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "tbl         v2.16b, {v2.16b, v3.16b}, v4.16b \n"
+      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
+      "stp         q0, q2, [%1], #32             \n"  // store 8 pixels
+      "b.gt        1b                            \n"
+      : "+r"(src_ab64),           // %0
+        "+r"(dst_argb),           // %1
+        "+r"(width)               // %2
+      : "r"(&kShuffleAB64ToARGB)  // %3
+      : "cc", "memory", "v0", "v1", "v2", "v3", "v4");
 }
 
 void ARGBToYRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
@@ -1573,9 +1786,9 @@ void ARGBToYRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
       "movi        v7.8b, #16                    \n"  // Add 16 constant
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v3.8h, v0.8b, v4.8b           \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v3.8h, v1.8b, v5.8b           \n"  // G
       "umlal       v3.8h, v2.8b, v6.8b           \n"  // R
       "uqrshrn     v0.8b, v3.8h, #8              \n"  // 16 bit to 8 bit Y
@@ -1614,9 +1827,9 @@ void ARGBToYJRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
       "movi        v6.8b, #77                    \n"  // R * 0.2990 coefficient
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v3.8h, v0.8b, v4.8b           \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v3.8h, v1.8b, v5.8b           \n"  // G
       "umlal       v3.8h, v2.8b, v6.8b           \n"  // R
       "uqrshrn     v0.8b, v3.8h, #8              \n"  // 16 bit to 8 bit Y
@@ -1629,22 +1842,22 @@ void ARGBToYJRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
       : "cc", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6");
 }
 
-void RGBAToYJRow_NEON(const uint8_t* src_argb, uint8_t* dst_y, int width) {
+void RGBAToYJRow_NEON(const uint8_t* src_rgba, uint8_t* dst_y, int width) {
   asm volatile(
       "movi        v4.8b, #29                    \n"  // B * 0.1140 coefficient
       "movi        v5.8b, #150                   \n"  // G * 0.5870 coefficient
       "movi        v6.8b, #77                    \n"  // R * 0.2990 coefficient
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 RGBA
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v0.8h, v1.8b, v4.8b           \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v0.8h, v2.8b, v5.8b           \n"  // G
       "umlal       v0.8h, v3.8b, v6.8b           \n"  // R
       "uqrshrn     v3.8b, v0.8h, #8              \n"  // 16 bit to 8 bit Y
       "st1         {v3.8b}, [%1], #8             \n"  // store 8 pixels Y.
       "b.gt        1b                            \n"
-      : "+r"(src_argb),  // %0
+      : "+r"(src_rgba),  // %0
         "+r"(dst_y),     // %1
         "+r"(width)      // %2
       :
@@ -1666,9 +1879,9 @@ void ARGBToUV444Row_NEON(const uint8_t* src_argb,
       "movi        v29.16b,#0x80                 \n"  // 128.5
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
       "umull       v4.8h, v0.8b, v24.8b          \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlsl       v4.8h, v1.8b, v25.8b          \n"  // G
       "umlsl       v4.8h, v2.8b, v26.8b          \n"  // R
       "add         v4.8h, v4.8h, v29.8h          \n"  // +128 -> unsigned
@@ -1729,14 +1942,14 @@ void ARGBToUVRow_NEON(const uint8_t* src_argb,
     RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v0.8h, v0.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v1.8h, v1.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uaddlp      v2.8h, v2.16b                 \n"  // R 16 bytes -> 8 shorts.
 
       "ld4         {v4.16b,v5.16b,v6.16b,v7.16b}, [%1], #64 \n"  // load next 16
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v0.8h, v4.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v1.8h, v5.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uadalp      v2.8h, v6.16b                 \n"  // R 16 bytes -> 8 shorts.
 
@@ -1775,13 +1988,13 @@ void ARGBToUVJRow_NEON(const uint8_t* src_argb,
       "movi        v25.16b, #0x80                \n"  // 128.5 (0x8080 in 16-bit)
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v0.8h, v0.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v1.8h, v1.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uaddlp      v2.8h, v2.16b                 \n"  // R 16 bytes -> 8 shorts.
       "ld4         {v4.16b,v5.16b,v6.16b,v7.16b}, [%1], #64 \n"  // load next 16
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v0.8h, v4.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v1.8h, v5.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uadalp      v2.8h, v6.16b                 \n"  // R 16 bytes -> 8 shorts.
 
@@ -1815,13 +2028,13 @@ void BGRAToUVRow_NEON(const uint8_t* src_bgra,
     RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v0.8h, v3.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v3.8h, v2.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uaddlp      v2.8h, v1.16b                 \n"  // R 16 bytes -> 8 shorts.
       "ld4         {v4.16b,v5.16b,v6.16b,v7.16b}, [%1], #64 \n"  // load 16 more
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v0.8h, v7.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v3.8h, v6.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uadalp      v2.8h, v5.16b                 \n"  // R 16 bytes -> 8 shorts.
 
@@ -1855,13 +2068,13 @@ void ABGRToUVRow_NEON(const uint8_t* src_abgr,
     RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v3.8h, v2.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v2.8h, v1.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uaddlp      v1.8h, v0.16b                 \n"  // R 16 bytes -> 8 shorts.
       "ld4         {v4.16b,v5.16b,v6.16b,v7.16b}, [%1], #64 \n"  // load 16 more.
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v3.8h, v6.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v2.8h, v5.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uadalp      v1.8h, v4.16b                 \n"  // R 16 bytes -> 8 shorts.
 
@@ -1895,13 +2108,13 @@ void RGBAToUVRow_NEON(const uint8_t* src_rgba,
     RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v0.8h, v1.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v1.8h, v2.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uaddlp      v2.8h, v3.16b                 \n"  // R 16 bytes -> 8 shorts.
       "ld4         {v4.16b,v5.16b,v6.16b,v7.16b}, [%1], #64 \n"  // load 16 more.
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v0.8h, v5.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v1.8h, v6.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uadalp      v2.8h, v7.16b                 \n"  // R 16 bytes -> 8 shorts.
 
@@ -1935,13 +2148,13 @@ void RGB24ToUVRow_NEON(const uint8_t* src_rgb24,
     RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld3         {v0.16b,v1.16b,v2.16b}, [%0], #48 \n"  // load 16 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v0.8h, v0.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v1.8h, v1.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uaddlp      v2.8h, v2.16b                 \n"  // R 16 bytes -> 8 shorts.
       "ld3         {v4.16b,v5.16b,v6.16b}, [%1], #48 \n"  // load 16 more.
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v0.8h, v4.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v1.8h, v5.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uadalp      v2.8h, v6.16b                 \n"  // R 16 bytes -> 8 shorts.
 
@@ -1975,13 +2188,13 @@ void RAWToUVRow_NEON(const uint8_t* src_raw,
     RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld3         {v0.16b,v1.16b,v2.16b}, [%0], #48 \n"  // load 8 RAW pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v2.8h, v2.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v1.8h, v1.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uaddlp      v0.8h, v0.16b                 \n"  // R 16 bytes -> 8 shorts.
       "ld3         {v4.16b,v5.16b,v6.16b}, [%1], #48 \n"  // load 8 more RAW pixels
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v2.8h, v6.16b                 \n"  // B 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v1.8h, v5.16b                 \n"  // G 16 bytes -> 8 shorts.
       "uadalp      v0.8h, v4.16b                 \n"  // R 16 bytes -> 8 shorts.
 
@@ -2016,9 +2229,9 @@ void RGB565ToUVRow_NEON(const uint8_t* src_rgb565,
       RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 RGB565 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       RGB565TOARGB
       "uaddlp      v16.4h, v0.8b                 \n"  // B 8 bytes -> 4 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v17.4h, v1.8b                 \n"  // G 8 bytes -> 4 shorts.
       "uaddlp      v18.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
       "ld1         {v0.16b}, [%0], #16           \n"  // next 8 RGB565 pixels.
@@ -2028,9 +2241,9 @@ void RGB565ToUVRow_NEON(const uint8_t* src_rgb565,
       "uaddlp      v28.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
 
       "ld1         {v0.16b}, [%1], #16           \n"  // load 8 RGB565 pixels.
-      "prfm        pldl1keep, [%1, 448]          \n"
       RGB565TOARGB
       "uadalp      v16.4h, v0.8b                 \n"  // B 8 bytes -> 4 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v17.4h, v1.8b                 \n"  // G 8 bytes -> 4 shorts.
       "uadalp      v18.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
       "ld1         {v0.16b}, [%1], #16           \n"  // next 8 RGB565 pixels.
@@ -2074,9 +2287,9 @@ void ARGB1555ToUVRow_NEON(const uint8_t* src_argb1555,
       RGBTOUV_SETUP_REG
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 ARGB1555 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       RGB555TOARGB
       "uaddlp      v16.4h, v0.8b                 \n"  // B 8 bytes -> 4 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v17.4h, v1.8b                 \n"  // G 8 bytes -> 4 shorts.
       "uaddlp      v18.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
       "ld1         {v0.16b}, [%0], #16           \n"  // next 8 ARGB1555 pixels.
@@ -2086,9 +2299,9 @@ void ARGB1555ToUVRow_NEON(const uint8_t* src_argb1555,
       "uaddlp      v28.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
 
       "ld1         {v0.16b}, [%1], #16           \n"  // load 8 ARGB1555 pixels.
-      "prfm        pldl1keep, [%1, 448]          \n"
       RGB555TOARGB
       "uadalp      v16.4h, v0.8b                 \n"  // B 8 bytes -> 4 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v17.4h, v1.8b                 \n"  // G 8 bytes -> 4 shorts.
       "uadalp      v18.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
       "ld1         {v0.16b}, [%1], #16           \n"  // next 8 ARGB1555 pixels.
@@ -2132,9 +2345,9 @@ void ARGB4444ToUVRow_NEON(const uint8_t* src_argb4444,
       RGBTOUV_SETUP_REG  // sets v20-v25
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 ARGB4444 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       ARGB4444TOARGB
       "uaddlp      v16.4h, v0.8b                 \n"  // B 8 bytes -> 4 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v17.4h, v1.8b                 \n"  // G 8 bytes -> 4 shorts.
       "uaddlp      v18.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
       "ld1         {v0.16b}, [%0], #16           \n"  // next 8 ARGB4444 pixels.
@@ -2144,9 +2357,9 @@ void ARGB4444ToUVRow_NEON(const uint8_t* src_argb4444,
       "uaddlp      v28.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
 
       "ld1         {v0.16b}, [%1], #16           \n"  // load 8 ARGB4444 pixels.
-      "prfm        pldl1keep, [%1, 448]          \n"
       ARGB4444TOARGB
       "uadalp      v16.4h, v0.8b                 \n"  // B 8 bytes -> 4 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v17.4h, v1.8b                 \n"  // G 8 bytes -> 4 shorts.
       "uadalp      v18.4h, v2.8b                 \n"  // R 8 bytes -> 4 shorts.
       "ld1         {v0.16b}, [%1], #16           \n"  // next 8 ARGB4444 pixels.
@@ -2189,10 +2402,10 @@ void RGB565ToYRow_NEON(const uint8_t* src_rgb565, uint8_t* dst_y, int width) {
       "movi        v27.8b, #16                   \n"  // Add 16 constant
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 RGB565 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       RGB565TOARGB
       "umull       v3.8h, v0.8b, v24.8b          \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v3.8h, v1.8b, v25.8b          \n"  // G
       "umlal       v3.8h, v2.8b, v26.8b          \n"  // R
       "uqrshrn     v0.8b, v3.8h, #8              \n"  // 16 bit to 8 bit Y
@@ -2217,10 +2430,10 @@ void ARGB1555ToYRow_NEON(const uint8_t* src_argb1555,
       "movi        v7.8b, #16                    \n"  // Add 16 constant
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 ARGB1555 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       ARGB1555TOARGB
       "umull       v3.8h, v0.8b, v4.8b           \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v3.8h, v1.8b, v5.8b           \n"  // G
       "umlal       v3.8h, v2.8b, v6.8b           \n"  // R
       "uqrshrn     v0.8b, v3.8h, #8              \n"  // 16 bit to 8 bit Y
@@ -2244,10 +2457,10 @@ void ARGB4444ToYRow_NEON(const uint8_t* src_argb4444,
       "movi        v27.8b, #16                   \n"  // Add 16 constant
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 8 ARGB4444 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       ARGB4444TOARGB
       "umull       v3.8h, v0.8b, v24.8b          \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v3.8h, v1.8b, v25.8b          \n"  // G
       "umlal       v3.8h, v2.8b, v26.8b          \n"  // R
       "uqrshrn     v0.8b, v3.8h, #8              \n"  // 16 bit to 8 bit Y
@@ -2269,9 +2482,9 @@ void BGRAToYRow_NEON(const uint8_t* src_bgra, uint8_t* dst_y, int width) {
       "movi        v7.8b, #16                    \n"  // Add 16 constant
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v16.8h, v1.8b, v4.8b          \n"  // R
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v16.8h, v2.8b, v5.8b          \n"  // G
       "umlal       v16.8h, v3.8b, v6.8b          \n"  // B
       "uqrshrn     v0.8b, v16.8h, #8             \n"  // 16 bit to 8 bit Y
@@ -2293,9 +2506,9 @@ void ABGRToYRow_NEON(const uint8_t* src_abgr, uint8_t* dst_y, int width) {
       "movi        v7.8b, #16                    \n"  // Add 16 constant
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v16.8h, v0.8b, v4.8b          \n"  // R
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v16.8h, v1.8b, v5.8b          \n"  // G
       "umlal       v16.8h, v2.8b, v6.8b          \n"  // B
       "uqrshrn     v0.8b, v16.8h, #8             \n"  // 16 bit to 8 bit Y
@@ -2317,9 +2530,9 @@ void RGBAToYRow_NEON(const uint8_t* src_rgba, uint8_t* dst_y, int width) {
       "movi        v7.8b, #16                    \n"  // Add 16 constant
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 pixels.
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v16.8h, v1.8b, v4.8b          \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v16.8h, v2.8b, v5.8b          \n"  // G
       "umlal       v16.8h, v3.8b, v6.8b          \n"  // R
       "uqrshrn     v0.8b, v16.8h, #8             \n"  // 16 bit to 8 bit Y
@@ -2341,9 +2554,9 @@ void RGB24ToYRow_NEON(const uint8_t* src_rgb24, uint8_t* dst_y, int width) {
       "movi        v7.8b, #16                    \n"  // Add 16 constant
       "1:                                        \n"
       "ld3         {v0.8b,v1.8b,v2.8b}, [%0], #24 \n"  // load 8 pixels.
+      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
+      "umull       v16.8h, v0.8b, v4.8b          \n"   // B
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      "umull       v16.8h, v0.8b, v4.8b          \n"  // B
       "umlal       v16.8h, v1.8b, v5.8b          \n"  // G
       "umlal       v16.8h, v2.8b, v6.8b          \n"  // R
       "uqrshrn     v0.8b, v16.8h, #8             \n"  // 16 bit to 8 bit Y
@@ -2365,9 +2578,9 @@ void RAWToYRow_NEON(const uint8_t* src_raw, uint8_t* dst_y, int width) {
       "movi        v7.8b, #16                    \n"  // Add 16 constant
       "1:                                        \n"
       "ld3         {v0.8b,v1.8b,v2.8b}, [%0], #24 \n"  // load 8 pixels.
+      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
+      "umull       v16.8h, v0.8b, v4.8b          \n"   // B
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      "umull       v16.8h, v0.8b, v4.8b          \n"  // B
       "umlal       v16.8h, v1.8b, v5.8b          \n"  // G
       "umlal       v16.8h, v2.8b, v6.8b          \n"  // R
       "uqrshrn     v0.8b, v16.8h, #8             \n"  // 16 bit to 8 bit Y
@@ -2388,9 +2601,9 @@ void RGB24ToYJRow_NEON(const uint8_t* src_rgb24, uint8_t* dst_yj, int width) {
       "movi        v6.8b, #77                    \n"  // R * 0.2990 coefficient
       "1:                                        \n"
       "ld3         {v0.8b,v1.8b,v2.8b}, [%0], #24 \n"  // load 8 pixels.
+      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
+      "umull       v0.8h, v0.8b, v4.8b           \n"   // B
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      "umull       v0.8h, v0.8b, v4.8b           \n"  // B
       "umlal       v0.8h, v1.8b, v5.8b           \n"  // G
       "umlal       v0.8h, v2.8b, v6.8b           \n"  // R
       "uqrshrn     v0.8b, v0.8h, #8              \n"  // 16 bit to 8 bit Y
@@ -2410,9 +2623,9 @@ void RAWToYJRow_NEON(const uint8_t* src_raw, uint8_t* dst_yj, int width) {
       "movi        v4.8b, #77                    \n"  // R * 0.2990 coefficient
       "1:                                        \n"
       "ld3         {v0.8b,v1.8b,v2.8b}, [%0], #24 \n"  // load 8 pixels.
+      "subs        %w2, %w2, #8                  \n"   // 8 processed per loop.
+      "umull       v0.8h, v0.8b, v4.8b           \n"   // B
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
-      "umull       v0.8h, v0.8b, v4.8b           \n"  // B
       "umlal       v0.8h, v1.8b, v5.8b           \n"  // G
       "umlal       v0.8h, v2.8b, v6.8b           \n"  // R
       "uqrshrn     v0.8b, v0.8h, #8              \n"  // 16 bit to 8 bit Y
@@ -2446,11 +2659,11 @@ void InterpolateRow_NEON(uint8_t* dst_ptr,
       "1:                                        \n"
       "ld1         {v0.16b}, [%1], #16           \n"
       "ld1         {v1.16b}, [%2], #16           \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
-      "prfm        pldl1keep, [%2, 448]          \n"
       "subs        %w3, %w3, #16                 \n"
       "umull       v2.8h, v0.8b,  v4.8b          \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "umull2      v3.8h, v0.16b, v4.16b         \n"
+      "prfm        pldl1keep, [%2, 448]          \n"
       "umlal       v2.8h, v1.8b,  v5.8b          \n"
       "umlal2      v3.8h, v1.16b, v5.16b         \n"
       "rshrn       v0.8b,  v2.8h, #8             \n"
@@ -2463,10 +2676,10 @@ void InterpolateRow_NEON(uint8_t* dst_ptr,
       "50:                                       \n"
       "ld1         {v0.16b}, [%1], #16           \n"
       "ld1         {v1.16b}, [%2], #16           \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
-      "prfm        pldl1keep, [%2, 448]          \n"
       "subs        %w3, %w3, #16                 \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "urhadd      v0.16b, v0.16b, v1.16b        \n"
+      "prfm        pldl1keep, [%2, 448]          \n"
       "st1         {v0.16b}, [%0], #16           \n"
       "b.gt        50b                           \n"
       "b           99f                           \n"
@@ -2474,8 +2687,8 @@ void InterpolateRow_NEON(uint8_t* dst_ptr,
       // Blend 100 / 0 - Copy row unchanged.
       "100:                                      \n"
       "ld1         {v0.16b}, [%1], #16           \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #16                 \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "st1         {v0.16b}, [%0], #16           \n"
       "b.gt        100b                          \n"
 
@@ -2491,7 +2704,7 @@ void InterpolateRow_NEON(uint8_t* dst_ptr,
 }
 
 // dr * (256 - sa) / 256 + sr = dr - dr * sa / 256 + sr
-void ARGBBlendRow_NEON(const uint8_t* src_argb0,
+void ARGBBlendRow_NEON(const uint8_t* src_argb,
                        const uint8_t* src_argb1,
                        uint8_t* dst_argb,
                        int width) {
@@ -2502,11 +2715,11 @@ void ARGBBlendRow_NEON(const uint8_t* src_argb0,
       "8:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB0
       "ld4         {v4.8b,v5.8b,v6.8b,v7.8b}, [%1], #32 \n"  // load 8 ARGB1
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
       "umull       v16.8h, v4.8b, v3.8b          \n"  // db * a
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umull       v17.8h, v5.8b, v3.8b          \n"  // dg * a
+      "prfm        pldl1keep, [%1, 448]          \n"
       "umull       v18.8h, v6.8b, v3.8b          \n"  // dr * a
       "uqrshrn     v16.8b, v16.8h, #8            \n"  // db >>= 8
       "uqrshrn     v17.8b, v17.8h, #8            \n"  // dg >>= 8
@@ -2532,11 +2745,11 @@ void ARGBBlendRow_NEON(const uint8_t* src_argb0,
                                                            // ARGB0.
       "ld4         {v4.b,v5.b,v6.b,v7.b}[0], [%1], #4 \n"  // load 1 pixel
                                                            // ARGB1.
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #1                  \n"  // 1 processed per loop.
       "umull       v16.8h, v4.8b, v3.8b          \n"  // db * a
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umull       v17.8h, v5.8b, v3.8b          \n"  // dg * a
+      "prfm        pldl1keep, [%1, 448]          \n"
       "umull       v18.8h, v6.8b, v3.8b          \n"  // dr * a
       "uqrshrn     v16.8b, v16.8h, #8            \n"  // db >>= 8
       "uqrshrn     v17.8b, v17.8h, #8            \n"  // dg >>= 8
@@ -2553,7 +2766,7 @@ void ARGBBlendRow_NEON(const uint8_t* src_argb0,
 
       "99:                                       \n"
 
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -2570,14 +2783,14 @@ void ARGBAttenuateRow_NEON(const uint8_t* src_argb,
       // Attenuate 8 pixels.
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v4.8h, v0.8b, v3.8b           \n"  // b * a
-      "umull       v5.8h, v1.8b, v3.8b           \n"  // g * a
-      "umull       v6.8h, v2.8b, v3.8b           \n"  // r * a
-      "uqrshrn     v0.8b, v4.8h, #8              \n"  // b >>= 8
-      "uqrshrn     v1.8b, v5.8h, #8              \n"  // g >>= 8
-      "uqrshrn     v2.8b, v6.8h, #8              \n"  // r >>= 8
+      "prfm        pldl1keep, [%0, 448]          \n"
+      "umull       v5.8h, v1.8b, v3.8b           \n"         // g * a
+      "umull       v6.8h, v2.8b, v3.8b           \n"         // r * a
+      "uqrshrn     v0.8b, v4.8h, #8              \n"         // b >>= 8
+      "uqrshrn     v1.8b, v5.8h, #8              \n"         // g >>= 8
+      "uqrshrn     v2.8b, v6.8h, #8              \n"         // r >>= 8
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%1], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
       : "+r"(src_argb),  // %0
@@ -2603,9 +2816,9 @@ void ARGBQuantizeRow_NEON(uint8_t* dst_argb,
       // 8 pixel loop.
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0] \n"  // load 8  ARGB.
+      "subs        %w1, %w1, #8                  \n"    // 8 processed per loop.
+      "uxtl        v0.8h, v0.8b                  \n"    // b (0 .. 255)
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w1, %w1, #8                  \n"  // 8 processed per loop.
-      "uxtl        v0.8h, v0.8b                  \n"  // b (0 .. 255)
       "uxtl        v1.8h, v1.8b                  \n"
       "uxtl        v2.8h, v2.8b                  \n"
       "sqdmulh     v0.8h, v0.8h, v4.8h           \n"  // b * scale
@@ -2645,9 +2858,9 @@ void ARGBShadeRow_NEON(const uint8_t* src_argb,
       // 8 pixel loop.
       "1:                                        \n"
       "ld4         {v4.8b,v5.8b,v6.8b,v7.8b}, [%0], #32 \n"  // load 8 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "uxtl        v4.8h, v4.8b                  \n"  // b (0 .. 255)
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uxtl        v5.8h, v5.8b                  \n"
       "uxtl        v6.8h, v6.8b                  \n"
       "uxtl        v7.8h, v7.8b                  \n"
@@ -2678,9 +2891,9 @@ void ARGBGrayRow_NEON(const uint8_t* src_argb, uint8_t* dst_argb, int width) {
       "movi        v26.8b, #77                   \n"  // R * 0.2990 coefficient
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "umull       v4.8h, v0.8b, v24.8b          \n"  // B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umlal       v4.8h, v1.8b, v25.8b          \n"  // G
       "umlal       v4.8h, v2.8b, v26.8b          \n"  // R
       "uqrshrn     v0.8b, v4.8h, #8              \n"  // 16 bit to 8 bit B
@@ -2713,9 +2926,9 @@ void ARGBSepiaRow_NEON(uint8_t* dst_argb, int width) {
       "movi        v30.8b, #50                   \n"  // BR coefficient
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0] \n"  // load 8 ARGB pixels.
+      "subs        %w1, %w1, #8                  \n"    // 8 processed per loop.
+      "umull       v4.8h, v0.8b, v20.8b          \n"    // B to Sepia B
       "prfm        pldl1keep, [%0, 448]          \n"
-      "subs        %w1, %w1, #8                  \n"  // 8 processed per loop.
-      "umull       v4.8h, v0.8b, v20.8b          \n"  // B to Sepia B
       "umlal       v4.8h, v1.8b, v21.8b          \n"  // G
       "umlal       v4.8h, v2.8b, v22.8b          \n"  // R
       "umull       v5.8h, v0.8b, v24.8b          \n"  // B to Sepia G
@@ -2750,9 +2963,9 @@ void ARGBColorMatrixRow_NEON(const uint8_t* src_argb,
 
       "1:                                        \n"
       "ld4         {v16.8b,v17.8b,v18.8b,v19.8b}, [%0], #32 \n"  // load 8 ARGB
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop.
       "uxtl        v16.8h, v16.8b                \n"  // b (0 .. 255) 16 bit
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uxtl        v17.8h, v17.8b                \n"  // g
       "uxtl        v18.8h, v18.8b                \n"  // r
       "uxtl        v19.8h, v19.8b                \n"  // a
@@ -2800,7 +3013,7 @@ void ARGBColorMatrixRow_NEON(const uint8_t* src_argb,
 
 // TODO(fbarchard): fix vqshrun in ARGBMultiplyRow_NEON and reenable.
 // Multiply 2 rows of ARGB pixels together, 8 pixels at a time.
-void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
+void ARGBMultiplyRow_NEON(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -2809,11 +3022,11 @@ void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
       "ld4         {v4.8b,v5.8b,v6.8b,v7.8b}, [%1], #32 \n"  // load 8 more
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
       "umull       v0.8h, v0.8b, v4.8b           \n"  // multiply B
+      "prfm        pldl1keep, [%0, 448]          \n"
       "umull       v1.8h, v1.8b, v5.8b           \n"  // multiply G
+      "prfm        pldl1keep, [%1, 448]          \n"
       "umull       v2.8h, v2.8b, v6.8b           \n"  // multiply R
       "umull       v3.8h, v3.8b, v7.8b           \n"  // multiply A
       "rshrn       v0.8b, v0.8h, #8              \n"  // 16 bit to 8 bit B
@@ -2822,7 +3035,7 @@ void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
       "rshrn       v3.8b, v3.8h, #8              \n"  // 16 bit to 8 bit A
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%2], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -2831,7 +3044,7 @@ void ARGBMultiplyRow_NEON(const uint8_t* src_argb0,
 }
 
 // Add 2 rows of ARGB pixels together, 8 pixels at a time.
-void ARGBAddRow_NEON(const uint8_t* src_argb0,
+void ARGBAddRow_NEON(const uint8_t* src_argb,
                      const uint8_t* src_argb1,
                      uint8_t* dst_argb,
                      int width) {
@@ -2840,16 +3053,16 @@ void ARGBAddRow_NEON(const uint8_t* src_argb0,
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
       "ld4         {v4.8b,v5.8b,v6.8b,v7.8b}, [%1], #32 \n"  // load 8 more
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
       "uqadd       v0.8b, v0.8b, v4.8b           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uqadd       v1.8b, v1.8b, v5.8b           \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uqadd       v2.8b, v2.8b, v6.8b           \n"
       "uqadd       v3.8b, v3.8b, v7.8b           \n"
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%2], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -2858,7 +3071,7 @@ void ARGBAddRow_NEON(const uint8_t* src_argb0,
 }
 
 // Subtract 2 rows of ARGB pixels, 8 pixels at a time.
-void ARGBSubtractRow_NEON(const uint8_t* src_argb0,
+void ARGBSubtractRow_NEON(const uint8_t* src_argb,
                           const uint8_t* src_argb1,
                           uint8_t* dst_argb,
                           int width) {
@@ -2867,16 +3080,16 @@ void ARGBSubtractRow_NEON(const uint8_t* src_argb0,
       "1:                                        \n"
       "ld4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%0], #32 \n"  // load 8 ARGB
       "ld4         {v4.8b,v5.8b,v6.8b,v7.8b}, [%1], #32 \n"  // load 8 more
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
       "uqsub       v0.8b, v0.8b, v4.8b           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uqsub       v1.8b, v1.8b, v5.8b           \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uqsub       v2.8b, v2.8b, v6.8b           \n"
       "uqsub       v3.8b, v3.8b, v7.8b           \n"
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%2], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
-      : "+r"(src_argb0),  // %0
+      : "+r"(src_argb),   // %0
         "+r"(src_argb1),  // %1
         "+r"(dst_argb),   // %2
         "+r"(width)       // %3
@@ -2899,11 +3112,11 @@ void SobelRow_NEON(const uint8_t* src_sobelx,
       "1:                                        \n"
       "ld1         {v0.8b}, [%0], #8             \n"  // load 8 sobelx.
       "ld1         {v1.8b}, [%1], #8             \n"  // load 8 sobely.
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
       "uqadd       v0.8b, v0.8b, v1.8b           \n"  // add
+      "prfm        pldl1keep, [%0, 448]          \n"
       "orr         v1.8b, v0.8b, v0.8b           \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "orr         v2.8b, v0.8b, v0.8b           \n"
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%2], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
@@ -2925,10 +3138,10 @@ void SobelToPlaneRow_NEON(const uint8_t* src_sobelx,
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], #16           \n"  // load 16 sobelx.
       "ld1         {v1.16b}, [%1], #16           \n"  // load 16 sobely.
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #16                 \n"  // 16 processed per loop.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uqadd       v0.16b, v0.16b, v1.16b        \n"  // add
+      "prfm        pldl1keep, [%1, 448]          \n"
       "st1         {v0.16b}, [%2], #16           \n"  // store 16 pixels.
       "b.gt        1b                            \n"
       : "+r"(src_sobelx),  // %0
@@ -2954,10 +3167,10 @@ void SobelXYRow_NEON(const uint8_t* src_sobelx,
       "1:                                        \n"
       "ld1         {v2.8b}, [%0], #8             \n"  // load 8 sobelx.
       "ld1         {v0.8b}, [%1], #8             \n"  // load 8 sobely.
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 processed per loop.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uqadd       v1.8b, v0.8b, v2.8b           \n"  // add
+      "prfm        pldl1keep, [%1, 448]          \n"
       "st4         {v0.8b,v1.8b,v2.8b,v3.8b}, [%2], #32 \n"  // store 8 ARGB
       "b.gt        1b                            \n"
       : "+r"(src_sobelx),  // %0
@@ -2981,18 +3194,18 @@ void SobelXRow_NEON(const uint8_t* src_y0,
       "1:                                        \n"
       "ld1         {v0.8b}, [%0],%5              \n"  // top
       "ld1         {v1.8b}, [%0],%6              \n"
-      "prfm        pldl1keep, [%0, 448]          \n"
       "usubl       v0.8h, v0.8b, v1.8b           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "ld1         {v2.8b}, [%1],%5              \n"  // center * 2
       "ld1         {v3.8b}, [%1],%6              \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "usubl       v1.8h, v2.8b, v3.8b           \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "add         v0.8h, v0.8h, v1.8h           \n"
       "add         v0.8h, v0.8h, v1.8h           \n"
       "ld1         {v2.8b}, [%2],%5              \n"  // bottom
       "ld1         {v3.8b}, [%2],%6              \n"
-      "prfm        pldl1keep, [%2, 448]          \n"
       "subs        %w4, %w4, #8                  \n"  // 8 pixels
+      "prfm        pldl1keep, [%2, 448]          \n"
       "usubl       v1.8h, v2.8b, v3.8b           \n"
       "add         v0.8h, v0.8h, v1.8h           \n"
       "abs         v0.8h, v0.8h                  \n"
@@ -3030,11 +3243,11 @@ void SobelYRow_NEON(const uint8_t* src_y0,
       "add         v0.8h, v0.8h, v1.8h           \n"
       "ld1         {v2.8b}, [%0],%5              \n"  // right
       "ld1         {v3.8b}, [%1],%5              \n"
-      "prfm        pldl1keep, [%0, 448]          \n"
-      "prfm        pldl1keep, [%1, 448]          \n"
       "subs        %w3, %w3, #8                  \n"  // 8 pixels
       "usubl       v1.8h, v2.8b, v3.8b           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "add         v0.8h, v0.8h, v1.8h           \n"
+      "prfm        pldl1keep, [%1, 448]          \n"
       "abs         v0.8h, v0.8h                  \n"
       "uqxtn       v0.8b, v0.8h                  \n"
       "st1         {v0.8b}, [%2], #8             \n"  // store 8 sobely
@@ -3057,9 +3270,9 @@ void HalfFloat1Row_NEON(const uint16_t* src,
   asm volatile(
       "1:                                        \n"
       "ld1         {v1.16b}, [%0], #16           \n"  // load 8 shorts
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 pixels per loop
       "uxtl        v2.4s, v1.4h                  \n"  // 8 int's
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uxtl2       v3.4s, v1.8h                  \n"
       "scvtf       v2.4s, v2.4s                  \n"  // 8 floats
       "scvtf       v3.4s, v3.4s                  \n"
@@ -3081,9 +3294,9 @@ void HalfFloatRow_NEON(const uint16_t* src,
   asm volatile(
       "1:                                        \n"
       "ld1         {v1.16b}, [%0], #16           \n"  // load 8 shorts
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 pixels per loop
       "uxtl        v2.4s, v1.4h                  \n"  // 8 int's
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uxtl2       v3.4s, v1.8h                  \n"
       "scvtf       v2.4s, v2.4s                  \n"  // 8 floats
       "scvtf       v3.4s, v3.4s                  \n"
@@ -3107,9 +3320,9 @@ void ByteToFloatRow_NEON(const uint8_t* src,
   asm volatile(
       "1:                                        \n"
       "ld1         {v1.8b}, [%0], #8             \n"  // load 8 bytes
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 pixels per loop
       "uxtl        v1.8h, v1.8b                  \n"  // 8 shorts
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uxtl        v2.4s, v1.4h                  \n"  // 8 ints
       "uxtl2       v3.4s, v1.8h                  \n"
       "scvtf       v2.4s, v2.4s                  \n"  // 8 floats
@@ -3136,9 +3349,9 @@ float ScaleMaxSamples_NEON(const float* src,
 
       "1:                                        \n"
       "ld1         {v1.4s, v2.4s}, [%0], #32     \n"  // load 8 samples
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop
       "fmul        v3.4s, v1.4s, %4.s[0]         \n"  // scale
+      "prfm        pldl1keep, [%0, 448]          \n"
       "fmul        v4.4s, v2.4s, %4.s[0]         \n"  // scale
       "fmax        v5.4s, v5.4s, v1.4s           \n"  // max
       "fmax        v6.4s, v6.4s, v2.4s           \n"
@@ -3166,9 +3379,9 @@ float ScaleSumSamples_NEON(const float* src,
 
       "1:                                        \n"
       "ld1         {v1.4s, v2.4s}, [%0], #32     \n"  // load 8 samples
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #8                  \n"  // 8 processed per loop
       "fmul        v3.4s, v1.4s, %4.s[0]         \n"  // scale
+      "prfm        pldl1keep, [%0, 448]          \n"
       "fmul        v4.4s, v2.4s, %4.s[0]         \n"
       "fmla        v5.4s, v1.4s, v1.4s           \n"  // sum of squares
       "fmla        v6.4s, v2.4s, v2.4s           \n"
@@ -3376,10 +3589,10 @@ void NV21ToYUV24Row_NEON(const uint8_t* src_y,
       "1:                                        \n"
       "ld1         {v2.16b}, [%0], #16           \n"  // load 16 Y values
       "ld2         {v0.8b, v1.8b}, [%1], #16     \n"  // load 8 VU values
+      "zip1        v0.16b, v0.16b, v0.16b        \n"  // replicate V values
       "prfm        pldl1keep, [%0, 448]          \n"
+      "zip1        v1.16b, v1.16b, v1.16b        \n"  // replicate U values
       "prfm        pldl1keep, [%1, 448]          \n"
-      "zip1        v0.16b, v0.16b, v0.16b        \n"      // replicate V values
-      "zip1        v1.16b, v1.16b, v1.16b        \n"      // replicate U values
       "subs        %w3, %w3, #16                 \n"      // 16 pixels per loop
       "st3         {v0.16b,v1.16b,v2.16b}, [%2], #48 \n"  // store 16 YUV pixels
       "b.gt        1b                            \n"
@@ -3391,6 +3604,7 @@ void NV21ToYUV24Row_NEON(const uint8_t* src_y,
       : "cc", "memory", "v0", "v1", "v2");
 }
 
+// AYUV is YVUA in memory.  UV for NV12 is UV order in memory.
 void AYUVToUVRow_NEON(const uint8_t* src_ayuv,
                       int src_stride_ayuv,
                       uint8_t* dst_uv,
@@ -3400,12 +3614,12 @@ void AYUVToUVRow_NEON(const uint8_t* src_ayuv,
 
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 ayuv
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v0.8h, v0.16b                 \n"  // V 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v1.8h, v1.16b                 \n"  // U 16 bytes -> 8 shorts.
       "ld4         {v4.16b,v5.16b,v6.16b,v7.16b}, [%1], #64 \n"  // load next 16
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v0.8h, v4.16b                 \n"  // V 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v1.8h, v5.16b                 \n"  // U 16 bytes -> 8 shorts.
       "uqrshrn     v3.8b, v0.8h, #2              \n"  // 2x2 average
       "uqrshrn     v2.8b, v1.8h, #2              \n"
@@ -3429,12 +3643,12 @@ void AYUVToVURow_NEON(const uint8_t* src_ayuv,
 
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16 ayuv
-      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v0.8h, v0.16b                 \n"  // V 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%0, 448]          \n"
       "uaddlp      v1.8h, v1.16b                 \n"  // U 16 bytes -> 8 shorts.
       "ld4         {v4.16b,v5.16b,v6.16b,v7.16b}, [%1], #64 \n"  // load next 16
-      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v0.8h, v4.16b                 \n"  // V 16 bytes -> 8 shorts.
+      "prfm        pldl1keep, [%1, 448]          \n"
       "uadalp      v1.8h, v5.16b                 \n"  // U 16 bytes -> 8 shorts.
       "uqrshrn     v0.8b, v0.8h, #2              \n"  // 2x2 average
       "uqrshrn     v1.8b, v1.8h, #2              \n"
@@ -3454,8 +3668,8 @@ void AYUVToYRow_NEON(const uint8_t* src_ayuv, uint8_t* dst_y, int width) {
   asm volatile(
       "1:                                        \n"
       "ld4         {v0.16b,v1.16b,v2.16b,v3.16b}, [%0], #64 \n"  // load 16
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #16                 \n"  // 16 pixels per loop
+      "prfm        pldl1keep, [%0, 448]          \n"
       "st1         {v2.16b}, [%1], #16           \n"  // store 16 Y pixels
       "b.gt        1b                            \n"
       : "+r"(src_ayuv),  // %0
@@ -3476,9 +3690,9 @@ void SwapUVRow_NEON(const uint8_t* src_uv, uint8_t* dst_vu, int width) {
       "1:                                        \n"
       "ld1         {v0.16b}, [%0], 16            \n"  // load 16 UV values
       "ld1         {v1.16b}, [%0], 16            \n"
-      "prfm        pldl1keep, [%0, 448]          \n"
       "subs        %w2, %w2, #16                 \n"  // 16 pixels per loop
       "tbl         v0.16b, {v0.16b}, v2.16b      \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "tbl         v1.16b, {v1.16b}, v2.16b      \n"
       "stp         q0, q1, [%1], 32              \n"  // store 16 VU pixels
       "b.gt        1b                            \n"
@@ -3531,34 +3745,24 @@ void SplitUVRow_16_NEON(const uint16_t* src_uv,
                         uint16_t* dst_v,
                         int depth,
                         int width) {
+  int shift = depth - 16;  // Negative for right shift.
   asm volatile(
-      "dup         v0.4s, %w3                    \n"
+      "dup         v2.8h, %w4                    \n"
       "1:                                        \n"
-      "ld2         {v1.8h, v2.8h}, [%0], #32     \n"  // load 8 UV
+      "ld2         {v0.8h, v1.8h}, [%0], #32     \n"  // load 8 UV
+      "subs        %w3, %w3, #8                  \n"  // 8 src pixels per loop
+      "ushl        v0.8h, v0.8h, v2.8h           \n"
       "prfm        pldl1keep, [%0, 448]          \n"
-      "ushll       v3.4s, v1.4h, #0              \n"
-      "ushll2      v4.4s, v1.8h, #0              \n"
-      "ushl        v3.4s, v3.4s, v0.4s           \n"
-      "ushl        v4.4s, v4.4s, v0.4s           \n"
-      "xtn         v1.4h, v3.4s                  \n"
-      "xtn2        v1.8h, v4.4s                  \n"
-      "ushll       v3.4s, v2.4h, #0              \n"
-      "ushll2      v4.4s, v2.8h, #0              \n"
-      "ushl        v3.4s, v3.4s, v0.4s           \n"
-      "ushl        v4.4s, v4.4s, v0.4s           \n"
-      "xtn         v2.4h, v3.4s                  \n"
-      "xtn2        v2.8h, v4.4s                  \n"
-      "subs        %w4, %w4, #8                  \n"  // 8 src pixels per loop
-      "st1         {v1.8h}, [%1], #16            \n"  // store 8 U pixels
-      "st1         {v2.8h}, [%2], #16            \n"  // store 8 V pixels
+      "ushl        v1.8h, v1.8h, v2.8h           \n"
+      "st1         {v0.8h}, [%1], #16            \n"  // store 8 U pixels
+      "st1         {v1.8h}, [%2], #16            \n"  // store 8 V pixels
       "b.gt        1b                            \n"
       : "+r"(src_uv),  // %0
         "+r"(dst_u),   // %1
         "+r"(dst_v),   // %2
-        "+r"(depth),   // %3
-        "+r"(width)    // %4
-      :
-      : "cc", "memory", "v0", "v1", "v2", "v3", "v4");
+        "+r"(width)    // %3
+      : "r"(shift)     // %4
+      : "cc", "memory", "v0", "v1", "v2");
 }
 
 void MergeUVRow_16_NEON(const uint16_t* src_u,
@@ -3568,23 +3772,22 @@ void MergeUVRow_16_NEON(const uint16_t* src_u,
                         int width) {
   int shift = 16 - depth;
   asm volatile(
-      "dup         v2.8h, %w3                    \n"
+      "dup         v2.8h, %w4                    \n"
       "1:                                        \n"
       "ld1         {v0.8h}, [%0], #16            \n"  // load 8 U
-      "prfm        pldl1keep, [%0, 448]          \n"
+      "subs        %w3, %w3, #8                  \n"  // 8 src pixels per loop
       "ld1         {v1.8h}, [%1], #16            \n"  // load 8 V
-      "prfm        pldl1keep, [%1, 448]          \n"
       "ushl        v0.8h, v0.8h, v2.8h           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "ushl        v1.8h, v1.8h, v2.8h           \n"
-      "subs        %w4, %w4, #8                  \n"  // 8 src pixels per loop
+      "prfm        pldl1keep, [%1, 448]          \n"
       "st2         {v0.8h, v1.8h}, [%2], #32     \n"  // store 8 UV pixels
       "b.gt        1b                            \n"
       : "+r"(src_u),   // %0
         "+r"(src_v),   // %1
         "+r"(dst_uv),  // %2
-        "+r"(shift),   // %3
-        "+r"(width)    // %4
-      :
+        "+r"(width)    // %3
+      : "r"(shift)     // %4
       : "cc", "memory", "v0", "v1", "v2");
 }
 
@@ -3595,10 +3798,9 @@ void MultiplyRow_16_NEON(const uint16_t* src_y,
   asm volatile(
       "dup         v2.8h, %w2                    \n"
       "1:                                        \n"
-      "ldp         q0, q1, [%0]                  \n"
-      "add         %0, %0, #32                   \n"
-      "prfm        pldl1keep, [%0, 448]          \n"
+      "ldp         q0, q1, [%0], #32             \n"
       "mul         v0.8h, v0.8h, v2.8h           \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "mul         v1.8h, v1.8h, v2.8h           \n"
       "stp         q0, q1, [%1]                  \n"  // store 16 pixels
       "add         %1, %1, #32                   \n"
@@ -3619,11 +3821,10 @@ void DivideRow_16_NEON(const uint16_t* src_y,
   asm volatile(
       "dup         v0.8h, %w2                    \n"
       "1:                                        \n"
-      "ldp         q1, q2, [%0]                  \n"
-      "add         %0, %0, #32                   \n"
-      "prfm        pldl1keep, [%0, 448]          \n"
+      "ldp         q1, q2, [%0], #32             \n"
       "ushll       v3.4s, v1.4h, #0              \n"
       "ushll       v4.4s, v2.4h, #0              \n"
+      "prfm        pldl1keep, [%0, 448]          \n"
       "ushll2      v1.4s, v1.8h, #0              \n"
       "ushll2      v2.4s, v2.8h, #0              \n"
       "mul         v3.4s, v0.4s, v3.4s           \n"
diff --git a/source/row_win.cc b/source/row_win.cc
index 95151892..2c3241cf 100644
--- a/source/row_win.cc
+++ b/source/row_win.cc
@@ -10,9 +10,9 @@
 
 #include "libyuv/row.h"
 
-// This module is for Visual C 32/64 bit and clangcl 32 bit
+// This module is for Visual C 32/64 bit
 #if !defined(LIBYUV_DISABLE_X86) && defined(_MSC_VER) && \
-    (defined(_M_IX86) || (defined(_M_X64) && !defined(__clang__)))
+    !defined(__clang__) && (defined(_M_IX86) || defined(_M_X64))
 
 #if defined(_M_X64)
 #include <emmintrin.h>
@@ -29,9 +29,9 @@ extern "C" {
 
 // Read 8 UV from 444
 #define READYUV444                                    \
-  xmm0 = _mm_loadl_epi64((__m128i*)u_buf);            \
+  xmm3 = _mm_loadl_epi64((__m128i*)u_buf);            \
   xmm1 = _mm_loadl_epi64((__m128i*)(u_buf + offset)); \
-  xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);               \
+  xmm3 = _mm_unpacklo_epi8(xmm3, xmm1);               \
   u_buf += 8;                                         \
   xmm4 = _mm_loadl_epi64((__m128i*)y_buf);            \
   xmm4 = _mm_unpacklo_epi8(xmm4, xmm4);               \
@@ -39,9 +39,9 @@ extern "C" {
 
 // Read 8 UV from 444, With 8 Alpha.
 #define READYUVA444                                   \
-  xmm0 = _mm_loadl_epi64((__m128i*)u_buf);            \
+  xmm3 = _mm_loadl_epi64((__m128i*)u_buf);            \
   xmm1 = _mm_loadl_epi64((__m128i*)(u_buf + offset)); \
-  xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);               \
+  xmm3 = _mm_unpacklo_epi8(xmm3, xmm1);               \
   u_buf += 8;                                         \
   xmm4 = _mm_loadl_epi64((__m128i*)y_buf);            \
   xmm4 = _mm_unpacklo_epi8(xmm4, xmm4);               \
@@ -51,10 +51,10 @@ extern "C" {
 
 // Read 4 UV from 422, upsample to 8 UV.
 #define READYUV422                                        \
-  xmm0 = _mm_cvtsi32_si128(*(uint32_t*)u_buf);            \
+  xmm3 = _mm_cvtsi32_si128(*(uint32_t*)u_buf);            \
   xmm1 = _mm_cvtsi32_si128(*(uint32_t*)(u_buf + offset)); \
-  xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);                   \
-  xmm0 = _mm_unpacklo_epi16(xmm0, xmm0);                  \
+  xmm3 = _mm_unpacklo_epi8(xmm3, xmm1);                   \
+  xmm3 = _mm_unpacklo_epi16(xmm3, xmm3);                  \
   u_buf += 4;                                             \
   xmm4 = _mm_loadl_epi64((__m128i*)y_buf);                \
   xmm4 = _mm_unpacklo_epi8(xmm4, xmm4);                   \
@@ -62,10 +62,10 @@ extern "C" {
 
 // Read 4 UV from 422, upsample to 8 UV.  With 8 Alpha.
 #define READYUVA422                                       \
-  xmm0 = _mm_cvtsi32_si128(*(uint32_t*)u_buf);            \
+  xmm3 = _mm_cvtsi32_si128(*(uint32_t*)u_buf);            \
   xmm1 = _mm_cvtsi32_si128(*(uint32_t*)(u_buf + offset)); \
-  xmm0 = _mm_unpacklo_epi8(xmm0, xmm1);                   \
-  xmm0 = _mm_unpacklo_epi16(xmm0, xmm0);                  \
+  xmm3 = _mm_unpacklo_epi8(xmm3, xmm1);                   \
+  xmm3 = _mm_unpacklo_epi16(xmm3, xmm3);                  \
   u_buf += 4;                                             \
   xmm4 = _mm_loadl_epi64((__m128i*)y_buf);                \
   xmm4 = _mm_unpacklo_epi8(xmm4, xmm4);                   \
@@ -74,24 +74,21 @@ extern "C" {
   a_buf += 8;
 
 // Convert 8 pixels: 8 UV and 8 Y.
-#define YUVTORGB(yuvconstants)                                     \
-  xmm1 = _mm_loadu_si128(&xmm0);                                   \
-  xmm2 = _mm_loadu_si128(&xmm0);                                   \
-  xmm0 = _mm_maddubs_epi16(xmm0, *(__m128i*)yuvconstants->kUVToB); \
-  xmm1 = _mm_maddubs_epi16(xmm1, *(__m128i*)yuvconstants->kUVToG); \
-  xmm2 = _mm_maddubs_epi16(xmm2, *(__m128i*)yuvconstants->kUVToR); \
-  xmm0 = _mm_sub_epi16(*(__m128i*)yuvconstants->kUVBiasB, xmm0);   \
-  xmm1 = _mm_sub_epi16(*(__m128i*)yuvconstants->kUVBiasG, xmm1);   \
-  xmm2 = _mm_sub_epi16(*(__m128i*)yuvconstants->kUVBiasR, xmm2);   \
-  xmm4 = _mm_mulhi_epu16(xmm4, *(__m128i*)yuvconstants->kYToRgb);  \
-  xmm0 = _mm_adds_epi16(xmm0, xmm4);                               \
-  xmm1 = _mm_adds_epi16(xmm1, xmm4);                               \
-  xmm2 = _mm_adds_epi16(xmm2, xmm4);                               \
-  xmm0 = _mm_srai_epi16(xmm0, 6);                                  \
-  xmm1 = _mm_srai_epi16(xmm1, 6);                                  \
-  xmm2 = _mm_srai_epi16(xmm2, 6);                                  \
-  xmm0 = _mm_packus_epi16(xmm0, xmm0);                             \
-  xmm1 = _mm_packus_epi16(xmm1, xmm1);                             \
+#define YUVTORGB(yuvconstants)                                            \
+  xmm3 = _mm_sub_epi8(xmm3, _mm_set1_epi8(0x80));                         \
+  xmm4 = _mm_mulhi_epu16(xmm4, *(__m128i*)yuvconstants->kYToRgb);         \
+  xmm4 = _mm_add_epi16(xmm4, *(__m128i*)yuvconstants->kYBiasToRgb);       \
+  xmm0 = _mm_maddubs_epi16(*(__m128i*)yuvconstants->kUVToB, xmm3);        \
+  xmm1 = _mm_maddubs_epi16(*(__m128i*)yuvconstants->kUVToG, xmm3);        \
+  xmm2 = _mm_maddubs_epi16(*(__m128i*)yuvconstants->kUVToR, xmm3);        \
+  xmm0 = _mm_adds_epi16(xmm4, xmm0);                                      \
+  xmm1 = _mm_subs_epi16(xmm4, xmm1);                                      \
+  xmm2 = _mm_adds_epi16(xmm4, xmm2);                                      \
+  xmm0 = _mm_srai_epi16(xmm0, 6);                                         \
+  xmm1 = _mm_srai_epi16(xmm1, 6);                                         \
+  xmm2 = _mm_srai_epi16(xmm2, 6);                                         \
+  xmm0 = _mm_packus_epi16(xmm0, xmm0);                                    \
+  xmm1 = _mm_packus_epi16(xmm1, xmm1);                                    \
   xmm2 = _mm_packus_epi16(xmm2, xmm2);
 
 // Store 8 ARGB values.
@@ -112,7 +109,7 @@ void I422ToARGBRow_SSSE3(const uint8_t* y_buf,
                          uint8_t* dst_argb,
                          const struct YuvConstants* yuvconstants,
                          int width) {
-  __m128i xmm0, xmm1, xmm2, xmm4;
+  __m128i xmm0, xmm1, xmm2, xmm3, xmm4;
   const __m128i xmm5 = _mm_set1_epi8(-1);
   const ptrdiff_t offset = (uint8_t*)v_buf - (uint8_t*)u_buf;
   while (width > 0) {
@@ -132,7 +129,7 @@ void I422AlphaToARGBRow_SSSE3(const uint8_t* y_buf,
                               uint8_t* dst_argb,
                               const struct YuvConstants* yuvconstants,
                               int width) {
-  __m128i xmm0, xmm1, xmm2, xmm4, xmm5;
+  __m128i xmm0, xmm1, xmm2, xmm3, xmm4, xmm5;
   const ptrdiff_t offset = (uint8_t*)v_buf - (uint8_t*)u_buf;
   while (width > 0) {
     READYUVA422
@@ -150,7 +147,7 @@ void I444ToARGBRow_SSSE3(const uint8_t* y_buf,
                          uint8_t* dst_argb,
                          const struct YuvConstants* yuvconstants,
                          int width) {
-  __m128i xmm0, xmm1, xmm2, xmm4;
+  __m128i xmm0, xmm1, xmm2, xmm3, xmm4;
   const __m128i xmm5 = _mm_set1_epi8(-1);
   const ptrdiff_t offset = (uint8_t*)v_buf - (uint8_t*)u_buf;
   while (width > 0) {
@@ -170,7 +167,7 @@ void I444AlphaToARGBRow_SSSE3(const uint8_t* y_buf,
                               uint8_t* dst_argb,
                               const struct YuvConstants* yuvconstants,
                               int width) {
-  __m128i xmm0, xmm1, xmm2, xmm4, xmm5;
+  __m128i xmm0, xmm1, xmm2, xmm3, xmm4, xmm5;
   const ptrdiff_t offset = (uint8_t*)v_buf - (uint8_t*)u_buf;
   while (width > 0) {
     READYUVA444
@@ -247,11 +244,11 @@ static const uvec8 kAddY16 = {16u, 16u, 16u, 16u, 16u, 16u, 16u, 16u,
 // 7 bit fixed point 0.5.
 static const vec16 kAddYJ64 = {64, 64, 64, 64, 64, 64, 64, 64};
 
-static const uvec8 kAddUV128 = {128u, 128u, 128u, 128u, 128u, 128u, 128u, 128u,
-                                128u, 128u, 128u, 128u, 128u, 128u, 128u, 128u};
-
-static const uvec16 kAddUVJ128 = {0x8080u, 0x8080u, 0x8080u, 0x8080u,
-                                  0x8080u, 0x8080u, 0x8080u, 0x8080u};
+// 8 bit fixed point 0.5, for bias of UV.
+static const ulvec8 kBiasUV128 = {
+    0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,
+    0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,
+    0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80};
 
 // Shuffle table for converting RGB24 to ARGB.
 static const uvec8 kShuffleMaskRGB24ToARGB = {
@@ -1427,7 +1424,7 @@ __declspec(naked) void RGBAToYRow_SSSE3(const uint8_t* src_argb,
   }
 }
 
-__declspec(naked) void ARGBToUVRow_SSSE3(const uint8_t* src_argb0,
+__declspec(naked) void ARGBToUVRow_SSSE3(const uint8_t* src_argb,
                                          int src_stride_argb,
                                          uint8_t* dst_u,
                                          uint8_t* dst_v,
@@ -1440,7 +1437,7 @@ __declspec(naked) void ARGBToUVRow_SSSE3(const uint8_t* src_argb0,
     mov        edx, [esp + 8 + 12]  // dst_u
     mov        edi, [esp + 8 + 16]  // dst_v
     mov        ecx, [esp + 8 + 20]  // width
-    movdqa     xmm5, xmmword ptr kAddUV128
+    movdqa     xmm5, xmmword ptr kBiasUV128
     movdqa     xmm6, xmmword ptr kARGBToV
     movdqa     xmm7, xmmword ptr kARGBToU
     sub        edi, edx  // stride from u to v
@@ -1499,7 +1496,7 @@ __declspec(naked) void ARGBToUVRow_SSSE3(const uint8_t* src_argb0,
   }
 }
 
-__declspec(naked) void ARGBToUVJRow_SSSE3(const uint8_t* src_argb0,
+__declspec(naked) void ARGBToUVJRow_SSSE3(const uint8_t* src_argb,
                                           int src_stride_argb,
                                           uint8_t* dst_u,
                                           uint8_t* dst_v,
@@ -1512,7 +1509,7 @@ __declspec(naked) void ARGBToUVJRow_SSSE3(const uint8_t* src_argb0,
     mov        edx, [esp + 8 + 12]  // dst_u
     mov        edi, [esp + 8 + 16]  // dst_v
     mov        ecx, [esp + 8 + 20]  // width
-    movdqa     xmm5, xmmword ptr kAddUVJ128
+    movdqa     xmm5, xmmword ptr kBiasUV128
     movdqa     xmm6, xmmword ptr kARGBToVJ
     movdqa     xmm7, xmmword ptr kARGBToUJ
     sub        edi, edx  // stride from u to v
@@ -1573,7 +1570,7 @@ __declspec(naked) void ARGBToUVJRow_SSSE3(const uint8_t* src_argb0,
 }
 
 #ifdef HAS_ARGBTOUVROW_AVX2
-__declspec(naked) void ARGBToUVRow_AVX2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBToUVRow_AVX2(const uint8_t* src_argb,
                                         int src_stride_argb,
                                         uint8_t* dst_u,
                                         uint8_t* dst_v,
@@ -1586,7 +1583,7 @@ __declspec(naked) void ARGBToUVRow_AVX2(const uint8_t* src_argb0,
     mov        edx, [esp + 8 + 12]  // dst_u
     mov        edi, [esp + 8 + 16]  // dst_v
     mov        ecx, [esp + 8 + 20]  // width
-    vbroadcastf128 ymm5, xmmword ptr kAddUV128
+    vbroadcastf128 ymm5, xmmword ptr kBiasUV128
     vbroadcastf128 ymm6, xmmword ptr kARGBToV
     vbroadcastf128 ymm7, xmmword ptr kARGBToU
     sub        edi, edx   // stride from u to v
@@ -1641,7 +1638,7 @@ __declspec(naked) void ARGBToUVRow_AVX2(const uint8_t* src_argb0,
 #endif  // HAS_ARGBTOUVROW_AVX2
 
 #ifdef HAS_ARGBTOUVJROW_AVX2
-__declspec(naked) void ARGBToUVJRow_AVX2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBToUVJRow_AVX2(const uint8_t* src_argb,
                                          int src_stride_argb,
                                          uint8_t* dst_u,
                                          uint8_t* dst_v,
@@ -1654,7 +1651,7 @@ __declspec(naked) void ARGBToUVJRow_AVX2(const uint8_t* src_argb0,
     mov        edx, [esp + 8 + 12]  // dst_u
     mov        edi, [esp + 8 + 16]  // dst_v
     mov        ecx, [esp + 8 + 20]  // width
-    vbroadcastf128 ymm5, xmmword ptr kAddUVJ128
+    vbroadcastf128 ymm5, xmmword ptr kBiasUV128
     vbroadcastf128 ymm6, xmmword ptr kARGBToVJ
     vbroadcastf128 ymm7, xmmword ptr kARGBToUJ
     sub        edi, edx   // stride from u to v
@@ -1709,7 +1706,7 @@ __declspec(naked) void ARGBToUVJRow_AVX2(const uint8_t* src_argb0,
 }
 #endif  // HAS_ARGBTOUVJROW_AVX2
 
-__declspec(naked) void ARGBToUV444Row_SSSE3(const uint8_t* src_argb0,
+__declspec(naked) void ARGBToUV444Row_SSSE3(const uint8_t* src_argb,
                                             uint8_t* dst_u,
                                             uint8_t* dst_v,
                                             int width) {
@@ -1719,7 +1716,7 @@ __declspec(naked) void ARGBToUV444Row_SSSE3(const uint8_t* src_argb0,
     mov        edx, [esp + 4 + 8]  // dst_u
     mov        edi, [esp + 4 + 12]  // dst_v
     mov        ecx, [esp + 4 + 16]  // width
-    movdqa     xmm5, xmmword ptr kAddUV128
+    movdqa     xmm5, xmmword ptr kBiasUV128
     movdqa     xmm6, xmmword ptr kARGBToV
     movdqa     xmm7, xmmword ptr kARGBToU
     sub        edi, edx    // stride from u to v
@@ -1767,7 +1764,7 @@ __declspec(naked) void ARGBToUV444Row_SSSE3(const uint8_t* src_argb0,
   }
 }
 
-__declspec(naked) void BGRAToUVRow_SSSE3(const uint8_t* src_argb0,
+__declspec(naked) void BGRAToUVRow_SSSE3(const uint8_t* src_argb,
                                          int src_stride_argb,
                                          uint8_t* dst_u,
                                          uint8_t* dst_v,
@@ -1780,7 +1777,7 @@ __declspec(naked) void BGRAToUVRow_SSSE3(const uint8_t* src_argb0,
     mov        edx, [esp + 8 + 12]  // dst_u
     mov        edi, [esp + 8 + 16]  // dst_v
     mov        ecx, [esp + 8 + 20]  // width
-    movdqa     xmm5, xmmword ptr kAddUV128
+    movdqa     xmm5, xmmword ptr kBiasUV128
     movdqa     xmm6, xmmword ptr kBGRAToV
     movdqa     xmm7, xmmword ptr kBGRAToU
     sub        edi, edx  // stride from u to v
@@ -1839,7 +1836,7 @@ __declspec(naked) void BGRAToUVRow_SSSE3(const uint8_t* src_argb0,
   }
 }
 
-__declspec(naked) void ABGRToUVRow_SSSE3(const uint8_t* src_argb0,
+__declspec(naked) void ABGRToUVRow_SSSE3(const uint8_t* src_argb,
                                          int src_stride_argb,
                                          uint8_t* dst_u,
                                          uint8_t* dst_v,
@@ -1852,7 +1849,7 @@ __declspec(naked) void ABGRToUVRow_SSSE3(const uint8_t* src_argb0,
     mov        edx, [esp + 8 + 12]  // dst_u
     mov        edi, [esp + 8 + 16]  // dst_v
     mov        ecx, [esp + 8 + 20]  // width
-    movdqa     xmm5, xmmword ptr kAddUV128
+    movdqa     xmm5, xmmword ptr kBiasUV128
     movdqa     xmm6, xmmword ptr kABGRToV
     movdqa     xmm7, xmmword ptr kABGRToU
     sub        edi, edx  // stride from u to v
@@ -1911,7 +1908,7 @@ __declspec(naked) void ABGRToUVRow_SSSE3(const uint8_t* src_argb0,
   }
 }
 
-__declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
+__declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb,
                                          int src_stride_argb,
                                          uint8_t* dst_u,
                                          uint8_t* dst_v,
@@ -1924,7 +1921,7 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
     mov        edx, [esp + 8 + 12]  // dst_u
     mov        edi, [esp + 8 + 16]  // dst_v
     mov        ecx, [esp + 8 + 20]  // width
-    movdqa     xmm5, xmmword ptr kAddUV128
+    movdqa     xmm5, xmmword ptr kBiasUV128
     movdqa     xmm6, xmmword ptr kRGBAToV
     movdqa     xmm7, xmmword ptr kRGBAToU
     sub        edi, edx  // stride from u to v
@@ -1987,12 +1984,12 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
 // Read 16 UV from 444
 #define READYUV444_AVX2 \
   __asm {                                                \
-    __asm vmovdqu    xmm0, [esi] /* U */                      \
+    __asm vmovdqu    xmm3, [esi] /* U */                      \
     __asm vmovdqu    xmm1, [esi + edi] /* V */                      \
     __asm lea        esi,  [esi + 16]                                          \
-    __asm vpermq     ymm0, ymm0, 0xd8                                          \
+    __asm vpermq     ymm3, ymm3, 0xd8                                          \
     __asm vpermq     ymm1, ymm1, 0xd8                                          \
-    __asm vpunpcklbw ymm0, ymm0, ymm1 /* UV */                     \
+    __asm vpunpcklbw ymm3, ymm3, ymm1 /* UV */                     \
     __asm vmovdqu    xmm4, [eax] /* Y */                      \
     __asm vpermq     ymm4, ymm4, 0xd8                                          \
     __asm vpunpcklbw ymm4, ymm4, ymm4                                          \
@@ -2001,12 +1998,12 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
 // Read 16 UV from 444.  With 16 Alpha.
 #define READYUVA444_AVX2 \
   __asm {                                                                      \
-    __asm vmovdqu    xmm0, [esi] /* U */                                       \
+    __asm vmovdqu    xmm3, [esi] /* U */                                       \
     __asm vmovdqu    xmm1, [esi + edi] /* V */                                 \
     __asm lea        esi,  [esi + 16]                                          \
-    __asm vpermq     ymm0, ymm0, 0xd8                                          \
+    __asm vpermq     ymm3, ymm3, 0xd8                                          \
     __asm vpermq     ymm1, ymm1, 0xd8                                          \
-    __asm vpunpcklbw ymm0, ymm0, ymm1 /* UV */                                 \
+    __asm vpunpcklbw ymm3, ymm3, ymm1 /* UV */                                 \
     __asm vmovdqu    xmm4, [eax] /* Y */                                       \
     __asm vpermq     ymm4, ymm4, 0xd8                                          \
     __asm vpunpcklbw ymm4, ymm4, ymm4                                          \
@@ -2018,12 +2015,12 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
 // Read 8 UV from 422, upsample to 16 UV.
 #define READYUV422_AVX2 \
   __asm {                                                \
-    __asm vmovq      xmm0, qword ptr [esi] /* U */                      \
+    __asm vmovq      xmm3, qword ptr [esi] /* U */                      \
     __asm vmovq      xmm1, qword ptr [esi + edi] /* V */                      \
     __asm lea        esi,  [esi + 8]                                           \
-    __asm vpunpcklbw ymm0, ymm0, ymm1 /* UV */                     \
-    __asm vpermq     ymm0, ymm0, 0xd8                                          \
-    __asm vpunpcklwd ymm0, ymm0, ymm0 /* UVUV (upsample) */        \
+    __asm vpunpcklbw ymm3, ymm3, ymm1 /* UV */                     \
+    __asm vpermq     ymm3, ymm3, 0xd8                                          \
+    __asm vpunpcklwd ymm3, ymm3, ymm3 /* UVUV (upsample) */        \
     __asm vmovdqu    xmm4, [eax] /* Y */                      \
     __asm vpermq     ymm4, ymm4, 0xd8                                          \
     __asm vpunpcklbw ymm4, ymm4, ymm4                                          \
@@ -2032,12 +2029,12 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
 // Read 8 UV from 422, upsample to 16 UV.  With 16 Alpha.
 #define READYUVA422_AVX2 \
   __asm {                                               \
-    __asm vmovq      xmm0, qword ptr [esi] /* U */                      \
+    __asm vmovq      xmm3, qword ptr [esi] /* U */                      \
     __asm vmovq      xmm1, qword ptr [esi + edi] /* V */                      \
     __asm lea        esi,  [esi + 8]                                           \
-    __asm vpunpcklbw ymm0, ymm0, ymm1 /* UV */                     \
-    __asm vpermq     ymm0, ymm0, 0xd8                                          \
-    __asm vpunpcklwd ymm0, ymm0, ymm0 /* UVUV (upsample) */        \
+    __asm vpunpcklbw ymm3, ymm3, ymm1 /* UV */                     \
+    __asm vpermq     ymm3, ymm3, 0xd8                                          \
+    __asm vpunpcklwd ymm3, ymm3, ymm3 /* UVUV (upsample) */        \
     __asm vmovdqu    xmm4, [eax] /* Y */                      \
     __asm vpermq     ymm4, ymm4, 0xd8                                          \
     __asm vpunpcklbw ymm4, ymm4, ymm4                                          \
@@ -2049,10 +2046,10 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
 // Read 8 UV from NV12, upsample to 16 UV.
 #define READNV12_AVX2 \
   __asm {                                                  \
-    __asm vmovdqu    xmm0, [esi] /* UV */                     \
+    __asm vmovdqu    xmm3, [esi] /* UV */                     \
     __asm lea        esi,  [esi + 16]                                          \
-    __asm vpermq     ymm0, ymm0, 0xd8                                          \
-    __asm vpunpcklwd ymm0, ymm0, ymm0 /* UVUV (upsample) */        \
+    __asm vpermq     ymm3, ymm3, 0xd8                                          \
+    __asm vpunpcklwd ymm3, ymm3, ymm3 /* UVUV (upsample) */        \
     __asm vmovdqu    xmm4, [eax] /* Y */                      \
     __asm vpermq     ymm4, ymm4, 0xd8                                          \
     __asm vpunpcklbw ymm4, ymm4, ymm4                                          \
@@ -2061,10 +2058,10 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
 // Read 8 UV from NV21, upsample to 16 UV.
 #define READNV21_AVX2 \
   __asm {                                                  \
-    __asm vmovdqu    xmm0, [esi] /* UV */                     \
+    __asm vmovdqu    xmm3, [esi] /* UV */                     \
     __asm lea        esi,  [esi + 16]                                          \
-    __asm vpermq     ymm0, ymm0, 0xd8                                          \
-    __asm vpshufb    ymm0, ymm0, ymmword ptr kShuffleNV21                      \
+    __asm vpermq     ymm3, ymm3, 0xd8                                          \
+    __asm vpshufb    ymm3, ymm3, ymmword ptr kShuffleNV21                      \
     __asm vmovdqu    xmm4, [eax] /* Y */                      \
     __asm vpermq     ymm4, ymm4, 0xd8                                          \
     __asm vpunpcklbw ymm4, ymm4, ymm4                                          \
@@ -2075,8 +2072,8 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
   __asm {                                                  \
     __asm vmovdqu    ymm4, [eax] /* YUY2 */                           \
     __asm vpshufb    ymm4, ymm4, ymmword ptr kShuffleYUY2Y                     \
-    __asm vmovdqu    ymm0, [eax] /* UV */                             \
-    __asm vpshufb    ymm0, ymm0, ymmword ptr kShuffleYUY2UV                    \
+    __asm vmovdqu    ymm3, [eax] /* UV */                             \
+    __asm vpshufb    ymm3, ymm3, ymmword ptr kShuffleYUY2UV                    \
     __asm lea        eax, [eax + 32]}
 
 // Read 8 UYVY with 16 Y and upsample 8 UV to 16 UV.
@@ -2084,32 +2081,32 @@ __declspec(naked) void RGBAToUVRow_SSSE3(const uint8_t* src_argb0,
   __asm {                                                  \
     __asm vmovdqu    ymm4, [eax] /* UYVY */                           \
     __asm vpshufb    ymm4, ymm4, ymmword ptr kShuffleUYVYY                     \
-    __asm vmovdqu    ymm0, [eax] /* UV */                             \
-    __asm vpshufb    ymm0, ymm0, ymmword ptr kShuffleUYVYUV                    \
+    __asm vmovdqu    ymm3, [eax] /* UV */                             \
+    __asm vpshufb    ymm3, ymm3, ymmword ptr kShuffleUYVYUV                    \
     __asm lea        eax, [eax + 32]}
 
 // Convert 16 pixels: 16 UV and 16 Y.
 #define YUVTORGB_AVX2(YuvConstants) \
-  __asm {                                    \
-    __asm vpmaddubsw ymm2, ymm0, ymmword ptr [YuvConstants + KUVTOR] /* R UV */\
-    __asm vpmaddubsw ymm1, ymm0, ymmword ptr [YuvConstants + KUVTOG] /* G UV */\
-    __asm vpmaddubsw ymm0, ymm0, ymmword ptr [YuvConstants + KUVTOB] /* B UV */\
-    __asm vmovdqu    ymm3, ymmword ptr [YuvConstants + KUVBIASR]               \
-    __asm vpsubw     ymm2, ymm3, ymm2                                          \
-    __asm vmovdqu    ymm3, ymmword ptr [YuvConstants + KUVBIASG]               \
-    __asm vpsubw     ymm1, ymm3, ymm1                                          \
-    __asm vmovdqu    ymm3, ymmword ptr [YuvConstants + KUVBIASB]               \
-    __asm vpsubw     ymm0, ymm3, ymm0 /* Step 2: Find Y contribution to 16 R,G,B values */                       \
+  __asm {                                                                      \
+    __asm vpsubb     ymm3, ymm3, ymmword ptr kBiasUV128                        \
     __asm vpmulhuw   ymm4, ymm4, ymmword ptr [YuvConstants + KYTORGB]          \
-    __asm vpaddsw    ymm0, ymm0, ymm4 /* B += Y */                   \
-    __asm vpaddsw    ymm1, ymm1, ymm4 /* G += Y */                   \
-    __asm vpaddsw    ymm2, ymm2, ymm4 /* R += Y */                   \
+    __asm vmovdqa    ymm0, ymmword ptr [YuvConstants + KUVTOB]                 \
+    __asm vmovdqa    ymm1, ymmword ptr [YuvConstants + KUVTOG]                 \
+    __asm vmovdqa    ymm2, ymmword ptr [YuvConstants + KUVTOR]                 \
+    __asm vpmaddubsw ymm0, ymm0, ymm3 /* B UV */                               \
+    __asm vpmaddubsw ymm1, ymm1, ymm3 /* G UV */                               \
+    __asm vpmaddubsw ymm2, ymm2, ymm3 /* B UV */                               \
+    __asm vmovdqu    ymm3, ymmword ptr [YuvConstants + KYBIASTORGB]            \
+    __asm vpaddw     ymm4, ymm3, ymm4                                          \
+    __asm vpaddsw    ymm0, ymm0, ymm4                                          \
+    __asm vpsubsw    ymm1, ymm4, ymm1                                          \
+    __asm vpaddsw    ymm2, ymm2, ymm4                                          \
     __asm vpsraw     ymm0, ymm0, 6                                             \
     __asm vpsraw     ymm1, ymm1, 6                                             \
     __asm vpsraw     ymm2, ymm2, 6                                             \
-    __asm vpackuswb  ymm0, ymm0, ymm0 /* B */                        \
-    __asm vpackuswb  ymm1, ymm1, ymm1 /* G */                        \
-    __asm vpackuswb  ymm2, ymm2, ymm2 /* R */                  \
+    __asm vpackuswb  ymm0, ymm0, ymm0                                          \
+    __asm vpackuswb  ymm1, ymm1, ymm1                                          \
+    __asm vpackuswb  ymm2, ymm2, ymm2                                          \
   }
 
 // Store 16 ARGB values.
@@ -2481,10 +2478,10 @@ __declspec(naked) void I422ToRGBARow_AVX2(
 // Read 8 UV from 444.
 #define READYUV444 \
   __asm {                                                     \
-    __asm movq       xmm0, qword ptr [esi] /* U */                             \
+    __asm movq       xmm3, qword ptr [esi] /* U */                             \
     __asm movq       xmm1, qword ptr [esi + edi] /* V */                       \
     __asm lea        esi,  [esi + 8]                                           \
-    __asm punpcklbw  xmm0, xmm1 /* UV */                             \
+    __asm punpcklbw  xmm3, xmm1 /* UV */                             \
     __asm movq       xmm4, qword ptr [eax]                                     \
     __asm punpcklbw  xmm4, xmm4                                                \
     __asm lea        eax, [eax + 8]}
@@ -2492,10 +2489,10 @@ __declspec(naked) void I422ToRGBARow_AVX2(
 // Read 4 UV from 444.  With 8 Alpha.
 #define READYUVA444 \
   __asm {                                                                      \
-    __asm movq       xmm0, qword ptr [esi] /* U */                             \
+    __asm movq       xmm3, qword ptr [esi] /* U */                             \
     __asm movq       xmm1, qword ptr [esi + edi] /* V */                       \
     __asm lea        esi,  [esi + 8]                                           \
-    __asm punpcklbw  xmm0, xmm1 /* UV */                                       \
+    __asm punpcklbw  xmm3, xmm1 /* UV */                                       \
     __asm movq       xmm4, qword ptr [eax]                                     \
     __asm punpcklbw  xmm4, xmm4                                                \
     __asm lea        eax, [eax + 8]                                            \
@@ -2505,11 +2502,11 @@ __declspec(naked) void I422ToRGBARow_AVX2(
 // Read 4 UV from 422, upsample to 8 UV.
 #define READYUV422 \
   __asm {                                                     \
-    __asm movd       xmm0, [esi] /* U */                              \
+    __asm movd       xmm3, [esi] /* U */                              \
     __asm movd       xmm1, [esi + edi] /* V */                              \
     __asm lea        esi,  [esi + 4]                                           \
-    __asm punpcklbw  xmm0, xmm1 /* UV */                             \
-    __asm punpcklwd  xmm0, xmm0 /* UVUV (upsample) */                \
+    __asm punpcklbw  xmm3, xmm1 /* UV */                             \
+    __asm punpcklwd  xmm3, xmm3 /* UVUV (upsample) */                \
     __asm movq       xmm4, qword ptr [eax]                                     \
     __asm punpcklbw  xmm4, xmm4                                                \
     __asm lea        eax, [eax + 8]}
@@ -2517,11 +2514,11 @@ __declspec(naked) void I422ToRGBARow_AVX2(
 // Read 4 UV from 422, upsample to 8 UV.  With 8 Alpha.
 #define READYUVA422 \
   __asm {                                                    \
-    __asm movd       xmm0, [esi] /* U */                              \
+    __asm movd       xmm3, [esi] /* U */                              \
     __asm movd       xmm1, [esi + edi] /* V */                              \
     __asm lea        esi,  [esi + 4]                                           \
-    __asm punpcklbw  xmm0, xmm1 /* UV */                             \
-    __asm punpcklwd  xmm0, xmm0 /* UVUV (upsample) */                \
+    __asm punpcklbw  xmm3, xmm1 /* UV */                             \
+    __asm punpcklwd  xmm3, xmm3 /* UVUV (upsample) */                \
     __asm movq       xmm4, qword ptr [eax] /* Y */                           \
     __asm punpcklbw  xmm4, xmm4                                                \
     __asm lea        eax, [eax + 8]                                            \
@@ -2531,9 +2528,9 @@ __declspec(naked) void I422ToRGBARow_AVX2(
 // Read 4 UV from NV12, upsample to 8 UV.
 #define READNV12 \
   __asm {                                                       \
-    __asm movq       xmm0, qword ptr [esi] /* UV */                            \
+    __asm movq       xmm3, qword ptr [esi] /* UV */                            \
     __asm lea        esi,  [esi + 8]                                           \
-    __asm punpcklwd  xmm0, xmm0 /* UVUV (upsample) */                \
+    __asm punpcklwd  xmm3, xmm3 /* UVUV (upsample) */                \
     __asm movq       xmm4, qword ptr [eax]                                     \
     __asm punpcklbw  xmm4, xmm4                                                \
     __asm lea        eax, [eax + 8]}
@@ -2541,9 +2538,9 @@ __declspec(naked) void I422ToRGBARow_AVX2(
 // Read 4 VU from NV21, upsample to 8 UV.
 #define READNV21 \
   __asm {                                                       \
-    __asm movq       xmm0, qword ptr [esi] /* UV */                            \
+    __asm movq       xmm3, qword ptr [esi] /* UV */                            \
     __asm lea        esi,  [esi + 8]                                           \
-    __asm pshufb     xmm0, xmmword ptr kShuffleNV21                            \
+    __asm pshufb     xmm3, xmmword ptr kShuffleNV21                            \
     __asm movq       xmm4, qword ptr [eax]                                     \
     __asm punpcklbw  xmm4, xmm4                                                \
     __asm lea        eax, [eax + 8]}
@@ -2553,8 +2550,8 @@ __declspec(naked) void I422ToRGBARow_AVX2(
   __asm {                                                       \
     __asm movdqu     xmm4, [eax] /* YUY2 */                           \
     __asm pshufb     xmm4, xmmword ptr kShuffleYUY2Y                           \
-    __asm movdqu     xmm0, [eax] /* UV */                             \
-    __asm pshufb     xmm0, xmmword ptr kShuffleYUY2UV                          \
+    __asm movdqu     xmm3, [eax] /* UV */                             \
+    __asm pshufb     xmm3, xmmword ptr kShuffleYUY2UV                          \
     __asm lea        eax, [eax + 16]}
 
 // Read 4 UYVY with 8 Y and upsample 4 UV to 8 UV.
@@ -2562,35 +2559,33 @@ __declspec(naked) void I422ToRGBARow_AVX2(
   __asm {                                                       \
     __asm movdqu     xmm4, [eax] /* UYVY */                           \
     __asm pshufb     xmm4, xmmword ptr kShuffleUYVYY                           \
-    __asm movdqu     xmm0, [eax] /* UV */                             \
-    __asm pshufb     xmm0, xmmword ptr kShuffleUYVYUV                          \
+    __asm movdqu     xmm3, [eax] /* UV */                             \
+    __asm pshufb     xmm3, xmmword ptr kShuffleUYVYUV                          \
     __asm lea        eax, [eax + 16]}
 
 // Convert 8 pixels: 8 UV and 8 Y.
 #define YUVTORGB(YuvConstants) \
-  __asm {                                         \
-    __asm movdqa     xmm1, xmm0                                                \
-    __asm movdqa     xmm2, xmm0                                                \
-    __asm movdqa     xmm3, xmm0                                                \
-    __asm movdqa     xmm0, xmmword ptr [YuvConstants + KUVBIASB]               \
-    __asm pmaddubsw  xmm1, xmmword ptr [YuvConstants + KUVTOB]                 \
-    __asm psubw      xmm0, xmm1                                                \
-    __asm movdqa     xmm1, xmmword ptr [YuvConstants + KUVBIASG]               \
-    __asm pmaddubsw  xmm2, xmmword ptr [YuvConstants + KUVTOG]                 \
-    __asm psubw      xmm1, xmm2                                                \
-    __asm movdqa     xmm2, xmmword ptr [YuvConstants + KUVBIASR]               \
-    __asm pmaddubsw  xmm3, xmmword ptr [YuvConstants + KUVTOR]                 \
-    __asm psubw      xmm2, xmm3                                                \
+  __asm {                                                                      \
+    __asm psubb      xmm3, xmmword ptr kBiasUV128                              \
     __asm pmulhuw    xmm4, xmmword ptr [YuvConstants + KYTORGB]                \
-    __asm paddsw     xmm0, xmm4 /* B += Y */                         \
-    __asm paddsw     xmm1, xmm4 /* G += Y */                         \
-    __asm paddsw     xmm2, xmm4 /* R += Y */                         \
+    __asm movdqa     xmm0, xmmword ptr [YuvConstants + KUVTOB]                 \
+    __asm movdqa     xmm1, xmmword ptr [YuvConstants + KUVTOG]                 \
+    __asm movdqa     xmm2, xmmword ptr [YuvConstants + KUVTOR]                 \
+    __asm pmaddubsw  xmm0, xmm3                                                \
+    __asm pmaddubsw  xmm1, xmm3                                                \
+    __asm pmaddubsw  xmm2, xmm3                                                \
+    __asm movdqa     xmm3, xmmword ptr [YuvConstants + KYBIASTORGB]            \
+    __asm paddw      xmm4, xmm3                                                \
+    __asm paddsw     xmm0, xmm4                                                \
+    __asm paddsw     xmm2, xmm4                                                \
+    __asm psubsw     xmm4, xmm1                                                \
+    __asm movdqa     xmm1, xmm4                                                \
     __asm psraw      xmm0, 6                                                   \
     __asm psraw      xmm1, 6                                                   \
     __asm psraw      xmm2, 6                                                   \
-    __asm packuswb   xmm0, xmm0 /* B */                              \
-    __asm packuswb   xmm1, xmm1 /* G */                              \
-    __asm packuswb   xmm2, xmm2 /* R */             \
+    __asm packuswb   xmm0, xmm0 /* B */                                        \
+    __asm packuswb   xmm1, xmm1 /* G */                                        \
+    __asm packuswb   xmm2, xmm2 /* R */                                        \
   }
 
 // Store 8 ARGB values.
@@ -4347,13 +4342,13 @@ static const uvec8 kShuffleAlpha = {3u,  0x80, 3u,  0x80, 7u,  0x80, 7u,  0x80,
                                     11u, 0x80, 11u, 0x80, 15u, 0x80, 15u, 0x80};
 
 // Blend 8 pixels at a time.
-__declspec(naked) void ARGBBlendRow_SSSE3(const uint8_t* src_argb0,
+__declspec(naked) void ARGBBlendRow_SSSE3(const uint8_t* src_argb,
                                           const uint8_t* src_argb1,
                                           uint8_t* dst_argb,
                                           int width) {
   __asm {
     push       esi
-    mov        eax, [esp + 4 + 4]  // src_argb0
+    mov        eax, [esp + 4 + 4]  // src_argb
     mov        esi, [esp + 4 + 8]  // src_argb1
     mov        edx, [esp + 4 + 12]  // dst_argb
     mov        ecx, [esp + 4 + 16]  // width
@@ -4442,7 +4437,7 @@ __declspec(naked) void ARGBAttenuateRow_SSSE3(const uint8_t* src_argb,
                                               uint8_t* dst_argb,
                                               int width) {
   __asm {
-    mov        eax, [esp + 4]  // src_argb0
+    mov        eax, [esp + 4]  // src_argb
     mov        edx, [esp + 8]  // dst_argb
     mov        ecx, [esp + 12]  // width
     pcmpeqb    xmm3, xmm3  // generate mask 0xff000000
@@ -4487,7 +4482,7 @@ __declspec(naked) void ARGBAttenuateRow_AVX2(const uint8_t* src_argb,
                                              uint8_t* dst_argb,
                                              int width) {
   __asm {
-    mov        eax, [esp + 4]  // src_argb0
+    mov        eax, [esp + 4]  // src_argb
     mov        edx, [esp + 8]  // dst_argb
     mov        ecx, [esp + 12]  // width
     sub        edx, eax
@@ -4581,7 +4576,7 @@ __declspec(naked) void ARGBUnattenuateRow_AVX2(const uint8_t* src_argb,
                                                uint8_t* dst_argb,
                                                int width) {
   __asm {
-    mov        eax, [esp + 4]  // src_argb0
+    mov        eax, [esp + 4]  // src_argb
     mov        edx, [esp + 8]  // dst_argb
     mov        ecx, [esp + 12]  // width
     sub        edx, eax
@@ -4937,20 +4932,20 @@ __declspec(naked) void ARGBShadeRow_SSE2(const uint8_t* src_argb,
 
 #ifdef HAS_ARGBMULTIPLYROW_SSE2
 // Multiply 2 rows of ARGB pixels together, 4 pixels at a time.
-__declspec(naked) void ARGBMultiplyRow_SSE2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBMultiplyRow_SSE2(const uint8_t* src_argb,
                                             const uint8_t* src_argb1,
                                             uint8_t* dst_argb,
                                             int width) {
   __asm {
     push       esi
-    mov        eax, [esp + 4 + 4]  // src_argb0
+    mov        eax, [esp + 4 + 4]  // src_argb
     mov        esi, [esp + 4 + 8]  // src_argb1
     mov        edx, [esp + 4 + 12]  // dst_argb
     mov        ecx, [esp + 4 + 16]  // width
     pxor       xmm5, xmm5  // constant 0
 
  convertloop:
-    movdqu     xmm0, [eax]  // read 4 pixels from src_argb0
+    movdqu     xmm0, [eax]  // read 4 pixels from src_argb
     movdqu     xmm2, [esi]  // read 4 pixels from src_argb1
     movdqu     xmm1, xmm0
     movdqu     xmm3, xmm2
@@ -4958,8 +4953,8 @@ __declspec(naked) void ARGBMultiplyRow_SSE2(const uint8_t* src_argb0,
     punpckhbw  xmm1, xmm1  // next 2
     punpcklbw  xmm2, xmm5  // first 2
     punpckhbw  xmm3, xmm5  // next 2
-    pmulhuw    xmm0, xmm2  // src_argb0 * src_argb1 first 2
-    pmulhuw    xmm1, xmm3  // src_argb0 * src_argb1 next 2
+    pmulhuw    xmm0, xmm2  // src_argb * src_argb1 first 2
+    pmulhuw    xmm1, xmm3  // src_argb * src_argb1 next 2
     lea        eax, [eax + 16]
     lea        esi, [esi + 16]
     packuswb   xmm0, xmm1
@@ -4977,13 +4972,13 @@ __declspec(naked) void ARGBMultiplyRow_SSE2(const uint8_t* src_argb0,
 #ifdef HAS_ARGBADDROW_SSE2
 // Add 2 rows of ARGB pixels together, 4 pixels at a time.
 // TODO(fbarchard): Port this to posix, neon and other math functions.
-__declspec(naked) void ARGBAddRow_SSE2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBAddRow_SSE2(const uint8_t* src_argb,
                                        const uint8_t* src_argb1,
                                        uint8_t* dst_argb,
                                        int width) {
   __asm {
     push       esi
-    mov        eax, [esp + 4 + 4]  // src_argb0
+    mov        eax, [esp + 4 + 4]  // src_argb
     mov        esi, [esp + 4 + 8]  // src_argb1
     mov        edx, [esp + 4 + 12]  // dst_argb
     mov        ecx, [esp + 4 + 16]  // width
@@ -4992,11 +4987,11 @@ __declspec(naked) void ARGBAddRow_SSE2(const uint8_t* src_argb0,
     jl         convertloop49
 
  convertloop4:
-    movdqu     xmm0, [eax]  // read 4 pixels from src_argb0
+    movdqu     xmm0, [eax]  // read 4 pixels from src_argb
     lea        eax, [eax + 16]
     movdqu     xmm1, [esi]  // read 4 pixels from src_argb1
     lea        esi, [esi + 16]
-    paddusb    xmm0, xmm1  // src_argb0 + src_argb1
+    paddusb    xmm0, xmm1  // src_argb + src_argb1
     movdqu     [edx], xmm0
     lea        edx, [edx + 16]
     sub        ecx, 4
@@ -5007,11 +5002,11 @@ __declspec(naked) void ARGBAddRow_SSE2(const uint8_t* src_argb0,
     jl         convertloop19
 
  convertloop1:
-    movd       xmm0, [eax]  // read 1 pixels from src_argb0
+    movd       xmm0, [eax]  // read 1 pixels from src_argb
     lea        eax, [eax + 4]
     movd       xmm1, [esi]  // read 1 pixels from src_argb1
     lea        esi, [esi + 4]
-    paddusb    xmm0, xmm1  // src_argb0 + src_argb1
+    paddusb    xmm0, xmm1  // src_argb + src_argb1
     movd       [edx], xmm0
     lea        edx, [edx + 4]
     sub        ecx, 1
@@ -5026,23 +5021,23 @@ __declspec(naked) void ARGBAddRow_SSE2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBSUBTRACTROW_SSE2
 // Subtract 2 rows of ARGB pixels together, 4 pixels at a time.
-__declspec(naked) void ARGBSubtractRow_SSE2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBSubtractRow_SSE2(const uint8_t* src_argb,
                                             const uint8_t* src_argb1,
                                             uint8_t* dst_argb,
                                             int width) {
   __asm {
     push       esi
-    mov        eax, [esp + 4 + 4]  // src_argb0
+    mov        eax, [esp + 4 + 4]  // src_argb
     mov        esi, [esp + 4 + 8]  // src_argb1
     mov        edx, [esp + 4 + 12]  // dst_argb
     mov        ecx, [esp + 4 + 16]  // width
 
  convertloop:
-    movdqu     xmm0, [eax]  // read 4 pixels from src_argb0
+    movdqu     xmm0, [eax]  // read 4 pixels from src_argb
     lea        eax, [eax + 16]
     movdqu     xmm1, [esi]  // read 4 pixels from src_argb1
     lea        esi, [esi + 16]
-    psubusb    xmm0, xmm1  // src_argb0 - src_argb1
+    psubusb    xmm0, xmm1  // src_argb - src_argb1
     movdqu     [edx], xmm0
     lea        edx, [edx + 16]
     sub        ecx, 4
@@ -5056,20 +5051,20 @@ __declspec(naked) void ARGBSubtractRow_SSE2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBMULTIPLYROW_AVX2
 // Multiply 2 rows of ARGB pixels together, 8 pixels at a time.
-__declspec(naked) void ARGBMultiplyRow_AVX2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBMultiplyRow_AVX2(const uint8_t* src_argb,
                                             const uint8_t* src_argb1,
                                             uint8_t* dst_argb,
                                             int width) {
   __asm {
     push       esi
-    mov        eax, [esp + 4 + 4]  // src_argb0
+    mov        eax, [esp + 4 + 4]  // src_argb
     mov        esi, [esp + 4 + 8]  // src_argb1
     mov        edx, [esp + 4 + 12]  // dst_argb
     mov        ecx, [esp + 4 + 16]  // width
     vpxor      ymm5, ymm5, ymm5  // constant 0
 
  convertloop:
-    vmovdqu    ymm1, [eax]  // read 8 pixels from src_argb0
+    vmovdqu    ymm1, [eax]  // read 8 pixels from src_argb
     lea        eax, [eax + 32]
     vmovdqu    ymm3, [esi]  // read 8 pixels from src_argb1
     lea        esi, [esi + 32]
@@ -5077,8 +5072,8 @@ __declspec(naked) void ARGBMultiplyRow_AVX2(const uint8_t* src_argb0,
     vpunpckhbw ymm1, ymm1, ymm1  // high 4
     vpunpcklbw ymm2, ymm3, ymm5  // low 4
     vpunpckhbw ymm3, ymm3, ymm5  // high 4
-    vpmulhuw   ymm0, ymm0, ymm2  // src_argb0 * src_argb1 low 4
-    vpmulhuw   ymm1, ymm1, ymm3  // src_argb0 * src_argb1 high 4
+    vpmulhuw   ymm0, ymm0, ymm2  // src_argb * src_argb1 low 4
+    vpmulhuw   ymm1, ymm1, ymm3  // src_argb * src_argb1 high 4
     vpackuswb  ymm0, ymm0, ymm1
     vmovdqu    [edx], ymm0
     lea        edx, [edx + 32]
@@ -5094,19 +5089,19 @@ __declspec(naked) void ARGBMultiplyRow_AVX2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBADDROW_AVX2
 // Add 2 rows of ARGB pixels together, 8 pixels at a time.
-__declspec(naked) void ARGBAddRow_AVX2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBAddRow_AVX2(const uint8_t* src_argb,
                                        const uint8_t* src_argb1,
                                        uint8_t* dst_argb,
                                        int width) {
   __asm {
     push       esi
-    mov        eax, [esp + 4 + 4]  // src_argb0
+    mov        eax, [esp + 4 + 4]  // src_argb
     mov        esi, [esp + 4 + 8]  // src_argb1
     mov        edx, [esp + 4 + 12]  // dst_argb
     mov        ecx, [esp + 4 + 16]  // width
 
  convertloop:
-    vmovdqu    ymm0, [eax]  // read 8 pixels from src_argb0
+    vmovdqu    ymm0, [eax]  // read 8 pixels from src_argb
     lea        eax, [eax + 32]
     vpaddusb   ymm0, ymm0, [esi]  // add 8 pixels from src_argb1
     lea        esi, [esi + 32]
@@ -5124,21 +5119,21 @@ __declspec(naked) void ARGBAddRow_AVX2(const uint8_t* src_argb0,
 
 #ifdef HAS_ARGBSUBTRACTROW_AVX2
 // Subtract 2 rows of ARGB pixels together, 8 pixels at a time.
-__declspec(naked) void ARGBSubtractRow_AVX2(const uint8_t* src_argb0,
+__declspec(naked) void ARGBSubtractRow_AVX2(const uint8_t* src_argb,
                                             const uint8_t* src_argb1,
                                             uint8_t* dst_argb,
                                             int width) {
   __asm {
     push       esi
-    mov        eax, [esp + 4 + 4]  // src_argb0
+    mov        eax, [esp + 4 + 4]  // src_argb
     mov        esi, [esp + 4 + 8]  // src_argb1
     mov        edx, [esp + 4 + 12]  // dst_argb
     mov        ecx, [esp + 4 + 16]  // width
 
  convertloop:
-    vmovdqu    ymm0, [eax]  // read 8 pixels from src_argb0
+    vmovdqu    ymm0, [eax]  // read 8 pixels from src_argb
     lea        eax, [eax + 32]
-    vpsubusb   ymm0, ymm0, [esi]  // src_argb0 - src_argb1
+    vpsubusb   ymm0, ymm0, [esi]  // src_argb - src_argb1
     lea        esi, [esi + 32]
     vmovdqu    [edx], ymm0
     lea        edx, [edx + 32]
diff --git a/source/scale.cc b/source/scale.cc
index a254737c..03b0486f 100644
--- a/source/scale.cc
+++ b/source/scale.cc
@@ -1446,7 +1446,8 @@ void ScalePlaneUp2_Bilinear(int src_width,
   for (x = 0; x < src_height - 1; ++x) {
     Scale2RowUp(src_ptr, src_stride, dst_ptr, dst_stride, dst_width);
     src_ptr += src_stride;
-    // TODO: Test performance of writing one row of destination at a time.
+    // TODO(fbarchard): Test performance of writing one row of destination at a
+    // time.
     dst_ptr += 2 * dst_stride;
   }
   if (!(dst_height & 1)) {
diff --git a/source/scale_gcc.cc b/source/scale_gcc.cc
index b1d39cf8..ebc6deb1 100644
--- a/source/scale_gcc.cc
+++ b/source/scale_gcc.cc
@@ -18,7 +18,7 @@ extern "C" {
 
 // This module is for GCC x86 and x64.
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 
 // Offsets for source bytes 0 to 9
 static const uvec8 kShuf0 = {0,   1,   3,   4,   5,   7,   8,   9,
diff --git a/source/scale_neon64.cc b/source/scale_neon64.cc
index cde4ee39..9f9636e6 100644
--- a/source/scale_neon64.cc
+++ b/source/scale_neon64.cc
@@ -601,8 +601,8 @@ void ScaleRowUp2_Bilinear_NEON(const uint8_t* src_ptr,
       "umlal       v4.8h, v1.8b, v31.8b          \n"  // 3*near+far (2, odd)
       "umlal       v5.8h, v0.8b, v31.8b          \n"  // 3*near+far (2, even)
 
-      "mov         v0.8h, v4.8h                  \n"
-      "mov         v1.8h, v5.8h                  \n"
+      "mov         v0.16b, v4.16b                \n"
+      "mov         v1.16b, v5.16b                \n"
       "mla         v4.8h, v2.8h, v30.8h          \n"  // 9 3 3 1 (1, odd)
       "mla         v5.8h, v3.8h, v30.8h          \n"  // 9 3 3 1 (1, even)
       "mla         v2.8h, v0.8h, v30.8h          \n"  // 9 3 3 1 (2, odd)
@@ -642,7 +642,7 @@ void ScaleRowUp2_Linear_12_NEON(const uint16_t* src_ptr,
       "ld1         {v1.8h}, [%1], #16            \n"  // 12345678 (16b)
       "prfm        pldl1keep, [%0, 448]          \n"  // prefetch 7 lines ahead
 
-      "mov         v2.8h, v0.8h                  \n"
+      "mov         v2.16b, v0.16b                \n"
       "mla         v0.8h, v1.8h, v31.8h          \n"  // 3*near+far (odd)
       "mla         v1.8h, v2.8h, v31.8h          \n"  // 3*near+far (even)
 
@@ -679,7 +679,7 @@ void ScaleRowUp2_Bilinear_12_NEON(const uint16_t* src_ptr,
       "ld1         {v3.8h}, [%2], #16            \n"  // 12345678 (16b)
       "prfm        pldl1keep, [%0, 448]          \n"  // prefetch 7 lines ahead
 
-      "mov         v0.8h, v2.8h                  \n"
+      "mov         v0.16b, v2.16b                \n"
       "mla         v2.8h, v3.8h, v31.8h          \n"  // 3*near+far (odd)
       "mla         v3.8h, v0.8h, v31.8h          \n"  // 3*near+far (even)
 
@@ -687,12 +687,12 @@ void ScaleRowUp2_Bilinear_12_NEON(const uint16_t* src_ptr,
       "ld1         {v5.8h}, [%3], #16            \n"  // 12345678 (16b)
       "prfm        pldl1keep, [%1, 448]          \n"  // prefetch 7 lines ahead
 
-      "mov         v0.8h, v4.8h                  \n"
+      "mov         v0.16b, v4.16b                \n"
       "mla         v4.8h, v5.8h, v31.8h          \n"  // 3*near+far (odd)
       "mla         v5.8h, v0.8h, v31.8h          \n"  // 3*near+far (even)
 
-      "mov         v0.8h, v4.8h                  \n"
-      "mov         v1.8h, v5.8h                  \n"
+      "mov         v0.16b, v4.16b                \n"
+      "mov         v1.16b, v5.16b                \n"
       "mla         v4.8h, v2.8h, v31.8h          \n"  // 9 3 3 1 (1, odd)
       "mla         v5.8h, v3.8h, v31.8h          \n"  // 9 3 3 1 (1, even)
       "mla         v2.8h, v0.8h, v31.8h          \n"  // 9 3 3 1 (2, odd)
@@ -791,8 +791,8 @@ void ScaleRowUp2_Bilinear_16_NEON(const uint16_t* src_ptr,
       "umlal       v4.4s, v1.4h, v31.4h          \n"  // 3*near+far (2, odd)
       "umlal       v5.4s, v0.4h, v31.4h          \n"  // 3*near+far (2, even)
 
-      "mov         v0.4s, v4.4s                  \n"
-      "mov         v1.4s, v5.4s                  \n"
+      "mov         v0.16b, v4.16b                \n"
+      "mov         v1.16b, v5.16b                \n"
       "mla         v4.4s, v2.4s, v30.4s          \n"  // 9 3 3 1 (1, odd)
       "mla         v5.4s, v3.4s, v30.4s          \n"  // 9 3 3 1 (1, even)
       "mla         v2.4s, v0.4s, v30.4s          \n"  // 9 3 3 1 (2, odd)
@@ -887,8 +887,8 @@ void ScaleUVRowUp2_Bilinear_NEON(const uint8_t* src_ptr,
       "umlal       v4.8h, v1.8b, v31.8b          \n"  // 3*near+far (2, odd)
       "umlal       v5.8h, v0.8b, v31.8b          \n"  // 3*near+far (2, even)
 
-      "mov         v0.8h, v4.8h                  \n"
-      "mov         v1.8h, v5.8h                  \n"
+      "mov         v0.16b, v4.16b                \n"
+      "mov         v1.16b, v5.16b                \n"
       "mla         v4.8h, v2.8h, v30.8h          \n"  // 9 3 3 1 (1, odd)
       "mla         v5.8h, v3.8h, v30.8h          \n"  // 9 3 3 1 (1, even)
       "mla         v2.8h, v0.8h, v30.8h          \n"  // 9 3 3 1 (2, odd)
@@ -988,8 +988,8 @@ void ScaleUVRowUp2_Bilinear_16_NEON(const uint16_t* src_ptr,
       "umlal       v4.4s, v1.4h, v31.4h          \n"  // 3*near+far (2, odd)
       "umlal       v5.4s, v0.4h, v31.4h          \n"  // 3*near+far (2, even)
 
-      "mov         v0.4s, v4.4s                  \n"
-      "mov         v1.4s, v5.4s                  \n"
+      "mov         v0.16b, v4.16b                \n"
+      "mov         v1.16b, v5.16b                \n"
       "mla         v4.4s, v2.4s, v30.4s          \n"  // 9 3 3 1 (1, odd)
       "mla         v5.4s, v3.4s, v30.4s          \n"  // 9 3 3 1 (1, even)
       "mla         v2.4s, v0.4s, v30.4s          \n"  // 9 3 3 1 (2, odd)
diff --git a/source/scale_uv.cc b/source/scale_uv.cc
index 7b977912..d9a31445 100644
--- a/source/scale_uv.cc
+++ b/source/scale_uv.cc
@@ -746,7 +746,8 @@ void ScaleUVBilinearUp2(int src_width,
   for (x = 0; x < src_height - 1; ++x) {
     Scale2RowUp(src_ptr, src_stride, dst_ptr, dst_stride, dst_width);
     src_ptr += src_stride;
-    // TODO: Test performance of writing one row of destination at a time.
+    // TODO(fbarchard): Test performance of writing one row of destination at a
+    // time.
     dst_ptr += 2 * dst_stride;
   }
   if (!(dst_height & 1)) {
@@ -851,7 +852,8 @@ void ScaleUVBilinearUp2_16(int src_width,
   for (x = 0; x < src_height - 1; ++x) {
     Scale2RowUp(src_ptr, src_stride, dst_ptr, dst_stride, dst_width);
     src_ptr += src_stride;
-    // TODO: Test performance of writing one row of destination at a time.
+    // TODO(fbarchard): Test performance of writing one row of destination at a
+    // time.
     dst_ptr += 2 * dst_stride;
   }
   if (!(dst_height & 1)) {
diff --git a/source/scale_win.cc b/source/scale_win.cc
index c5fc86f3..ea1f95c6 100644
--- a/source/scale_win.cc
+++ b/source/scale_win.cc
@@ -16,8 +16,9 @@ namespace libyuv {
 extern "C" {
 #endif
 
-// This module is for 32 bit Visual C x86 and clangcl
-#if !defined(LIBYUV_DISABLE_X86) && defined(_M_IX86) && defined(_MSC_VER)
+// This module is for 32 bit Visual C x86
+#if !defined(LIBYUV_DISABLE_X86) && defined(_MSC_VER) && \
+    !defined(__clang__) && defined(_M_IX86)
 
 // Offsets for source bytes 0 to 9
 static const uvec8 kShuf0 = {0,   1,   3,   4,   5,   7,   8,   9,
diff --git a/tools_libyuv/autoroller/roll_deps.py b/tools_libyuv/autoroller/roll_deps.py
index 9b9660de..5b3cf8d6 100755
--- a/tools_libyuv/autoroller/roll_deps.py
+++ b/tools_libyuv/autoroller/roll_deps.py
@@ -46,7 +46,7 @@ CHECKOUT_SRC_DIR = os.path.realpath(os.path.join(SCRIPT_DIR, os.pardir,
 CHECKOUT_ROOT_DIR = os.path.realpath(os.path.join(CHECKOUT_SRC_DIR, os.pardir))
 
 sys.path.append(os.path.join(CHECKOUT_SRC_DIR, 'build'))
-import find_depot_tools
+import find_depot_tools  # pylint: disable=wrong-import-position
 find_depot_tools.add_depot_tools_to_path()
 
 CLANG_UPDATE_SCRIPT_URL_PATH = 'tools/clang/scripts/update.py'
@@ -366,12 +366,12 @@ def _IsTreeClean():
 def _EnsureUpdatedMasterBranch(dry_run):
   current_branch = _RunCommand(
       ['git', 'rev-parse', '--abbrev-ref', 'HEAD'])[0].splitlines()[0]
-  if current_branch != 'master':
-    logging.error('Please checkout the master branch and re-run this script.')
+  if current_branch != 'main':
+    logging.error('Please checkout the main branch and re-run this script.')
     if not dry_run:
       sys.exit(-1)
 
-  logging.info('Updating master branch...')
+  logging.info('Updating main branch...')
   _RunCommand(['git', 'pull'])
 
 
@@ -384,7 +384,7 @@ def _CreateRollBranch(dry_run):
 def _RemovePreviousRollBranch(dry_run):
   active_branch, branches = _GetBranches()
   if active_branch == ROLL_BRANCH_NAME:
-    active_branch = 'master'
+    active_branch = 'main'
   if ROLL_BRANCH_NAME in branches:
     logging.info('Removing previous roll branch (%s)', ROLL_BRANCH_NAME)
     if not dry_run:
@@ -444,7 +444,7 @@ def main():
                        'tryjobs.'))
   p.add_argument('-i', '--ignore-unclean-workdir', action='store_true',
                  default=False,
-                 help=('Ignore if the current branch is not master or if there '
+                 help=('Ignore if the current branch is not main or if there '
                        'are uncommitted changes (default: %(default)s).'))
   grp = p.add_mutually_exclusive_group()
   grp.add_argument('--skip-cq', action='store_true', default=False,
diff --git a/tools_libyuv/autoroller/unittests/roll_deps_test.py b/tools_libyuv/autoroller/unittests/roll_deps_test.py
index 477b6e40..a7e3f8a8 100755
--- a/tools_libyuv/autoroller/unittests/roll_deps_test.py
+++ b/tools_libyuv/autoroller/unittests/roll_deps_test.py
@@ -18,9 +18,10 @@ import unittest
 SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
 PARENT_DIR = os.path.join(SCRIPT_DIR, os.pardir)
 sys.path.append(PARENT_DIR)
-import roll_deps
+import roll_deps  # pylint: disable=wrong-import-position
 from roll_deps import CalculateChangedDeps, GetMatchingDepsEntries, \
-  ParseDepsDict, ParseLocalDepsFile, UpdateDepsFile
+  ParseDepsDict, ParseLocalDepsFile, \
+  UpdateDepsFile  # pylint: disable=wrong-import-position
 
 
 TEST_DATA_VARS = {
diff --git a/tools_libyuv/valgrind/chrome_tests.bat b/tools_libyuv/valgrind/chrome_tests.bat
deleted file mode 100755
index 9d4c8ca8..00000000
--- a/tools_libyuv/valgrind/chrome_tests.bat
+++ /dev/null
@@ -1,53 +0,0 @@
-@echo off
-:: Copyright (c) 2011 The Chromium Authors. All rights reserved.
-:: Use of this source code is governed by a BSD-style license that can be
-:: found in the LICENSE file.
-
-setlocal
-
-set THISDIR=%~dp0
-set TOOL_NAME="unknown"
-
-:: Get the tool name and put it into TOOL_NAME {{{1
-:: NB: SHIFT command doesn't modify %*
-:PARSE_ARGS_LOOP
-  if %1 == () GOTO:TOOLNAME_NOT_FOUND
-  if %1 == --tool GOTO:TOOLNAME_FOUND
-  SHIFT
-  goto :PARSE_ARGS_LOOP
-
-:TOOLNAME_NOT_FOUND
-echo "Please specify a tool (e.g. drmemory) by using --tool flag"
-exit /B 1
-
-:TOOLNAME_FOUND
-SHIFT
-set TOOL_NAME=%1
-:: }}}
-if "%TOOL_NAME%" == "drmemory"          GOTO :SETUP_DRMEMORY
-if "%TOOL_NAME%" == "drmemory_light"    GOTO :SETUP_DRMEMORY
-if "%TOOL_NAME%" == "drmemory_full"     GOTO :SETUP_DRMEMORY
-if "%TOOL_NAME%" == "drmemory_pattern"  GOTO :SETUP_DRMEMORY
-echo "Unknown tool: `%TOOL_NAME%`! Only drmemory is supported right now"
-exit /B 1
-
-:SETUP_DRMEMORY
-:: Set up DRMEMORY_COMMAND to invoke Dr. Memory {{{1
-set DRMEMORY_PATH=%THISDIR%..\..\third_party\drmemory
-set DRMEMORY_SFX=%DRMEMORY_PATH%\drmemory-windows-sfx.exe
-if EXIST %DRMEMORY_SFX% GOTO DRMEMORY_BINARY_OK
-echo "Can't find Dr. Memory executables."
-echo "See http://www.chromium.org/developers/how-tos/using-valgrind/dr-memory"
-echo "for the instructions on how to get them."
-exit /B 1
-
-:DRMEMORY_BINARY_OK
-%DRMEMORY_SFX% -o%DRMEMORY_PATH%\unpacked -y
-set DRMEMORY_COMMAND=%DRMEMORY_PATH%\unpacked\bin\drmemory.exe
-:: }}}
-goto :RUN_TESTS
-
-:RUN_TESTS
-set PYTHONPATH=%THISDIR%../python/google
-set RUNNING_ON_VALGRIND=yes
-python %THISDIR%/chrome_tests.py %*
diff --git a/tools_libyuv/valgrind/chrome_tests.py b/tools_libyuv/valgrind/chrome_tests.py
deleted file mode 100755
index fe899bce..00000000
--- a/tools_libyuv/valgrind/chrome_tests.py
+++ /dev/null
@@ -1,869 +0,0 @@
-#!/usr/bin/env python
-# Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-''' Runs various chrome tests through valgrind_test.py.'''
-
-import glob
-import logging
-import multiprocessing
-import optparse
-import os
-import stat
-import subprocess
-import sys
-
-import logging_utils
-import path_utils
-
-import common
-import valgrind_test
-
-class TestNotFound(Exception): pass
-
-class MultipleGTestFiltersSpecified(Exception): pass
-
-class BuildDirNotFound(Exception): pass
-
-class BuildDirAmbiguous(Exception): pass
-
-class ExecutableNotFound(Exception): pass
-
-class BadBinary(Exception): pass
-
-class ChromeTests:
-  SLOW_TOOLS = ["memcheck", "drmemory"]
-  LAYOUT_TESTS_DEFAULT_CHUNK_SIZE = 300
-
-  def __init__(self, options, args, test):
-    if ':' in test:
-      (self._test, self._gtest_filter) = test.split(':', 1)
-    else:
-      self._test = test
-      self._gtest_filter = options.gtest_filter
-
-    if self._test not in self._test_list:
-      raise TestNotFound("Unknown test: %s" % test)
-
-    if options.gtest_filter and options.gtest_filter != self._gtest_filter:
-      raise MultipleGTestFiltersSpecified("Can not specify both --gtest_filter "
-                                          "and --test %s" % test)
-
-    self._options = options
-    self._args = args
-
-    script_dir = path_utils.ScriptDir()
-    # Compute the top of the tree (the "source dir") from the script dir (where
-    # this script lives).  We assume that the script dir is in tools/valgrind/
-    # relative to the top of the tree.
-    self._source_dir = os.path.dirname(os.path.dirname(script_dir))
-    # since this path is used for string matching, make sure it's always
-    # an absolute Unix-style path
-    self._source_dir = os.path.abspath(self._source_dir).replace('\\', '/')
-    valgrind_test_script = os.path.join(script_dir, "valgrind_test.py")
-    self._command_preamble = ["--source-dir=%s" % (self._source_dir)]
-
-    if not self._options.build_dir:
-      dirs = [
-        os.path.join(self._source_dir, "xcodebuild", "Debug"),
-        os.path.join(self._source_dir, "out", "Debug"),
-        os.path.join(self._source_dir, "build", "Debug"),
-      ]
-      build_dir = [d for d in dirs if os.path.isdir(d)]
-      if len(build_dir) > 1:
-        raise BuildDirAmbiguous("Found more than one suitable build dir:\n"
-                                "%s\nPlease specify just one "
-                                "using --build-dir" % ", ".join(build_dir))
-      elif build_dir:
-        self._options.build_dir = build_dir[0]
-      else:
-        self._options.build_dir = None
-
-    if self._options.build_dir:
-      build_dir = os.path.abspath(self._options.build_dir)
-      self._command_preamble += ["--build-dir=%s" % (self._options.build_dir)]
-
-  def _EnsureBuildDirFound(self):
-    if not self._options.build_dir:
-      raise BuildDirNotFound("Oops, couldn't find a build dir, please "
-                             "specify it manually using --build-dir")
-
-  def _DefaultCommand(self, tool, exe=None, valgrind_test_args=None):
-    '''Generates the default command array that most tests will use.'''
-    if exe and common.IsWindows():
-      exe += '.exe'
-
-    cmd = list(self._command_preamble)
-
-    # Find all suppressions matching the following pattern:
-    # tools/valgrind/TOOL/suppressions[_PLATFORM].txt
-    # and list them with --suppressions= prefix.
-    script_dir = path_utils.ScriptDir()
-    tool_name = tool.ToolName();
-    suppression_file = os.path.join(script_dir, tool_name, "suppressions.txt")
-    if os.path.exists(suppression_file):
-      cmd.append("--suppressions=%s" % suppression_file)
-    # Platform-specific suppression
-    for platform in common.PlatformNames():
-      platform_suppression_file = \
-          os.path.join(script_dir, tool_name, 'suppressions_%s.txt' % platform)
-      if os.path.exists(platform_suppression_file):
-        cmd.append("--suppressions=%s" % platform_suppression_file)
-
-    if tool_name == "drmemory":
-      if self._options.drmemory_ops:
-        # prepending " " to avoid Dr. Memory's option confusing optparse
-        cmd += ["--drmemory_ops", " " + self._options.drmemory_ops]
-
-    if self._options.valgrind_tool_flags:
-      cmd += self._options.valgrind_tool_flags.split(" ")
-    if self._options.keep_logs:
-      cmd += ["--keep_logs"]
-    if valgrind_test_args != None:
-      for arg in valgrind_test_args:
-        cmd.append(arg)
-    if exe:
-      self._EnsureBuildDirFound()
-      exe_path = os.path.join(self._options.build_dir, exe)
-      if not os.path.exists(exe_path):
-        raise ExecutableNotFound("Couldn't find '%s'" % exe_path)
-
-      # Make sure we don't try to test ASan-built binaries
-      # with other dynamic instrumentation-based tools.
-      # TODO(timurrrr): also check TSan and MSan?
-      # `nm` might not be available, so use try-except.
-      try:
-        # Do not perform this check on OS X, as 'nm' on 10.6 can't handle
-        # binaries built with Clang 3.5+.
-        if not common.IsMac():
-          nm_output = subprocess.check_output(["nm", exe_path])
-          if nm_output.find("__asan_init") != -1:
-            raise BadBinary("You're trying to run an executable instrumented "
-                            "with AddressSanitizer under %s. Please provide "
-                            "an uninstrumented executable." % tool_name)
-      except OSError:
-        pass
-
-      cmd.append(exe_path)
-      # Valgrind runs tests slowly, so slow tests hurt more; show elapased time
-      # so we can find the slowpokes.
-      cmd.append("--gtest_print_time")
-      # Built-in test launcher for gtest-based executables runs tests using
-      # multiple process by default. Force the single-process mode back.
-      cmd.append("--single-process-tests")
-    if self._options.gtest_repeat:
-      cmd.append("--gtest_repeat=%s" % self._options.gtest_repeat)
-    if self._options.gtest_shuffle:
-      cmd.append("--gtest_shuffle")
-    if self._options.gtest_break_on_failure:
-      cmd.append("--gtest_break_on_failure")
-    if self._options.test_launcher_bot_mode:
-      cmd.append("--test-launcher-bot-mode")
-    if self._options.test_launcher_total_shards is not None:
-      cmd.append("--test-launcher-total-shards=%d"
-                 % self._options.test_launcher_total_shards)
-    if self._options.test_launcher_shard_index is not None:
-      cmd.append("--test-launcher-shard-index=%d"
-                 % self._options.test_launcher_shard_index)
-    return cmd
-
-  def Run(self):
-    ''' Runs the test specified by command-line argument --test '''
-    logging.info("running test %s" % (self._test))
-    return self._test_list[self._test](self)
-
-  def _AppendGtestFilter(self, tool, name, cmd):
-    '''Append an appropriate --gtest_filter flag to the googletest binary
-       invocation.
-       If the user passed their own filter mentioning only one test, just use
-       it. Otherwise, filter out tests listed in the appropriate gtest_exclude
-       files.
-    '''
-    if (self._gtest_filter and
-        ":" not in self._gtest_filter and
-        "?" not in self._gtest_filter and
-        "*" not in self._gtest_filter):
-      cmd.append("--gtest_filter=%s" % self._gtest_filter)
-      return
-
-    filters = []
-    gtest_files_dir = os.path.join(path_utils.ScriptDir(), "gtest_exclude")
-
-    gtest_filter_files = [
-        os.path.join(gtest_files_dir, name + ".gtest-%s.txt" % tool.ToolName())]
-    # Use ".gtest.txt" files only for slow tools, as they now contain
-    # Valgrind- and Dr.Memory-specific filters.
-    # TODO(glider): rename the files to ".gtest_slow.txt"
-    if tool.ToolName() in ChromeTests.SLOW_TOOLS:
-      gtest_filter_files += [os.path.join(gtest_files_dir, name + ".gtest.txt")]
-    for platform_suffix in common.PlatformNames():
-      gtest_filter_files += [
-        os.path.join(gtest_files_dir, name + ".gtest_%s.txt" % platform_suffix),
-        os.path.join(gtest_files_dir, name + ".gtest-%s_%s.txt" % \
-            (tool.ToolName(), platform_suffix))]
-    logging.info("Reading gtest exclude filter files:")
-    for filename in gtest_filter_files:
-      # strip the leading absolute path (may be very long on the bot)
-      # and the following / or \.
-      readable_filename = filename.replace("\\", "/")  # '\' on Windows
-      readable_filename = readable_filename.replace(self._source_dir, "")[1:]
-      if not os.path.exists(filename):
-        logging.info("  \"%s\" - not found" % readable_filename)
-        continue
-      logging.info("  \"%s\" - OK" % readable_filename)
-      f = open(filename, 'r')
-      for line in f.readlines():
-        if line.startswith("#") or line.startswith("//") or line.isspace():
-          continue
-        line = line.rstrip()
-        test_prefixes = ["FLAKY", "FAILS"]
-        for p in test_prefixes:
-          # Strip prefixes from the test names.
-          line = line.replace(".%s_" % p, ".")
-        # Exclude the original test name.
-        filters.append(line)
-        if line[-2:] != ".*":
-          # List all possible prefixes if line doesn't end with ".*".
-          for p in test_prefixes:
-            filters.append(line.replace(".", ".%s_" % p))
-    # Get rid of duplicates.
-    filters = set(filters)
-    gtest_filter = self._gtest_filter
-    if len(filters):
-      if gtest_filter:
-        gtest_filter += ":"
-        if gtest_filter.find("-") < 0:
-          gtest_filter += "-"
-      else:
-        gtest_filter = "-"
-      gtest_filter += ":".join(filters)
-    if gtest_filter:
-      cmd.append("--gtest_filter=%s" % gtest_filter)
-
-  @staticmethod
-  def ShowTests():
-    test_to_names = {}
-    for name, test_function in ChromeTests._test_list.iteritems():
-      test_to_names.setdefault(test_function, []).append(name)
-
-    name_to_aliases = {}
-    for names in test_to_names.itervalues():
-      names.sort(key=lambda name: len(name))
-      name_to_aliases[names[0]] = names[1:]
-
-    print
-    print "Available tests:"
-    print "----------------"
-    for name, aliases in sorted(name_to_aliases.iteritems()):
-      if aliases:
-        print "   {} (aka {})".format(name, ', '.join(aliases))
-      else:
-        print "   {}".format(name)
-
-  def SetupLdPath(self, requires_build_dir):
-    if requires_build_dir:
-      self._EnsureBuildDirFound()
-    elif not self._options.build_dir:
-      return
-
-    # Append build_dir to LD_LIBRARY_PATH so external libraries can be loaded.
-    if (os.getenv("LD_LIBRARY_PATH")):
-      os.putenv("LD_LIBRARY_PATH", "%s:%s" % (os.getenv("LD_LIBRARY_PATH"),
-                                              self._options.build_dir))
-    else:
-      os.putenv("LD_LIBRARY_PATH", self._options.build_dir)
-
-  def SimpleTest(self, module, name, valgrind_test_args=None, cmd_args=None):
-    tool = valgrind_test.CreateTool(self._options.valgrind_tool)
-    cmd = self._DefaultCommand(tool, name, valgrind_test_args)
-    self._AppendGtestFilter(tool, name, cmd)
-    cmd.extend(['--test-tiny-timeout=1000'])
-    if cmd_args:
-      cmd.extend(cmd_args)
-
-    self.SetupLdPath(True)
-    return tool.Run(cmd, module)
-
-  def RunCmdLine(self):
-    tool = valgrind_test.CreateTool(self._options.valgrind_tool)
-    cmd = self._DefaultCommand(tool, None, self._args)
-    self.SetupLdPath(False)
-    return tool.Run(cmd, None)
-
-  def TestAccessibility(self):
-    return self.SimpleTest("accessibility", "accessibility_unittests")
-
-  def TestAddressInput(self):
-    return self.SimpleTest("addressinput", "libaddressinput_unittests")
-
-  def TestAngle(self):
-    return self.SimpleTest("angle", "angle_unittests")
-
-  def TestAppList(self):
-    return self.SimpleTest("app_list", "app_list_unittests")
-
-  def TestAsh(self):
-    return self.SimpleTest("ash", "ash_unittests")
-
-  def TestAura(self):
-    return self.SimpleTest("aura", "aura_unittests")
-
-  def TestBase(self):
-    return self.SimpleTest("base", "base_unittests")
-
-  def TestBlinkHeap(self):
-    return self.SimpleTest("blink_heap", "blink_heap_unittests")
-
-  def TestBlinkPlatform(self):
-    return self.SimpleTest("blink_platform", "blink_platform_unittests")
-
-  def TestCacheInvalidation(self):
-    return self.SimpleTest("cacheinvalidation", "cacheinvalidation_unittests")
-
-  def TestCast(self):
-    return self.SimpleTest("chrome", "cast_unittests")
-
-  def TestCC(self):
-    return self.SimpleTest("cc", "cc_unittests",
-                           cmd_args=[
-                               "--cc-layer-tree-test-long-timeout"])
-
-  def TestChromeApp(self):
-    return self.SimpleTest("chrome_app", "chrome_app_unittests")
-
-  def TestChromeElf(self):
-    return self.SimpleTest("chrome_elf", "chrome_elf_unittests")
-
-  def TestChromeDriver(self):
-    return self.SimpleTest("chromedriver", "chromedriver_unittests")
-
-  def TestChromeOS(self):
-    return self.SimpleTest("chromeos", "chromeos_unittests")
-
-  def TestComponents(self):
-    return self.SimpleTest("components", "components_unittests")
-
-  def TestCompositor(self):
-    return self.SimpleTest("compositor", "compositor_unittests")
-
-  def TestContent(self):
-    return self.SimpleTest("content", "content_unittests")
-
-  def TestCourgette(self):
-    return self.SimpleTest("courgette", "courgette_unittests")
-
-  def TestCrypto(self):
-    return self.SimpleTest("crypto", "crypto_unittests")
-
-  def TestDevice(self):
-    return self.SimpleTest("device", "device_unittests")
-
-  def TestDisplay(self):
-    return self.SimpleTest("display", "display_unittests")
-
-  def TestEvents(self):
-    return self.SimpleTest("events", "events_unittests")
-
-  def TestExtensions(self):
-    return self.SimpleTest("extensions", "extensions_unittests")
-
-  def TestFFmpegRegressions(self):
-    return self.SimpleTest("chrome", "ffmpeg_regression_tests")
-
-  def TestGCM(self):
-    return self.SimpleTest("gcm", "gcm_unit_tests")
-
-  def TestGfx(self):
-    return self.SimpleTest("gfx", "gfx_unittests")
-
-  def TestGin(self):
-    return self.SimpleTest("gin", "gin_unittests")
-
-  def TestGoogleApis(self):
-    return self.SimpleTest("google_apis", "google_apis_unittests")
-
-  def TestGPU(self):
-    return self.SimpleTest("gpu", "gpu_unittests")
-
-  def TestIpc(self):
-    return self.SimpleTest("ipc", "ipc_tests",
-                           valgrind_test_args=["--trace_children"])
-
-  def TestInstallerUtil(self):
-    return self.SimpleTest("installer_util", "installer_util_unittests")
-
-  def TestInstallStatic(self):
-    return self.SimpleTest("install_static", "install_static_unittests")
-
-  def TestJingle(self):
-    return self.SimpleTest("chrome", "jingle_unittests")
-
-  def TestKeyboard(self):
-    return self.SimpleTest("keyboard", "keyboard_unittests")
-
-  def TestLatency(self):
-    return self.SimpleTest("latency", "latency_unittests")
-
-  def TestMedia(self):
-    return self.SimpleTest("chrome", "media_unittests")
-
-  def TestMessageCenter(self):
-    return self.SimpleTest("message_center", "message_center_unittests")
-
-  def TestMidi(self):
-    return self.SimpleTest("chrome", "midi_unittests")
-
-  def TestMojoCommon(self):
-    return self.SimpleTest("mojo_common", "mojo_common_unittests")
-
-  def TestMojoPublicBindings(self):
-    return self.SimpleTest("mojo_public_bindings",
-                           "mojo_public_bindings_unittests")
-
-  def TestMojoPublicSystem(self):
-    return self.SimpleTest("mojo_public_system",
-                           "mojo_public_system_unittests")
-
-  def TestMojoPublicSysPerf(self):
-    return self.SimpleTest("mojo_public_sysperf",
-                           "mojo_public_system_perftests")
-
-  def TestMojoSystem(self):
-    return self.SimpleTest("mojo_system", "mojo_system_unittests")
-
-  def TestNet(self):
-    return self.SimpleTest("net", "net_unittests")
-
-  def TestNetPerf(self):
-    return self.SimpleTest("net", "net_perftests")
-
-  def TestPhoneNumber(self):
-    return self.SimpleTest("phonenumber", "libphonenumber_unittests")
-
-  def TestPPAPI(self):
-    return self.SimpleTest("chrome", "ppapi_unittests")
-
-  def TestPrinting(self):
-    return self.SimpleTest("chrome", "printing_unittests")
-
-  def TestRemoting(self):
-    return self.SimpleTest("chrome", "remoting_unittests",
-                           cmd_args=[
-                               "--ui-test-action-timeout=60000",
-                               "--ui-test-action-max-timeout=150000"])
-
-  def TestSkia(self):
-    return self.SimpleTest("skia", "skia_unittests")
-
-  def TestSql(self):
-    return self.SimpleTest("chrome", "sql_unittests")
-
-  def TestStorage(self):
-    return self.SimpleTest("storage", "storage_unittests")
-
-  def TestLinuxSandbox(self):
-    return self.SimpleTest("sandbox", "sandbox_linux_unittests")
-
-  def TestUnit(self):
-    # http://crbug.com/51716
-    # Disabling all unit tests
-    # Problems reappeared after r119922
-    if common.IsMac() and (self._options.valgrind_tool == "memcheck"):
-      logging.warning("unit_tests are disabled for memcheck on MacOS.")
-      return 0;
-    return self.SimpleTest("chrome", "unit_tests")
-
-  def TestUIBaseUnit(self):
-    return self.SimpleTest("chrome", "ui_base_unittests")
-
-  def TestUIChromeOS(self):
-    return self.SimpleTest("chrome", "ui_chromeos_unittests")
-
-  def TestURL(self):
-    return self.SimpleTest("chrome", "url_unittests")
-
-  def TestViews(self):
-    return self.SimpleTest("views", "views_unittests")
-
-
-  # Valgrind timeouts are in seconds.
-  UI_VALGRIND_ARGS = ["--timeout=14400", "--trace_children", "--indirect"]
-  # UI test timeouts are in milliseconds.
-  UI_TEST_ARGS = ["--ui-test-action-timeout=60000",
-                  "--ui-test-action-max-timeout=150000",
-                  "--no-sandbox"]
-
-  # TODO(thestig) fine-tune these values.
-  # Valgrind timeouts are in seconds.
-  BROWSER_VALGRIND_ARGS = ["--timeout=50000", "--trace_children", "--indirect"]
-  # Browser test timeouts are in milliseconds.
-  BROWSER_TEST_ARGS = ["--ui-test-action-timeout=400000",
-                       "--ui-test-action-max-timeout=800000",
-                       "--no-sandbox"]
-
-  def TestBrowser(self):
-    return self.SimpleTest("chrome", "browser_tests",
-                           valgrind_test_args=self.BROWSER_VALGRIND_ARGS,
-                           cmd_args=self.BROWSER_TEST_ARGS)
-
-  def TestContentBrowser(self):
-    return self.SimpleTest("content", "content_browsertests",
-                           valgrind_test_args=self.BROWSER_VALGRIND_ARGS,
-                           cmd_args=self.BROWSER_TEST_ARGS)
-
-  def TestInteractiveUI(self):
-    return self.SimpleTest("chrome", "interactive_ui_tests",
-                           valgrind_test_args=self.UI_VALGRIND_ARGS,
-                           cmd_args=self.UI_TEST_ARGS)
-
-  def TestSyncIntegration(self):
-    return self.SimpleTest("chrome", "sync_integration_tests",
-                           valgrind_test_args=self.UI_VALGRIND_ARGS,
-                           cmd_args=(["--ui-test-action-max-timeout=450000"]))
-
-  def TestLayoutChunk(self, chunk_num, chunk_size):
-    # Run tests [chunk_num*chunk_size .. (chunk_num+1)*chunk_size) from the
-    # list of tests.  Wrap around to beginning of list at end.
-    # If chunk_size is zero, run all tests in the list once.
-    # If a text file is given as argument, it is used as the list of tests.
-    assert((chunk_size == 0) != (len(self._args) == 0))
-    # Build the ginormous commandline in 'cmd'.
-    # It's going to be roughly
-    #  python valgrind_test.py ...
-    # but we'll use the --indirect flag to valgrind_test.py
-    # to avoid valgrinding python.
-    # Start by building the valgrind_test.py commandline.
-    tool = valgrind_test.CreateTool(self._options.valgrind_tool)
-    cmd = self._DefaultCommand(tool)
-    cmd.append("--trace_children")
-    cmd.append("--indirect_webkit_layout")
-    cmd.append("--ignore_exit_code")
-    # Now build script_cmd, the run-webkits-tests commandline.
-    # Store each chunk in its own directory so that we can find the data later
-    chunk_dir = os.path.join("layout", "chunk_%05d" % chunk_num)
-    out_dir = os.path.join(path_utils.ScriptDir(), "latest")
-    out_dir = os.path.join(out_dir, chunk_dir)
-    if os.path.exists(out_dir):
-      old_files = glob.glob(os.path.join(out_dir, "*.txt"))
-      for f in old_files:
-        os.remove(f)
-    else:
-      os.makedirs(out_dir)
-    script = os.path.join(self._source_dir, "third_party", "WebKit", "Tools",
-                          "Scripts", "run-webkit-tests")
-    # http://crbug.com/260627: After the switch to content_shell from DRT, each
-    # test now brings up 3 processes.  Under Valgrind, they become memory bound
-    # and can eventually OOM if we don't reduce the total count.
-    # It'd be nice if content_shell automatically throttled the startup of new
-    # tests if we're low on memory.
-    jobs = max(1, int(multiprocessing.cpu_count() * 0.3))
-    script_cmd = ["python", script, "-v",
-                  # run a separate DumpRenderTree for each test
-                  "--batch-size=1",
-                  "--fully-parallel",
-                  "--child-processes=%d" % jobs,
-                  "--time-out-ms=800000",
-                  "--no-retry-failures",  # retrying takes too much time
-                  # http://crbug.com/176908: Don't launch a browser when done.
-                  "--no-show-results",
-                  "--nocheck-sys-deps",
-                  "--additional-driver-flag=--no-sandbox"]
-    # Pass build mode to run-webkit-tests.  We aren't passed it directly,
-    # so parse it out of build_dir.  run-webkit-tests can only handle
-    # the two values "Release" and "Debug".
-    # TODO(Hercules): unify how all our scripts pass around build mode
-    # (--mode / --target / --build-dir / --debug)
-    if self._options.build_dir:
-      build_root, mode = os.path.split(self._options.build_dir)
-      script_cmd.extend(["--build-directory", build_root, "--target", mode])
-    if (chunk_size > 0):
-      script_cmd.append("--run-chunk=%d:%d" % (chunk_num, chunk_size))
-    if len(self._args):
-      # if the arg is a txt file, then treat it as a list of tests
-      if os.path.isfile(self._args[0]) and self._args[0][-4:] == ".txt":
-        script_cmd.append("--test-list=%s" % self._args[0])
-      else:
-        script_cmd.extend(self._args)
-    self._AppendGtestFilter(tool, "layout", script_cmd)
-    # Now run script_cmd with the wrapper in cmd
-    cmd.extend(["--"])
-    cmd.extend(script_cmd)
-
-    # Layout tests often times fail quickly, but the buildbot remains green.
-    # Detect this situation when running with the default chunk size.
-    if chunk_size == self.LAYOUT_TESTS_DEFAULT_CHUNK_SIZE:
-      min_runtime_in_seconds=120
-    else:
-      min_runtime_in_seconds=0
-    ret = tool.Run(cmd, "layout", min_runtime_in_seconds=min_runtime_in_seconds)
-    return ret
-
-
-  def TestLayout(self):
-    # A "chunk file" is maintained in the local directory so that each test
-    # runs a slice of the layout tests of size chunk_size that increments with
-    # each run.  Since tests can be added and removed from the layout tests at
-    # any time, this is not going to give exact coverage, but it will allow us
-    # to continuously run small slices of the layout tests under valgrind rather
-    # than having to run all of them in one shot.
-    chunk_size = self._options.num_tests
-    if chunk_size == 0 or len(self._args):
-      return self.TestLayoutChunk(0, 0)
-    chunk_num = 0
-    chunk_file = os.path.join("valgrind_layout_chunk.txt")
-    logging.info("Reading state from " + chunk_file)
-    try:
-      f = open(chunk_file)
-      if f:
-        chunk_str = f.read()
-        if len(chunk_str):
-          chunk_num = int(chunk_str)
-        # This should be enough so that we have a couple of complete runs
-        # of test data stored in the archive (although note that when we loop
-        # that we almost guaranteed won't be at the end of the test list)
-        if chunk_num > 10000:
-          chunk_num = 0
-        f.close()
-    except IOError, (errno, strerror):
-      logging.error("error reading from file %s (%d, %s)" % (chunk_file,
-                    errno, strerror))
-    # Save the new chunk size before running the tests. Otherwise if a
-    # particular chunk hangs the bot, the chunk number will never get
-    # incremented and the bot will be wedged.
-    logging.info("Saving state to " + chunk_file)
-    try:
-      f = open(chunk_file, "w")
-      chunk_num += 1
-      f.write("%d" % chunk_num)
-      f.close()
-    except IOError, (errno, strerror):
-      logging.error("error writing to file %s (%d, %s)" % (chunk_file, errno,
-                    strerror))
-    # Since we're running small chunks of the layout tests, it's important to
-    # mark the ones that have errors in them.  These won't be visible in the
-    # summary list for long, but will be useful for someone reviewing this bot.
-    return self.TestLayoutChunk(chunk_num, chunk_size)
-
-  # The known list of tests.
-  # Recognise the original abbreviations as well as full executable names.
-  _test_list = {
-    "cmdline" : RunCmdLine,
-    "addressinput": TestAddressInput,
-    "libaddressinput_unittests": TestAddressInput,
-    "accessibility": TestAccessibility,
-    "angle": TestAngle,          "angle_unittests": TestAngle,
-    "app_list": TestAppList,     "app_list_unittests": TestAppList,
-    "ash": TestAsh,              "ash_unittests": TestAsh,
-    "aura": TestAura,            "aura_unittests": TestAura,
-    "base": TestBase,            "base_unittests": TestBase,
-    "blink_heap": TestBlinkHeap,
-    "blink_platform": TestBlinkPlatform,
-    "browser": TestBrowser,      "browser_tests": TestBrowser,
-    "cacheinvalidation": TestCacheInvalidation,
-    "cacheinvalidation_unittests": TestCacheInvalidation,
-    "cast": TestCast,            "cast_unittests": TestCast,
-    "cc": TestCC,                "cc_unittests": TestCC,
-    "chrome_app": TestChromeApp,
-    "chrome_elf": TestChromeElf,
-    "chromedriver": TestChromeDriver,
-    "chromeos": TestChromeOS,    "chromeos_unittests": TestChromeOS,
-    "components": TestComponents,"components_unittests": TestComponents,
-    "compositor": TestCompositor,"compositor_unittests": TestCompositor,
-    "content": TestContent,      "content_unittests": TestContent,
-    "content_browsertests": TestContentBrowser,
-    "courgette": TestCourgette,  "courgette_unittests": TestCourgette,
-    "crypto": TestCrypto,        "crypto_unittests": TestCrypto,
-    "device": TestDevice,        "device_unittests": TestDevice,
-    "display": TestDisplay,      "display_unittests": TestDisplay,
-    "events": TestEvents,        "events_unittests": TestEvents,
-    "extensions": TestExtensions, "extensions_unittests": TestExtensions,
-    "ffmpeg_regression_tests": TestFFmpegRegressions,
-    "gcm": TestGCM,              "gcm_unit_tests": TestGCM,
-    "gin": TestGin,              "gin_unittests": TestGin,
-    "gfx": TestGfx,              "gfx_unittests": TestGfx,
-    "google_apis": TestGoogleApis,
-    "gpu": TestGPU,              "gpu_unittests": TestGPU,
-    "ipc": TestIpc,              "ipc_tests": TestIpc,
-    "installer_util": TestInstallerUtil,
-    "installer_util_unittests": TestInstallerUtil,
-    "install_static_unittests": TestInstallStatic,
-    "interactive_ui": TestInteractiveUI,
-    "jingle": TestJingle,        "jingle_unittests": TestJingle,
-    "keyboard": TestKeyboard,    "keyboard_unittests": TestKeyboard,
-    "latency": TestLatency,      "latency_unittests": TestLatency,
-    "layout": TestLayout,        "layout_tests": TestLayout,
-    "media": TestMedia,          "media_unittests": TestMedia,
-    "message_center": TestMessageCenter,
-    "message_center_unittests" : TestMessageCenter,
-    "midi": TestMidi,             "midi_unittests": TestMidi,
-    "mojo_common": TestMojoCommon,
-    "mojo_common_unittests": TestMojoCommon,
-    "mojo_system": TestMojoSystem,
-    "mojo_system_unittests": TestMojoSystem,
-    "mojo_public_system": TestMojoPublicSystem,
-    "mojo_public_system_unittests": TestMojoPublicSystem,
-    "mojo_public_bindings": TestMojoPublicBindings,
-    "mojo_public_bindings_unittests": TestMojoPublicBindings,
-    "mojo_public_sysperf": TestMojoPublicSysPerf,
-    "net": TestNet,              "net_unittests": TestNet,
-    "net_perf": TestNetPerf,     "net_perftests": TestNetPerf,
-    "phonenumber": TestPhoneNumber,
-    "libphonenumber_unittests": TestPhoneNumber,
-    "ppapi": TestPPAPI,          "ppapi_unittests": TestPPAPI,
-    "printing": TestPrinting,    "printing_unittests": TestPrinting,
-    "remoting": TestRemoting,    "remoting_unittests": TestRemoting,
-    "sandbox": TestLinuxSandbox, "sandbox_linux_unittests": TestLinuxSandbox,
-    "skia": TestSkia,            "skia_unittests": TestSkia,
-    "sql": TestSql,              "sql_unittests": TestSql,
-    "storage": TestStorage,      "storage_unittests": TestStorage,
-    "sync_integration_tests": TestSyncIntegration,
-    "sync_integration": TestSyncIntegration,
-    "ui_base_unit": TestUIBaseUnit,       "ui_base_unittests": TestUIBaseUnit,
-    "ui_chromeos": TestUIChromeOS, "ui_chromeos_unittests": TestUIChromeOS,
-    "unit": TestUnit,            "unit_tests": TestUnit,
-    "url": TestURL,              "url_unittests": TestURL,
-    "views": TestViews,          "views_unittests": TestViews,
-    "webkit": TestLayout,
-  }
-
-
-def _main():
-  parser = optparse.OptionParser("usage: %prog -b <dir> -t <test> "
-                                 "[-t <test> ...]")
-
-  parser.add_option("--help-tests", dest="help_tests", action="store_true",
-                    default=False, help="List all available tests")
-  parser.add_option("-b", "--build-dir",
-                    help="the location of the compiler output")
-  parser.add_option("--target", help="Debug or Release")
-  parser.add_option("-t", "--test", action="append", default=[],
-                    help="which test to run, supports test:gtest_filter format "
-                         "as well.")
-  parser.add_option("--baseline", action="store_true", default=False,
-                    help="generate baseline data instead of validating")
-  parser.add_option("-f", "--force", action="store_true", default=False,
-                    help="run a broken test anyway")
-  parser.add_option("--gtest_filter",
-                    help="additional arguments to --gtest_filter")
-  parser.add_option("--gtest_repeat", help="argument for --gtest_repeat")
-  parser.add_option("--gtest_shuffle", action="store_true", default=False,
-                    help="Randomize tests' orders on every iteration.")
-  parser.add_option("--gtest_break_on_failure", action="store_true",
-                    default=False,
-                    help="Drop in to debugger on assertion failure. Also "
-                         "useful for forcing tests to exit with a stack dump "
-                         "on the first assertion failure when running with "
-                         "--gtest_repeat=-1")
-  parser.add_option("-v", "--verbose", action="store_true", default=False,
-                    help="verbose output - enable debug log messages")
-  parser.add_option("--tool", dest="valgrind_tool", default="memcheck",
-                    help="specify a valgrind tool to run the tests under")
-  parser.add_option("--tool_flags", dest="valgrind_tool_flags", default="",
-                    help="specify custom flags for the selected valgrind tool")
-  parser.add_option("--keep_logs", action="store_true", default=False,
-                    help="store memory tool logs in the <tool>.logs directory "
-                         "instead of /tmp.\nThis can be useful for tool "
-                         "developers/maintainers.\nPlease note that the <tool>"
-                         ".logs directory will be clobbered on tool startup.")
-  parser.add_option("-n", "--num_tests", type="int",
-                    default=ChromeTests.LAYOUT_TESTS_DEFAULT_CHUNK_SIZE,
-                    help="for layout tests: # of subtests per run.  0 for all.")
-  parser.add_option("--test-launcher-bot-mode", action="store_true",
-                    help="run the tests with --test-launcher-bot-mode")
-  parser.add_option("--test-launcher-total-shards", type=int,
-                    help="run the tests with --test-launcher-total-shards")
-  parser.add_option("--test-launcher-shard-index", type=int,
-                    help="run the tests with --test-launcher-shard-index")
-  parser.add_option("--drmemory_ops",
-                    help="extra options passed to Dr. Memory")
-
-  options, args = parser.parse_args()
-
-  # Bake target into build_dir.
-  if options.target and options.build_dir:
-    assert (options.target !=
-            os.path.basename(os.path.dirname(options.build_dir)))
-    options.build_dir = os.path.join(os.path.abspath(options.build_dir),
-                                     options.target)
-
-  if options.verbose:
-    logging_utils.config_root(logging.DEBUG)
-  else:
-    logging_utils.config_root()
-
-  if options.help_tests:
-    ChromeTests.ShowTests()
-    return 0
-
-  if not options.test:
-    parser.error("--test not specified")
-
-  if len(options.test) != 1 and options.gtest_filter:
-    parser.error("--gtest_filter and multiple tests don't make sense together")
-
-  BROKEN_TESTS = {
-    'drmemory_light': [
-      'addressinput',
-      'aura',
-      'base_unittests',
-      'cc',
-      'components', # x64 only?
-      'content',
-      'gfx',
-      'mojo_public_bindings',
-    ],
-    'drmemory_full': [
-      'addressinput',
-      'aura',
-      'base_unittests',
-      'blink_heap',
-      'blink_platform',
-      'browser_tests',
-      'cast',
-      'cc',
-      'chromedriver',
-      'compositor',
-      'content',
-      'content_browsertests',
-      'device',
-      'events',
-      'extensions',
-      'gfx',
-      'google_apis',
-      'gpu',
-      'ipc_tests',
-      'jingle',
-      'keyboard',
-      'media',
-      'midi',
-      'mojo_common',
-      'mojo_public_bindings',
-      'mojo_public_sysperf',
-      'mojo_public_system',
-      'mojo_system',
-      'net',
-      'remoting',
-      'unit',
-      'url',
-    ],
-  }
-
-  for t in options.test:
-    if t in BROKEN_TESTS[options.valgrind_tool] and not options.force:
-      logging.info("Skipping broken %s test %s -- see crbug.com/633693" %
-                   (options.valgrind_tool, t))
-      return 0
-
-    tests = ChromeTests(options, args, t)
-    ret = tests.Run()
-    if ret: return ret
-  return 0
-
-
-if __name__ == "__main__":
-  sys.exit(_main())
diff --git a/tools_libyuv/valgrind/chrome_tests.sh b/tools_libyuv/valgrind/chrome_tests.sh
deleted file mode 100755
index dc17684f..00000000
--- a/tools_libyuv/valgrind/chrome_tests.sh
+++ /dev/null
@@ -1,94 +0,0 @@
-#!/bin/bash
-
-# Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-# Set up some paths and re-direct the arguments to chrome_tests.py
-
-export THISDIR=`dirname $0`
-ARGV_COPY="$@"
-
-# We need to set CHROME_VALGRIND iff using Memcheck:
-#   tools/valgrind/chrome_tests.sh --tool memcheck
-# or
-#   tools/valgrind/chrome_tests.sh --tool=memcheck
-tool="memcheck"  # Default to memcheck.
-while (( "$#" ))
-do
-  if [[ "$1" == "--tool" ]]
-  then
-    tool="$2"
-    shift
-  elif [[ "$1" =~ --tool=(.*) ]]
-  then
-    tool="${BASH_REMATCH[1]}"
-  fi
-  shift
-done
-
-NEEDS_VALGRIND=0
-NEEDS_DRMEMORY=0
-
-case "$tool" in
-  "memcheck")
-    NEEDS_VALGRIND=1
-    ;;
-  "drmemory" | "drmemory_light" | "drmemory_full" | "drmemory_pattern")
-    NEEDS_DRMEMORY=1
-    ;;
-esac
-
-if [ "$NEEDS_VALGRIND" == "1" ]
-then
-  export CHROME_VALGRIND=`sh $THISDIR/locate_valgrind.sh`
-  if [ "$CHROME_VALGRIND" = "" ]
-  then
-    # locate_valgrind.sh failed
-    exit 1
-  fi
-  echo "Using valgrind binaries from ${CHROME_VALGRIND}"
-
-  PATH="${CHROME_VALGRIND}/bin:$PATH"
-  # We need to set these variables to override default lib paths hard-coded into
-  # Valgrind binary.
-  export VALGRIND_LIB="$CHROME_VALGRIND/lib/valgrind"
-  export VALGRIND_LIB_INNER="$CHROME_VALGRIND/lib/valgrind"
-
-  # Clean up some /tmp directories that might be stale due to interrupted
-  # chrome_tests.py execution.
-  # FYI:
-  #   -mtime +1  <- only print files modified more than 24h ago,
-  #   -print0/-0 are needed to handle possible newlines in the filenames.
-  echo "Cleanup /tmp from Valgrind stuff"
-  find /tmp -maxdepth 1 \(\
-        -name "vgdb-pipe-*" -or -name "vg_logs_*" -or -name "valgrind.*" \
-      \) -mtime +1 -print0 | xargs -0 rm -rf
-fi
-
-if [ "$NEEDS_DRMEMORY" == "1" ]
-then
-  if [ -z "$DRMEMORY_COMMAND" ]
-  then
-    DRMEMORY_PATH="$THISDIR/../../third_party/drmemory"
-    DRMEMORY_SFX="$DRMEMORY_PATH/drmemory-windows-sfx.exe"
-    if [ ! -f "$DRMEMORY_SFX" ]
-    then
-      echo "Can't find Dr. Memory executables."
-      echo "See http://www.chromium.org/developers/how-tos/using-valgrind/dr-memory"
-      echo "for the instructions on how to get them."
-      exit 1
-    fi
-
-    chmod +x "$DRMEMORY_SFX"  # Cygwin won't run it without +x.
-    "$DRMEMORY_SFX" -o"$DRMEMORY_PATH/unpacked" -y
-    export DRMEMORY_COMMAND="$DRMEMORY_PATH/unpacked/bin/drmemory.exe"
-  fi
-fi
-
-PYTHONPATH=$THISDIR/../python/google python \
-           "$THISDIR/chrome_tests.py" $ARGV_COPY
diff --git a/tools_libyuv/valgrind/common.py b/tools_libyuv/valgrind/common.py
deleted file mode 100644
index e9ee51e4..00000000
--- a/tools_libyuv/valgrind/common.py
+++ /dev/null
@@ -1,256 +0,0 @@
-# Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-import logging
-import platform
-import os
-import signal
-import subprocess
-import sys
-import time
-
-
-class NotImplementedError(Exception):
-  pass
-
-
-class TimeoutError(Exception):
-  pass
-
-
-def RunSubprocessInBackground(proc):
-  """Runs a subprocess in the background. Returns a handle to the process."""
-  logging.info("running %s in the background" % " ".join(proc))
-  return subprocess.Popen(proc)
-
-
-def RunSubprocess(proc, timeout=0):
-  """ Runs a subprocess, until it finishes or |timeout| is exceeded and the
-  process is killed with taskkill.  A |timeout| <= 0  means no timeout.
-
-  Args:
-    proc: list of process components (exe + args)
-    timeout: how long to wait before killing, <= 0 means wait forever
-  """
-
-  logging.info("running %s, timeout %d sec" % (" ".join(proc), timeout))
-  sys.stdout.flush()
-  sys.stderr.flush()
-
-  # Manually read and print out stdout and stderr.
-  # By default, the subprocess is supposed to inherit these from its parent,
-  # however when run under buildbot, it seems unable to read data from a
-  # grandchild process, so we have to read the child and print the data as if
-  # it came from us for buildbot to read it.  We're not sure why this is
-  # necessary.
-  # TODO(erikkay): should we buffer stderr and stdout separately?
-  p = subprocess.Popen(proc, universal_newlines=True,
-                       bufsize=0,  # unbuffered
-                       stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
-
-  logging.info("started subprocess")
-
-  did_timeout = False
-  if timeout > 0:
-    wait_until = time.time() + timeout
-  while p.poll() is None and not did_timeout:
-    # Have to use readline rather than readlines() or "for line in p.stdout:",
-    # otherwise we get buffered even with bufsize=0.
-    line = p.stdout.readline()
-    while line and not did_timeout:
-      sys.stdout.write(line)
-      sys.stdout.flush()
-      line = p.stdout.readline()
-      if timeout > 0:
-        did_timeout = time.time() > wait_until
-
-  if did_timeout:
-    logging.info("process timed out")
-  else:
-    logging.info("process ended, did not time out")
-
-  if did_timeout:
-    if IsWindows():
-      subprocess.call(["taskkill", "/T", "/F", "/PID", str(p.pid)])
-    else:
-      # Does this kill all children, too?
-      os.kill(p.pid, signal.SIGINT)
-    logging.error("KILLED %d" % p.pid)
-    # Give the process a chance to actually die before continuing
-    # so that cleanup can happen safely.
-    time.sleep(1.0)
-    logging.error("TIMEOUT waiting for %s" % proc[0])
-    raise TimeoutError(proc[0])
-  else:
-    for line in p.stdout:
-      sys.stdout.write(line)
-    if not IsMac():   # stdout flush fails on Mac
-      logging.info("flushing stdout")
-      sys.stdout.flush()
-
-  logging.info("collecting result code")
-  result = p.poll()
-  if result:
-    logging.error("%s exited with non-zero result code %d" % (proc[0], result))
-  return result
-
-
-def IsLinux():
-  return sys.platform.startswith('linux')
-
-
-def IsMac():
-  return sys.platform.startswith('darwin')
-
-
-def IsWindows():
-  return sys.platform == 'cygwin' or sys.platform.startswith('win')
-
-
-def WindowsVersionName():
-  """Returns the name of the Windows version if it is known, or None.
-
-  Possible return values are: xp, vista, 7, 8, or None
-  """
-  if sys.platform == 'cygwin':
-    # Windows version number is hiding in system name.  Looks like:
-    # CYGWIN_NT-6.1-WOW64
-    try:
-      version_str = platform.uname()[0].split('-')[1]
-    except:
-      return None
-  elif sys.platform.startswith('win'):
-    # Normal Windows version string.  Mine: 6.1.7601
-    version_str = platform.version()
-  else:
-    return None
-
-  parts = version_str.split('.')
-  try:
-    major = int(parts[0])
-    minor = int(parts[1])
-  except:
-    return None  # Can't parse, unknown version.
-
-  if major == 5:
-    return 'xp'
-  elif major == 6 and minor == 0:
-    return 'vista'
-  elif major == 6 and minor == 1:
-    return '7'
-  elif major == 6 and minor == 2:
-    return '8'  # Future proof.  ;)
-  return None
-
-
-def PlatformNames():
-  """Return an array of string to be used in paths for the platform
-  (e.g. suppressions, gtest filters, ignore files etc.)
-  The first element of the array describes the 'main' platform
-  """
-  if IsLinux():
-    return ['linux']
-  if IsMac():
-    return ['mac']
-  if IsWindows():
-    names = ['win32']
-    version_name = WindowsVersionName()
-    if version_name is not None:
-      names.append('win-%s' % version_name)
-    return names
-  raise NotImplementedError('Unknown platform "%s".' % sys.platform)
-
-
-def PutEnvAndLog(env_name, env_value):
-  os.putenv(env_name, env_value)
-  logging.info('export %s=%s', env_name, env_value)
-
-def BoringCallers(mangled, use_re_wildcards):
-  """Return a list of 'boring' function names (optinally mangled)
-  with */? wildcards (optionally .*/.).
-  Boring = we drop off the bottom of stack traces below such functions.
-  """
-
-  need_mangling = [
-    # Don't show our testing framework:
-    ("testing::Test::Run",     "_ZN7testing4Test3RunEv"),
-    ("testing::TestInfo::Run", "_ZN7testing8TestInfo3RunEv"),
-    ("testing::internal::Handle*ExceptionsInMethodIfSupported*",
-     "_ZN7testing8internal3?Handle*ExceptionsInMethodIfSupported*"),
-
-    # Depend on scheduling:
-    ("MessageLoop::Run",     "_ZN11MessageLoop3RunEv"),
-    ("MessageLoop::RunTask", "_ZN11MessageLoop7RunTask*"),
-    ("RunnableMethod*",      "_ZN14RunnableMethod*"),
-    ("DispatchToMethod*",    "_Z*16DispatchToMethod*"),
-    ("base::internal::Invoker*::DoInvoke*",
-     "_ZN4base8internal8Invoker*DoInvoke*"),  # Invoker{1,2,3}
-    ("base::internal::RunnableAdapter*::Run*",
-     "_ZN4base8internal15RunnableAdapter*Run*"),
-  ]
-
-  ret = []
-  for pair in need_mangling:
-    ret.append(pair[1 if mangled else 0])
-
-  ret += [
-    # Also don't show the internals of libc/pthread.
-    "start_thread",
-    "main",
-    "BaseThreadInitThunk",
-  ]
-
-  if use_re_wildcards:
-    for i in range(0, len(ret)):
-      ret[i] = ret[i].replace('*', '.*').replace('?', '.')
-
-  return ret
-
-def NormalizeWindowsPath(path):
-  """If we're using Cygwin Python, turn the path into a Windows path.
-
-  Don't turn forward slashes into backslashes for easier copy-pasting and
-  escaping.
-
-  TODO(rnk): If we ever want to cut out the subprocess invocation, we can use
-  _winreg to get the root Cygwin directory from the registry key:
-  HKEY_LOCAL_MACHINE\SOFTWARE\Cygwin\setup\rootdir.
-  """
-  if sys.platform.startswith("cygwin"):
-    p = subprocess.Popen(["cygpath", "-m", path],
-                         stdout=subprocess.PIPE,
-                         stderr=subprocess.PIPE)
-    (out, err) = p.communicate()
-    if err:
-      logging.warning("WARNING: cygpath error: %s", err)
-    return out.strip()
-  else:
-    return path
-
-############################
-# Common output format code
-
-def PrintUsedSuppressionsList(suppcounts):
-  """ Prints out the list of used suppressions in a format common to all the
-      memory tools. If the list is empty, prints nothing and returns False,
-      otherwise True.
-
-      suppcounts: a dictionary of used suppression counts,
-                  Key -> name, Value -> count.
-  """
-  if not suppcounts:
-    return False
-
-  print "-----------------------------------------------------"
-  print "Suppressions used:"
-  print "  count name"
-  for (name, count) in sorted(suppcounts.items(), key=lambda (k,v): (v,k)):
-    print "%7d %s" % (count, name)
-  print "-----------------------------------------------------"
-  sys.stdout.flush()
-  return True
diff --git a/tools_libyuv/valgrind/gdb_helper.py b/tools_libyuv/valgrind/gdb_helper.py
deleted file mode 100644
index d127f760..00000000
--- a/tools_libyuv/valgrind/gdb_helper.py
+++ /dev/null
@@ -1,91 +0,0 @@
-# Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-''' A bunch of helper functions for querying gdb.'''
-
-import logging
-import os
-import re
-import tempfile
-
-GDB_LINE_RE = re.compile(r'Line ([0-9]*) of "([^"]*)".*')
-
-def _GdbOutputToFileLine(output_line):
-  ''' Parse the gdb output line, return a pair (file, line num) '''
-  match =  GDB_LINE_RE.match(output_line)
-  if match:
-    return match.groups()[1], match.groups()[0]
-  else:
-    return None
-
-def ResolveAddressesWithinABinary(binary_name, load_address, address_list):
-  ''' For each address, return a pair (file, line num) '''
-  commands = tempfile.NamedTemporaryFile()
-  commands.write('add-symbol-file "%s" %s\n' % (binary_name, load_address))
-  for addr in address_list:
-    commands.write('info line *%s\n' % addr)
-  commands.write('quit\n')
-  commands.flush()
-  gdb_commandline = 'gdb -batch -x %s 2>/dev/null' % commands.name
-  gdb_pipe = os.popen(gdb_commandline)
-  result = gdb_pipe.readlines()
-
-  address_count = 0
-  ret = {}
-  for line in result:
-    if line.startswith('Line'):
-      ret[address_list[address_count]] = _GdbOutputToFileLine(line)
-      address_count += 1
-    if line.startswith('No line'):
-      ret[address_list[address_count]] = (None, None)
-      address_count += 1
-  gdb_pipe.close()
-  commands.close()
-  return ret
-
-class AddressTable(object):
-  ''' Object to do batched line number lookup. '''
-  def __init__(self):
-    self._load_addresses = {}
-    self._binaries = {}
-    self._all_resolved = False
-
-  def AddBinaryAt(self, binary, load_address):
-    ''' Register a new shared library or executable. '''
-    self._load_addresses[binary] = load_address
-
-  def Add(self, binary, address):
-    ''' Register a lookup request. '''
-    if binary == '':
-      logging.warn('adding address %s in empty binary?' % address)
-    if binary in self._binaries:
-      self._binaries[binary].append(address)
-    else:
-      self._binaries[binary] = [address]
-    self._all_resolved = False
-
-  def ResolveAll(self):
-    ''' Carry out all lookup requests. '''
-    self._translation = {}
-    for binary in self._binaries.keys():
-      if binary != '' and binary in self._load_addresses:
-        load_address = self._load_addresses[binary]
-        addr = ResolveAddressesWithinABinary(
-            binary, load_address, self._binaries[binary])
-        self._translation[binary] = addr
-    self._all_resolved = True
-
-  def GetFileLine(self, binary, addr):
-    ''' Get the (filename, linenum) result of a previously-registered lookup
-    request.
-    '''
-    if self._all_resolved:
-      if binary in self._translation:
-        if addr in self._translation[binary]:
-          return self._translation[binary][addr]
-    return (None, None)
diff --git a/tools_libyuv/valgrind/libyuv_tests.bat b/tools_libyuv/valgrind/libyuv_tests.bat
deleted file mode 100644
index 5fceca67..00000000
--- a/tools_libyuv/valgrind/libyuv_tests.bat
+++ /dev/null
@@ -1,79 +0,0 @@
-@echo off
-:: Copyright (c) 2012 The LibYuv Project Authors. All rights reserved.
-::
-:: Use of this source code is governed by a BSD-style license
-:: that can be found in the LICENSE file in the root of the source
-:: tree. An additional intellectual property rights grant can be found
-:: in the file PATENTS.  All contributing project authors may
-:: be found in the AUTHORS file in the root of the source tree.
-
-:: This script is a copy of chrome_tests.bat with the following changes:
-:: - Invokes libyuv_tests.py instead of chrome_tests.py
-:: - Chromium's Valgrind scripts directory is added to the PYTHONPATH to make
-::   it possible to execute the Python scripts properly.
-
-:: TODO(timurrrr): batch files 'export' all the variables to the parent shell
-set THISDIR=%~dp0
-set TOOL_NAME="unknown"
-
-:: Get the tool name and put it into TOOL_NAME {{{1
-:: NB: SHIFT command doesn't modify %*
-:PARSE_ARGS_LOOP
-  if %1 == () GOTO:TOOLNAME_NOT_FOUND
-  if %1 == --tool GOTO:TOOLNAME_FOUND
-  SHIFT
-  goto :PARSE_ARGS_LOOP
-
-:TOOLNAME_NOT_FOUND
-echo "Please specify a tool (tsan or drmemory) by using --tool flag"
-exit /B 1
-
-:TOOLNAME_FOUND
-SHIFT
-set TOOL_NAME=%1
-:: }}}
-if "%TOOL_NAME%" == "drmemory"          GOTO :SETUP_DRMEMORY
-if "%TOOL_NAME%" == "drmemory_light"    GOTO :SETUP_DRMEMORY
-if "%TOOL_NAME%" == "drmemory_full"     GOTO :SETUP_DRMEMORY
-if "%TOOL_NAME%" == "drmemory_pattern"  GOTO :SETUP_DRMEMORY
-if "%TOOL_NAME%" == "tsan"     GOTO :SETUP_TSAN
-echo "Unknown tool: `%TOOL_NAME%`! Only tsan and drmemory are supported."
-exit /B 1
-
-:SETUP_DRMEMORY
-if NOT "%DRMEMORY_COMMAND%"=="" GOTO :RUN_TESTS
-:: Set up DRMEMORY_COMMAND to invoke Dr. Memory {{{1
-set DRMEMORY_PATH=%THISDIR%..\..\third_party\drmemory
-set DRMEMORY_SFX=%DRMEMORY_PATH%\drmemory-windows-sfx.exe
-if EXIST %DRMEMORY_SFX% GOTO DRMEMORY_BINARY_OK
-echo "Can't find Dr. Memory executables."
-echo "See http://www.chromium.org/developers/how-tos/using-valgrind/dr-memory"
-echo "for the instructions on how to get them."
-exit /B 1
-
-:DRMEMORY_BINARY_OK
-%DRMEMORY_SFX% -o%DRMEMORY_PATH%\unpacked -y
-set DRMEMORY_COMMAND=%DRMEMORY_PATH%\unpacked\bin\drmemory.exe
-:: }}}
-goto :RUN_TESTS
-
-:SETUP_TSAN
-:: Set up PIN_COMMAND to invoke TSan {{{1
-set TSAN_PATH=%THISDIR%..\..\third_party\tsan
-set TSAN_SFX=%TSAN_PATH%\tsan-x86-windows-sfx.exe
-if EXIST %TSAN_SFX% GOTO TSAN_BINARY_OK
-echo "Can't find ThreadSanitizer executables."
-echo "See http://www.chromium.org/developers/how-tos/using-valgrind/threadsanitizer/threadsanitizer-on-windows"
-echo "for the instructions on how to get them."
-exit /B 1
-
-:TSAN_BINARY_OK
-%TSAN_SFX% -o%TSAN_PATH%\unpacked -y
-set PIN_COMMAND=%TSAN_PATH%\unpacked\tsan-x86-windows\tsan.bat
-:: }}}
-goto :RUN_TESTS
-
-:RUN_TESTS
-set PYTHONPATH=%THISDIR%..\python\google;%THISDIR%..\valgrind
-set RUNNING_ON_VALGRIND=yes
-python %THISDIR%libyuv_tests.py %*
diff --git a/tools_libyuv/valgrind/libyuv_tests.py b/tools_libyuv/valgrind/libyuv_tests.py
deleted file mode 100755
index e780bd95..00000000
--- a/tools_libyuv/valgrind/libyuv_tests.py
+++ /dev/null
@@ -1,139 +0,0 @@
-#!/usr/bin/env python
-# Copyright (c) 2012 The LibYuv Project Authors. All rights reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-"""Runs various libyuv tests through valgrind_test.py.
-
-This script inherits the chrome_tests.py in Chrome, but allows running any test
-instead of only the hard-coded ones. It uses the -t cmdline flag to do this, and
-only supports specifying a single test for each run.
-
-Suppression files:
-The Chrome valgrind directory we use as a DEPS dependency contains the following
-suppression files:
-  valgrind/memcheck/suppressions.txt
-  valgrind/memcheck/suppressions_mac.txt
-  valgrind/tsan/suppressions.txt
-  valgrind/tsan/suppressions_mac.txt
-  valgrind/tsan/suppressions_win32.txt
-Since they're referenced from the chrome_tests.py script, we have similar files
-below the directory of this script. When executing, this script will setup both
-Chrome's suppression files and our own, so we can easily maintain libyuv
-specific suppressions in our own files.
-"""
-
-import logging
-import optparse
-import os
-import sys
-
-import logging_utils
-import path_utils
-
-import chrome_tests
-
-
-class LibyuvTest(chrome_tests.ChromeTests):
-  """Class that handles setup of suppressions for libyuv.
-
-  Everything else is inherited from chrome_tests.ChromeTests.
-  """
-
-  def _DefaultCommand(self, tool, exe=None, valgrind_test_args=None):
-    """Override command-building method so we can add more suppressions."""
-    cmd = chrome_tests.ChromeTests._DefaultCommand(self, tool, exe,
-                                                   valgrind_test_args)
-    # When ChromeTests._DefaultCommand has executed, it has setup suppression
-    # files based on what's found in the memcheck/ or tsan/ subdirectories of
-    # this script's location. If Mac or Windows is executing, additional
-    # platform specific files have also been added.
-    # Since only the ones located below this directory is added, we must also
-    # add the ones maintained by Chrome, located in ../../tools/valgrind.
-
-    # The idea is to look for --suppression arguments in the cmd list and add a
-    # modified copy of each suppression file, for the corresponding file in
-    # ../../tools/valgrind.
-    script_dir = path_utils.ScriptDir()
-    old_base, _ = os.path.split(script_dir)
-
-    checkout_src = os.path.abspath(os.path.join(script_dir, os.pardir,
-                                                os.pardir))
-    new_dir = os.path.join(checkout_src, 'tools', 'valgrind')
-    add_suppressions = []
-    for token in cmd:
-      if '--suppressions' in token:
-        add_suppressions.append(token.replace(script_dir, new_dir))
-    return add_suppressions + cmd
-
-
-def main(_):
-  parser = optparse.OptionParser('usage: %prog -b <dir> -t <test> <test args>')
-  parser.disable_interspersed_args()
-  parser.add_option('-b', '--build-dir',
-                    help=('Location of the compiler output. Can only be used '
-                          'when the test argument does not contain this path.'))
-  parser.add_option("--target", help="Debug or Release")
-  parser.add_option('-t', '--test', help='Test to run.')
-  parser.add_option('', '--baseline', action='store_true', default=False,
-                    help='Generate baseline data instead of validating')
-  parser.add_option('', '--gtest_filter',
-                    help='Additional arguments to --gtest_filter')
-  parser.add_option('', '--gtest_repeat',
-                    help='Argument for --gtest_repeat')
-  parser.add_option("--gtest_shuffle", action="store_true", default=False,
-                    help="Randomize tests' orders on every iteration.")
-  parser.add_option("--gtest_break_on_failure", action="store_true",
-                    default=False,
-                    help="Drop in to debugger on assertion failure. Also "
-                         "useful for forcing tests to exit with a stack dump "
-                         "on the first assertion failure when running with "
-                         "--gtest_repeat=-1")
-  parser.add_option('-v', '--verbose', action='store_true', default=False,
-                    help='Verbose output - enable debug log messages')
-  parser.add_option('', '--tool', dest='valgrind_tool', default='memcheck',
-                    help='Specify a valgrind tool to run the tests under')
-  parser.add_option('', '--tool_flags', dest='valgrind_tool_flags', default='',
-                    help='Specify custom flags for the selected valgrind tool')
-  parser.add_option('', '--keep_logs', action='store_true', default=False,
-                    help=('Store memory tool logs in the <tool>.logs directory '
-                          'instead of /tmp.\nThis can be useful for tool '
-                          'developers/maintainers.\nPlease note that the <tool>'
-                          '.logs directory will be clobbered on tool startup.'))
-  parser.add_option("--test-launcher-bot-mode", action="store_true",
-                    help="run the tests with --test-launcher-bot-mode")
-  parser.add_option("--test-launcher-total-shards", type=int,
-                    help="run the tests with --test-launcher-total-shards")
-  parser.add_option("--test-launcher-shard-index", type=int,
-                    help="run the tests with --test-launcher-shard-index")
-  options, args = parser.parse_args()
-
-  if options.verbose:
-    logging_utils.config_root(logging.DEBUG)
-  else:
-    logging_utils.config_root()
-
-  if not options.test:
-    parser.error('--test not specified')
-
-  # Support build dir both with and without the target.
-  if (options.target and options.build_dir and
-      not options.build_dir.endswith(options.target)):
-    options.build_dir = os.path.join(options.build_dir, options.target)
-
-  # If --build_dir is provided, prepend it to the test executable if needed.
-  test_executable = options.test
-  if options.build_dir and not test_executable.startswith(options.build_dir):
-    test_executable = os.path.join(options.build_dir, test_executable)
-  args = [test_executable] + args
-
-  test = LibyuvTest(options, args, 'cmdline')
-  return test.Run()
-
-if __name__ == '__main__':
-  return_code = main(sys.argv)
-  sys.exit(return_code)
diff --git a/tools_libyuv/valgrind/libyuv_tests.sh b/tools_libyuv/valgrind/libyuv_tests.sh
deleted file mode 100755
index 249032ca..00000000
--- a/tools_libyuv/valgrind/libyuv_tests.sh
+++ /dev/null
@@ -1,101 +0,0 @@
-#!/bin/bash
-# Copyright (c) 2012 The LibYuv Project Authors. All rights reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-# Set up some paths and re-direct the arguments to libyuv_tests.py
-
-# This script is a copy of the chrome_tests.sh wrapper script with the following
-# changes:
-# - The locate_valgrind.sh of Chromium's Valgrind scripts dir is used to locate
-#   the Valgrind framework install. If it fails a fallback path is used instead
-#   (../../chromium/src/third_party/valgrind/linux_x64) and a warning message
-#   is showed by |show_locate_valgrind_failed_warning|.
-# - libyuv_tests.py is invoked instead of chrome_tests.py.
-# - Chromium's Valgrind scripts directory is added to the PYTHONPATH to make it
-#   possible to execute the Python scripts properly.
-
-export THISDIR=`dirname $0`
-ARGV_COPY="$@"
-
-# We need to set CHROME_VALGRIND iff using Memcheck:
-#   tools_libyuv/valgrind/libyuv_tests.sh --tool memcheck
-# or
-#   tools_libyuv/valgrind/libyuv_tests.sh --tool=memcheck
-tool="memcheck"  # Default to memcheck.
-while (( "$#" ))
-do
-  if [[ "$1" == "--tool" ]]
-  then
-    tool="$2"
-    shift
-  elif [[ "$1" =~ --tool=(.*) ]]
-  then
-    tool="${BASH_REMATCH[1]}"
-  fi
-  shift
-done
-
-NEEDS_VALGRIND=0
-
-case "$tool" in
-  "memcheck")
-    NEEDS_VALGRIND=1
-    ;;
-esac
-
-# For libyuv, we'll use the locate_valgrind.sh script in Chromium's Valgrind
-# scripts dir to locate the Valgrind framework install
-CHROME_VALGRIND_SCRIPTS=$THISDIR/../../tools/valgrind
-
-if [ "$NEEDS_VALGRIND" == "1" ]
-then
-  CHROME_VALGRIND=`sh $THISDIR/locate_valgrind.sh`
-  if [ "$CHROME_VALGRIND" = "" ]
-  then
-    CHROME_VALGRIND=../../src/third_party/valgrind/linux_x64
-    echo
-    echo "-------------------- WARNING ------------------------"
-    echo "locate_valgrind.sh failed."
-    echo "Using $CHROME_VALGRIND as a fallback location."
-    echo "This might be because:"
-    echo "1) This is a swarming bot"
-    echo "2) You haven't set up the valgrind binaries correctly."
-    echo "In this case, please make sure you have followed the instructions at"
-    echo "http://www.chromium.org/developers/how-tos/using-valgrind/get-valgrind"
-    echo "Notice: In the .gclient file, you need to add this for the 'libyuv'"
-    echo "solution since our directory structure is different from Chromium's:"
-    echo "\"custom_deps\": {"
-    echo "  \"libyuv/third_party/valgrind\":"
-    echo "      \"https://chromium.googlesource.com/chromium/deps/valgrind/binaries\","
-    echo "},"
-    echo "-----------------------------------------------------"
-    echo
-  fi
-  echo "Using valgrind binaries from ${CHROME_VALGRIND}"
-
-  PATH="${CHROME_VALGRIND}/bin:$PATH"
-  # We need to set these variables to override default lib paths hard-coded into
-  # Valgrind binary.
-  export VALGRIND_LIB="$CHROME_VALGRIND/lib/valgrind"
-  export VALGRIND_LIB_INNER="$CHROME_VALGRIND/lib/valgrind"
-
-  # Clean up some /tmp directories that might be stale due to interrupted
-  # chrome_tests.py execution.
-  # FYI:
-  #   -mtime +1  <- only print files modified more than 24h ago,
-  #   -print0/-0 are needed to handle possible newlines in the filenames.
-  echo "Cleanup /tmp from Valgrind stuff"
-  find /tmp -maxdepth 1 \(\
-        -name "vgdb-pipe-*" -or -name "vg_logs_*" -or -name "valgrind.*" \
-      \) -mtime +1 -print0 | xargs -0 rm -rf
-fi
-
-# Add Chrome's Valgrind scripts dir to the PYTHON_PATH since it contains
-# the scripts that are needed for this script to run
-PYTHONPATH=$THISDIR/../../tools/python/google:$CHROME_VALGRIND_SCRIPTS python \
-           "$THISDIR/libyuv_tests.py" $ARGV_COPY
diff --git a/tools_libyuv/valgrind/locate_valgrind.sh b/tools_libyuv/valgrind/locate_valgrind.sh
deleted file mode 100755
index d9594f48..00000000
--- a/tools_libyuv/valgrind/locate_valgrind.sh
+++ /dev/null
@@ -1,73 +0,0 @@
-#!/bin/bash
-
-# Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-# Prints a path to Valgrind binaries to be used for Chromium.
-# Select the valgrind from third_party/valgrind by default,
-# but allow users to override this default without editing scripts and
-# without specifying a commandline option
-
-export THISDIR=`dirname $0`
-
-# User may use their own valgrind by giving its path with CHROME_VALGRIND env.
-if [ "$CHROME_VALGRIND" = "" ]
-then
-  # Guess which binaries we should use by uname
-  case "$(uname -a)" in
-  *Linux*x86_64*)
-    PLATFORM="linux_x64"
-    ;;
-  *Linux*86*)
-    PLATFORM="linux_x86"
-    ;;
-  *Darwin*9.[678].[01]*i386*)
-    # Didn't test other kernels.
-    PLATFORM="mac"
-    ;;
-  *Darwin*10.[0-9].[0-9]*i386*)
-    PLATFORM="mac_10.6"
-    ;;
-  *Darwin*10.[0-9].[0-9]*x86_64*)
-    PLATFORM="mac_10.6"
-    ;;
-  *Darwin*11.[0-9].[0-9]*x86_64*)
-    PLATFORM="mac_10.7"
-    ;;
-  *)
-    (echo "Sorry, your platform is not supported:" &&
-     uname -a
-     echo
-     echo "If you're on Mac OS X, please see http://crbug.com/441425") >&2
-    exit 42
-  esac
-
-  # The binaries should be in third_party/valgrind
-  # (checked out from deps/third_party/valgrind/binaries).
-  CHROME_VALGRIND="$THISDIR/../../third_party/valgrind/$PLATFORM"
-
-  # TODO(timurrrr): readlink -f is not present on Mac...
-  if [ "$PLATFORM" != "mac" ] && \
-    [ "$PLATFORM" != "mac_10.6" ] && \
-    [ "$PLATFORM" != "mac_10.7" ]
-  then
-    # Get rid of all "../" dirs
-    CHROME_VALGRIND=$(readlink -f $CHROME_VALGRIND)
-  fi
-fi
-
-if ! test -x $CHROME_VALGRIND/bin/valgrind
-then
-  echo "Oops, could not find Valgrind binaries in your checkout." >&2
-  echo "Please see" >&2
-  echo "  http://dev.chromium.org/developers/how-tos/using-valgrind/get-valgrind" >&2
-  echo "for the instructions on how to download pre-built binaries." >&2
-  exit 1
-fi
-
-echo $CHROME_VALGRIND
diff --git a/tools_libyuv/valgrind/memcheck/OWNERS b/tools_libyuv/valgrind/memcheck/OWNERS
deleted file mode 100644
index 72e8ffc0..00000000
--- a/tools_libyuv/valgrind/memcheck/OWNERS
+++ /dev/null
@@ -1 +0,0 @@
-*
diff --git a/tools_libyuv/valgrind/memcheck/PRESUBMIT.py b/tools_libyuv/valgrind/memcheck/PRESUBMIT.py
deleted file mode 100644
index 03329214..00000000
--- a/tools_libyuv/valgrind/memcheck/PRESUBMIT.py
+++ /dev/null
@@ -1,99 +0,0 @@
-#!/usr/bin/env python
-# Copyright (c) 2012 The LibYuv Project Authors. All rights reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-"""
-Copied from Chrome's src/tools/valgrind/memcheck/PRESUBMIT.py
-
-See http://dev.chromium.org/developers/how-tos/depottools/presubmit-scripts
-for more details on the presubmit API built into gcl.
-"""
-
-import os
-import re
-import sys
-
-def CheckChange(input_api, output_api):
-  """Checks the memcheck suppressions files for bad data."""
-
-  # Add the path to the Chrome valgrind dir to the import path:
-  tools_vg_path = os.path.join(input_api.PresubmitLocalPath(), '..', '..', '..',
-                               'tools', 'valgrind')
-  sys.path.append(tools_vg_path)
-  import suppressions
-
-  sup_regex = re.compile('suppressions.*\.txt$')
-  suppressions = {}
-  errors = []
-  check_for_memcheck = False
-  # skip_next_line has 3 possible values:
-  # - False: don't skip the next line.
-  # - 'skip_suppression_name': the next line is a suppression name, skip.
-  # - 'skip_param': the next line is a system call parameter error, skip.
-  skip_next_line = False
-  for f in filter(lambda x: sup_regex.search(x.LocalPath()),
-                  input_api.AffectedFiles()):
-    for line, line_num in zip(f.NewContents(),
-                              xrange(1, len(f.NewContents()) + 1)):
-      line = line.lstrip()
-      if line.startswith('#') or not line:
-        continue
-
-      if skip_next_line:
-        if skip_next_line == 'skip_suppression_name':
-          if 'insert_a_suppression_name_here' in line:
-            errors.append('"insert_a_suppression_name_here" is not a valid '
-                          'suppression name')
-          if suppressions.has_key(line):
-            if f.LocalPath() == suppressions[line][1]:
-              errors.append('suppression with name "%s" at %s line %s '
-                            'has already been defined at line %s' %
-                            (line, f.LocalPath(), line_num,
-                             suppressions[line][1]))
-            else:
-              errors.append('suppression with name "%s" at %s line %s '
-                            'has already been defined at %s line %s' %
-                            (line, f.LocalPath(), line_num,
-                             suppressions[line][0], suppressions[line][1]))
-          else:
-            suppressions[line] = (f, line_num)
-            check_for_memcheck = True;
-        skip_next_line = False
-        continue
-      if check_for_memcheck:
-        if not line.startswith('Memcheck:'):
-          errors.append('"%s" should be "Memcheck:..." in %s line %s' %
-                        (line, f.LocalPath(), line_num))
-        check_for_memcheck = False;
-      if line == '{':
-        skip_next_line = 'skip_suppression_name'
-        continue
-      if line == "Memcheck:Param":
-        skip_next_line = 'skip_param'
-        continue
-
-      if (line.startswith('fun:') or line.startswith('obj:') or
-          line.startswith('Memcheck:') or line == '}' or
-          line == '...'):
-        continue
-      errors.append('"%s" is probably wrong: %s line %s' % (line, f.LocalPath(),
-                                                            line_num))
-  if errors:
-    return [output_api.PresubmitError('\n'.join(errors))]
-  return []
-
-def CheckChangeOnUpload(input_api, output_api):
-  return CheckChange(input_api, output_api)
-
-def CheckChangeOnCommit(input_api, output_api):
-  return CheckChange(input_api, output_api)
-
-def GetPreferredTrySlaves():
-  # We don't have any memcheck slaves yet, so there's no use for this method.
-  # When we have, the slave name(s) should be put into this list.
-  return []
diff --git a/tools_libyuv/valgrind/memcheck/suppressions.txt b/tools_libyuv/valgrind/memcheck/suppressions.txt
deleted file mode 100644
index 3f0f6d44..00000000
--- a/tools_libyuv/valgrind/memcheck/suppressions.txt
+++ /dev/null
@@ -1,21 +0,0 @@
-# This file is used in addition to the one already maintained in Chrome.
-# It acts as a place holder for future additions for this project.
-# It must exist for the Python wrapper script to work properly.
-
-# There are two of suppressions in this file.
-# 1. third_party libraries
-# 2. libyuv stuff
-# 3. libjingle stuff (talk folder)
-#-----------------------------------------------------------------------
-
-# third_party libraries
-{
-   bug_729
-   Memcheck:Free
-   fun:_ZdaPv
-   ...
-   fun:_ZN7testing8internal12UnitTestImplD1Ev
-   ...
-}
-
-# libyuv (empty so far)
diff --git a/tools_libyuv/valgrind/memcheck/suppressions_mac.txt b/tools_libyuv/valgrind/memcheck/suppressions_mac.txt
deleted file mode 100644
index 3ad0c8cc..00000000
--- a/tools_libyuv/valgrind/memcheck/suppressions_mac.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-# This file is used in addition to the one already maintained in Chrome.
-# It acts as a place holder for future additions for this project.
-# It must exist for the Python wrapper script to work properly.
-
-
diff --git a/tools_libyuv/valgrind/memcheck/suppressions_win32.txt b/tools_libyuv/valgrind/memcheck/suppressions_win32.txt
deleted file mode 100644
index 3ad0c8cc..00000000
--- a/tools_libyuv/valgrind/memcheck/suppressions_win32.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-# This file is used in addition to the one already maintained in Chrome.
-# It acts as a place holder for future additions for this project.
-# It must exist for the Python wrapper script to work properly.
-
-
diff --git a/tools_libyuv/valgrind/memcheck_analyze.py b/tools_libyuv/valgrind/memcheck_analyze.py
deleted file mode 100755
index 80e85eb4..00000000
--- a/tools_libyuv/valgrind/memcheck_analyze.py
+++ /dev/null
@@ -1,644 +0,0 @@
-#!/usr/bin/env python
-# Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-# memcheck_analyze.py
-
-''' Given a valgrind XML file, parses errors and uniques them.'''
-
-import gdb_helper
-
-from collections import defaultdict
-import hashlib
-import logging
-import optparse
-import os
-import re
-import subprocess
-import sys
-import time
-from xml.dom.minidom import parse
-from xml.parsers.expat import ExpatError
-
-import common
-
-# Global symbol table (yuck)
-TheAddressTable = None
-
-# These are regexps that define functions (using C++ mangled names)
-# we don't want to see in stack traces while pretty printing
-# or generating suppressions.
-# Just stop printing the stack/suppression frames when the current one
-# matches any of these.
-_BORING_CALLERS = common.BoringCallers(mangled=True, use_re_wildcards=True)
-
-def getTextOf(top_node, name):
-  ''' Returns all text in all DOM nodes with a certain |name| that are children
-  of |top_node|.
-  '''
-
-  text = ""
-  for nodes_named in top_node.getElementsByTagName(name):
-    text += "".join([node.data for node in nodes_named.childNodes
-                     if node.nodeType == node.TEXT_NODE])
-  return text
-
-def getCDATAOf(top_node, name):
-  ''' Returns all CDATA in all DOM nodes with a certain |name| that are children
-  of |top_node|.
-  '''
-
-  text = ""
-  for nodes_named in top_node.getElementsByTagName(name):
-    text += "".join([node.data for node in nodes_named.childNodes
-                     if node.nodeType == node.CDATA_SECTION_NODE])
-  if (text == ""):
-    return None
-  return text
-
-def shortenFilePath(source_dir, directory):
-  '''Returns a string with the string prefix |source_dir| removed from
-  |directory|.'''
-  prefixes_to_cut = ["build/src/", "valgrind/coregrind/", "out/Release/../../"]
-
-  if source_dir:
-    prefixes_to_cut.append(source_dir)
-
-  for p in prefixes_to_cut:
-    index = directory.rfind(p)
-    if index != -1:
-      directory = directory[index + len(p):]
-
-  return directory
-
-# Constants that give real names to the abbreviations in valgrind XML output.
-INSTRUCTION_POINTER = "ip"
-OBJECT_FILE = "obj"
-FUNCTION_NAME = "fn"
-SRC_FILE_DIR = "dir"
-SRC_FILE_NAME = "file"
-SRC_LINE = "line"
-
-def gatherFrames(node, source_dir):
-  frames = []
-  for frame in node.getElementsByTagName("frame"):
-    frame_dict = {
-      INSTRUCTION_POINTER : getTextOf(frame, INSTRUCTION_POINTER),
-      OBJECT_FILE         : getTextOf(frame, OBJECT_FILE),
-      FUNCTION_NAME       : getTextOf(frame, FUNCTION_NAME),
-      SRC_FILE_DIR        : shortenFilePath(
-          source_dir, getTextOf(frame, SRC_FILE_DIR)),
-      SRC_FILE_NAME       : getTextOf(frame, SRC_FILE_NAME),
-      SRC_LINE            : getTextOf(frame, SRC_LINE)
-    }
-
-    # Ignore this frame and all the following if it's a "boring" function.
-    enough_frames = False
-    for regexp in _BORING_CALLERS:
-      if re.match("^%s$" % regexp, frame_dict[FUNCTION_NAME]):
-        enough_frames = True
-        break
-    if enough_frames:
-      break
-
-    frames += [frame_dict]
-
-    global TheAddressTable
-    if TheAddressTable != None and frame_dict[SRC_LINE] == "":
-      # Try using gdb
-      TheAddressTable.Add(frame_dict[OBJECT_FILE],
-                          frame_dict[INSTRUCTION_POINTER])
-  return frames
-
-class ValgrindError:
-  ''' Takes a <DOM Element: error> node and reads all the data from it. A
-  ValgrindError is immutable and is hashed on its pretty printed output.
-  '''
-
-  def __init__(self, source_dir, error_node, commandline, testcase):
-    ''' Copies all the relevant information out of the DOM and into object
-    properties.
-
-    Args:
-      error_node: The <error></error> DOM node we're extracting from.
-      source_dir: Prefix that should be stripped from the <dir> node.
-      commandline: The command that was run under valgrind
-      testcase: The test case name, if known.
-    '''
-
-    # Valgrind errors contain one <what><stack> pair, plus an optional
-    # <auxwhat><stack> pair, plus an optional <origin><what><stack></origin>,
-    # plus (since 3.5.0) a <suppression></suppression> pair.
-    # (Origin is nicely enclosed; too bad the other two aren't.)
-    # The most common way to see all three in one report is
-    # a syscall with a parameter that points to uninitialized memory, e.g.
-    # Format:
-    # <error>
-    #   <unique>0x6d</unique>
-    #   <tid>1</tid>
-    #   <kind>SyscallParam</kind>
-    #   <what>Syscall param write(buf) points to uninitialised byte(s)</what>
-    #   <stack>
-    #     <frame>
-    #     ...
-    #     </frame>
-    #   </stack>
-    #   <auxwhat>Address 0x5c9af4f is 7 bytes inside a block of ...</auxwhat>
-    #   <stack>
-    #     <frame>
-    #     ...
-    #     </frame>
-    #   </stack>
-    #   <origin>
-    #   <what>Uninitialised value was created by a heap allocation</what>
-    #   <stack>
-    #     <frame>
-    #     ...
-    #     </frame>
-    #   </stack>
-    #   </origin>
-    #   <suppression>
-    #     <sname>insert_a_suppression_name_here</sname>
-    #     <skind>Memcheck:Param</skind>
-    #     <skaux>write(buf)</skaux>
-    #     <sframe> <fun>__write_nocancel</fun> </sframe>
-    #     ...
-    #     <sframe> <fun>main</fun> </sframe>
-    #     <rawtext>
-    # <![CDATA[
-    # {
-    #    <insert_a_suppression_name_here>
-    #    Memcheck:Param
-    #    write(buf)
-    #    fun:__write_nocancel
-    #    ...
-    #    fun:main
-    # }
-    # ]]>
-    #     </rawtext>
-    #   </suppression>
-    # </error>
-    #
-    # Each frame looks like this:
-    #  <frame>
-    #    <ip>0x83751BC</ip>
-    #    <obj>/data/dkegel/chrome-build/src/out/Release/base_unittests</obj>
-    #    <fn>_ZN7testing8internal12TestInfoImpl7RunTestEPNS_8TestInfoE</fn>
-    #    <dir>/data/dkegel/chrome-build/src/testing/gtest/src</dir>
-    #    <file>gtest-internal-inl.h</file>
-    #    <line>655</line>
-    #  </frame>
-    # although the dir, file, and line elements are missing if there is
-    # no debug info.
-
-    self._kind = getTextOf(error_node, "kind")
-    self._backtraces = []
-    self._suppression = None
-    self._commandline = commandline
-    self._testcase = testcase
-    self._additional = []
-
-    # Iterate through the nodes, parsing <what|auxwhat><stack> pairs.
-    description = None
-    for node in error_node.childNodes:
-      if node.localName == "what" or node.localName == "auxwhat":
-        description = "".join([n.data for n in node.childNodes
-                              if n.nodeType == n.TEXT_NODE])
-      elif node.localName == "xwhat":
-        description = getTextOf(node, "text")
-      elif node.localName == "stack":
-        assert description
-        self._backtraces.append([description, gatherFrames(node, source_dir)])
-        description = None
-      elif node.localName == "origin":
-        description = getTextOf(node, "what")
-        stack = node.getElementsByTagName("stack")[0]
-        frames = gatherFrames(stack, source_dir)
-        self._backtraces.append([description, frames])
-        description = None
-        stack = None
-        frames = None
-      elif description and node.localName != None:
-        # The lastest description has no stack, e.g. "Address 0x28 is unknown"
-        self._additional.append(description)
-        description = None
-
-      if node.localName == "suppression":
-        self._suppression = getCDATAOf(node, "rawtext");
-
-  def __str__(self):
-    ''' Pretty print the type and backtrace(s) of this specific error,
-        including suppression (which is just a mangled backtrace).'''
-    output = ""
-    output += "\n" # Make sure the ### is at the beginning of line.
-    output += "### BEGIN MEMORY TOOL REPORT (error hash=#%016X#)\n" % \
-        self.ErrorHash()
-    if (self._commandline):
-      output += self._commandline + "\n"
-
-    output += self._kind + "\n"
-    for backtrace in self._backtraces:
-      output += backtrace[0] + "\n"
-      filter = subprocess.Popen("c++filt -n", stdin=subprocess.PIPE,
-                                stdout=subprocess.PIPE,
-                                stderr=subprocess.STDOUT,
-                                shell=True,
-                                close_fds=True)
-      buf = ""
-      for frame in backtrace[1]:
-        buf +=  (frame[FUNCTION_NAME] or frame[INSTRUCTION_POINTER]) + "\n"
-      (stdoutbuf, stderrbuf) = filter.communicate(buf.encode('latin-1'))
-      demangled_names = stdoutbuf.split("\n")
-
-      i = 0
-      for frame in backtrace[1]:
-        output += ("  " + demangled_names[i])
-        i = i + 1
-
-        global TheAddressTable
-        if TheAddressTable != None and frame[SRC_FILE_DIR] == "":
-           # Try using gdb
-           foo = TheAddressTable.GetFileLine(frame[OBJECT_FILE],
-                                             frame[INSTRUCTION_POINTER])
-           if foo[0] != None:
-             output += (" (" + foo[0] + ":" + foo[1] + ")")
-        elif frame[SRC_FILE_DIR] != "":
-          output += (" (" + frame[SRC_FILE_DIR] + "/" + frame[SRC_FILE_NAME] +
-                     ":" + frame[SRC_LINE] + ")")
-        else:
-          output += " (" + frame[OBJECT_FILE] + ")"
-        output += "\n"
-
-    for additional in self._additional:
-      output += additional + "\n"
-
-    assert self._suppression != None, "Your Valgrind doesn't generate " \
-                                      "suppressions - is it too old?"
-
-    if self._testcase:
-      output += "The report came from the `%s` test.\n" % self._testcase
-    output += "Suppression (error hash=#%016X#):\n" % self.ErrorHash()
-    output += ("  For more info on using suppressions see "
-               "http://dev.chromium.org/developers/tree-sheriffs/sheriff-details-chromium/memory-sheriff#TOC-Suppressing-memory-reports")
-
-    # Widen suppression slightly to make portable between mac and linux
-    # TODO(timurrrr): Oops, these transformations should happen
-    # BEFORE calculating the hash!
-    supp = self._suppression;
-    supp = supp.replace("fun:_Znwj", "fun:_Znw*")
-    supp = supp.replace("fun:_Znwm", "fun:_Znw*")
-    supp = supp.replace("fun:_Znaj", "fun:_Zna*")
-    supp = supp.replace("fun:_Znam", "fun:_Zna*")
-
-    # Make suppressions even less platform-dependent.
-    for sz in [1, 2, 4, 8]:
-      supp = supp.replace("Memcheck:Addr%d" % sz, "Memcheck:Unaddressable")
-      supp = supp.replace("Memcheck:Value%d" % sz, "Memcheck:Uninitialized")
-    supp = supp.replace("Memcheck:Cond", "Memcheck:Uninitialized")
-
-    # Split into lines so we can enforce length limits
-    supplines = supp.split("\n")
-    supp = None  # to avoid re-use
-
-    # Truncate at line 26 (VG_MAX_SUPP_CALLERS plus 2 for name and type)
-    # or at the first 'boring' caller.
-    # (https://bugs.kde.org/show_bug.cgi?id=199468 proposes raising
-    # VG_MAX_SUPP_CALLERS, but we're probably fine with it as is.)
-    newlen = min(26, len(supplines));
-
-    # Drop boring frames and all the following.
-    enough_frames = False
-    for frameno in range(newlen):
-      for boring_caller in _BORING_CALLERS:
-        if re.match("^ +fun:%s$" % boring_caller, supplines[frameno]):
-          newlen = frameno
-          enough_frames = True
-          break
-      if enough_frames:
-        break
-    if (len(supplines) > newlen):
-      supplines = supplines[0:newlen]
-      supplines.append("}")
-
-    for frame in range(len(supplines)):
-      # Replace the always-changing anonymous namespace prefix with "*".
-      m = re.match("( +fun:)_ZN.*_GLOBAL__N_.*\.cc_" +
-                   "[0-9a-fA-F]{8}_[0-9a-fA-F]{8}(.*)",
-                   supplines[frame])
-      if m:
-        supplines[frame] = "*".join(m.groups())
-
-    output += "\n".join(supplines) + "\n"
-    output += "### END MEMORY TOOL REPORT (error hash=#%016X#)\n" % \
-        self.ErrorHash()
-
-    return output
-
-  def UniqueString(self):
-    ''' String to use for object identity. Don't print this, use str(obj)
-    instead.'''
-    rep = self._kind + " "
-    for backtrace in self._backtraces:
-      for frame in backtrace[1]:
-        rep += frame[FUNCTION_NAME]
-
-        if frame[SRC_FILE_DIR] != "":
-          rep += frame[SRC_FILE_DIR] + "/" + frame[SRC_FILE_NAME]
-        else:
-          rep += frame[OBJECT_FILE]
-
-    return rep
-
-  # This is a device-independent hash identifying the suppression.
-  # By printing out this hash we can find duplicate reports between tests and
-  # different shards running on multiple buildbots
-  def ErrorHash(self):
-    return int(hashlib.md5(self.UniqueString()).hexdigest()[:16], 16)
-
-  def __hash__(self):
-    return hash(self.UniqueString())
-  def __eq__(self, rhs):
-    return self.UniqueString() == rhs
-
-def log_is_finished(f, force_finish):
-  f.seek(0)
-  prev_line = ""
-  while True:
-    line = f.readline()
-    if line == "":
-      if not force_finish:
-        return False
-      # Okay, the log is not finished but we can make it up to be parseable:
-      if prev_line.strip() in ["</error>", "</errorcounts>", "</status>"]:
-        f.write("</valgrindoutput>\n")
-        return True
-      return False
-    if '</valgrindoutput>' in line:
-      # Valgrind often has garbage after </valgrindoutput> upon crash.
-      f.truncate()
-      return True
-    prev_line = line
-
-class MemcheckAnalyzer:
-  ''' Given a set of Valgrind XML files, parse all the errors out of them,
-  unique them and output the results.'''
-
-  SANITY_TEST_SUPPRESSIONS = {
-      "Memcheck sanity test 01 (memory leak).": 1,
-      "Memcheck sanity test 02 (malloc/read left).": 1,
-      "Memcheck sanity test 03 (malloc/read right).": 1,
-      "Memcheck sanity test 04 (malloc/write left).": 1,
-      "Memcheck sanity test 05 (malloc/write right).": 1,
-      "Memcheck sanity test 06 (new/read left).": 1,
-      "Memcheck sanity test 07 (new/read right).": 1,
-      "Memcheck sanity test 08 (new/write left).": 1,
-      "Memcheck sanity test 09 (new/write right).": 1,
-      "Memcheck sanity test 10 (write after free).": 1,
-      "Memcheck sanity test 11 (write after delete).": 1,
-      "Memcheck sanity test 12 (array deleted without []).": 1,
-      "Memcheck sanity test 13 (single element deleted with []).": 1,
-      "Memcheck sanity test 14 (malloc/read uninit).": 1,
-      "Memcheck sanity test 15 (new/read uninit).": 1,
-  }
-
-  # Max time to wait for memcheck logs to complete.
-  LOG_COMPLETION_TIMEOUT = 180.0
-
-  def __init__(self, source_dir, show_all_leaks=False, use_gdb=False):
-    '''Create a parser for Memcheck logs.
-
-    Args:
-      source_dir: Path to top of source tree for this build
-      show_all_leaks: Whether to show even less important leaks
-      use_gdb: Whether to use gdb to resolve source filenames and line numbers
-               in the report stacktraces
-    '''
-    self._source_dir = source_dir
-    self._show_all_leaks = show_all_leaks
-    self._use_gdb = use_gdb
-
-    # Contains the set of unique errors
-    self._errors = set()
-
-    # Contains the time when the we started analyzing the first log file.
-    # This variable is used to skip incomplete logs after some timeout.
-    self._analyze_start_time = None
-
-
-  def Report(self, files, testcase, check_sanity=False):
-    '''Reads in a set of files and prints Memcheck report.
-
-    Args:
-      files: A list of filenames.
-      check_sanity: if true, search for SANITY_TEST_SUPPRESSIONS
-    '''
-    # Beyond the detailed errors parsed by ValgrindError above,
-    # the xml file contain records describing suppressions that were used:
-    # <suppcounts>
-    #  <pair>
-    #    <count>28</count>
-    #    <name>pango_font_leak_todo</name>
-    #  </pair>
-    #  <pair>
-    #    <count>378</count>
-    #    <name>bug_13243</name>
-    #  </pair>
-    # </suppcounts
-    # Collect these and print them at the end.
-    #
-    # With our patch for https://bugs.kde.org/show_bug.cgi?id=205000 in,
-    # the file also includes records of the form
-    # <load_obj><obj>/usr/lib/libgcc_s.1.dylib</obj><ip>0x27000</ip></load_obj>
-    # giving the filename and load address of each binary that was mapped
-    # into the process.
-
-    global TheAddressTable
-    if self._use_gdb:
-      TheAddressTable = gdb_helper.AddressTable()
-    else:
-      TheAddressTable = None
-    cur_report_errors = set()
-    suppcounts = defaultdict(int)
-    badfiles = set()
-
-    if self._analyze_start_time == None:
-      self._analyze_start_time = time.time()
-    start_time = self._analyze_start_time
-
-    parse_failed = False
-    for file in files:
-      # Wait up to three minutes for valgrind to finish writing all files,
-      # but after that, just skip incomplete files and warn.
-      f = open(file, "r+")
-      pid = re.match(".*\.([0-9]+)$", file)
-      if pid:
-        pid = pid.groups()[0]
-      found = False
-      running = True
-      firstrun = True
-      skip = False
-      origsize = os.path.getsize(file)
-      while (running and not found and not skip and
-             (firstrun or
-              ((time.time() - start_time) < self.LOG_COMPLETION_TIMEOUT))):
-        firstrun = False
-        f.seek(0)
-        if pid:
-          # Make sure the process is still running so we don't wait for
-          # 3 minutes if it was killed. See http://crbug.com/17453
-          ps_out = subprocess.Popen("ps p %s" % pid, shell=True,
-                                    stdout=subprocess.PIPE).stdout
-          if len(ps_out.readlines()) < 2:
-            running = False
-        else:
-          skip = True
-          running = False
-        found = log_is_finished(f, False)
-        if not running and not found:
-          logging.warn("Valgrind process PID = %s is not running but its "
-                       "XML log has not been finished correctly.\n"
-                       "Make it up by adding some closing tags manually." % pid)
-          found = log_is_finished(f, not running)
-        if running and not found:
-          time.sleep(1)
-      f.close()
-      if not found:
-        badfiles.add(file)
-      else:
-        newsize = os.path.getsize(file)
-        if origsize > newsize+1:
-          logging.warn(str(origsize - newsize) +
-                       " bytes of junk were after </valgrindoutput> in %s!" %
-                       file)
-        try:
-          parsed_file = parse(file);
-        except ExpatError, e:
-          parse_failed = True
-          logging.warn("could not parse %s: %s" % (file, e))
-          lineno = e.lineno - 1
-          context_lines = 5
-          context_start = max(0, lineno - context_lines)
-          context_end = lineno + context_lines + 1
-          context_file = open(file, "r")
-          for i in range(0, context_start):
-            context_file.readline()
-          for i in range(context_start, context_end):
-            context_data = context_file.readline().rstrip()
-            if i != lineno:
-              logging.warn("  %s" % context_data)
-            else:
-              logging.warn("> %s" % context_data)
-          context_file.close()
-          continue
-        if TheAddressTable != None:
-          load_objs = parsed_file.getElementsByTagName("load_obj")
-          for load_obj in load_objs:
-            obj = getTextOf(load_obj, "obj")
-            ip = getTextOf(load_obj, "ip")
-            TheAddressTable.AddBinaryAt(obj, ip)
-
-        commandline = None
-        preamble = parsed_file.getElementsByTagName("preamble")[0];
-        for node in preamble.getElementsByTagName("line"):
-          if node.localName == "line":
-            for x in node.childNodes:
-              if x.nodeType == node.TEXT_NODE and "Command" in x.data:
-                commandline = x.data
-                break
-
-        raw_errors = parsed_file.getElementsByTagName("error")
-        for raw_error in raw_errors:
-          # Ignore "possible" leaks for now by default.
-          if (self._show_all_leaks or
-              getTextOf(raw_error, "kind") != "Leak_PossiblyLost"):
-            error = ValgrindError(self._source_dir,
-                                  raw_error, commandline, testcase)
-            if error not in cur_report_errors:
-              # We haven't seen such errors doing this report yet...
-              if error in self._errors:
-                # ... but we saw it in earlier reports, e.g. previous UI test
-                cur_report_errors.add("This error was already printed in "
-                                      "some other test, see 'hash=#%016X#'" % \
-                                      error.ErrorHash())
-              else:
-                # ... and we haven't seen it in other tests as well
-                self._errors.add(error)
-                cur_report_errors.add(error)
-
-        suppcountlist = parsed_file.getElementsByTagName("suppcounts")
-        if len(suppcountlist) > 0:
-          suppcountlist = suppcountlist[0]
-          for node in suppcountlist.getElementsByTagName("pair"):
-            count = getTextOf(node, "count");
-            name = getTextOf(node, "name");
-            suppcounts[name] += int(count)
-
-    if len(badfiles) > 0:
-      logging.warn("valgrind didn't finish writing %d files?!" % len(badfiles))
-      for file in badfiles:
-        logging.warn("Last 20 lines of %s :" % file)
-        os.system("tail -n 20 '%s' 1>&2" % file)
-
-    if parse_failed:
-      logging.error("FAIL! Couldn't parse Valgrind output file")
-      return -2
-
-    common.PrintUsedSuppressionsList(suppcounts)
-
-    retcode = 0
-    if cur_report_errors:
-      logging.error("FAIL! There were %s errors: " % len(cur_report_errors))
-
-      if TheAddressTable != None:
-        TheAddressTable.ResolveAll()
-
-      for error in cur_report_errors:
-        logging.error(error)
-
-      retcode = -1
-
-    # Report tool's insanity even if there were errors.
-    if check_sanity:
-      remaining_sanity_supp = MemcheckAnalyzer.SANITY_TEST_SUPPRESSIONS
-      for (name, count) in suppcounts.iteritems():
-        # Workaround for http://crbug.com/334074
-        if (name in remaining_sanity_supp and
-            remaining_sanity_supp[name] <= count):
-          del remaining_sanity_supp[name]
-      if remaining_sanity_supp:
-        logging.error("FAIL! Sanity check failed!")
-        logging.info("The following test errors were not handled: ")
-        for (name, count) in remaining_sanity_supp.iteritems():
-          logging.info("  * %dx %s" % (count, name))
-        retcode = -3
-
-    if retcode != 0:
-      return retcode
-
-    logging.info("PASS! No errors found!")
-    return 0
-
-
-def _main():
-  '''For testing only. The MemcheckAnalyzer class should be imported instead.'''
-  parser = optparse.OptionParser("usage: %prog [options] <files to analyze>")
-  parser.add_option("", "--source-dir",
-                    help="path to top of source tree for this build"
-                    "(used to normalize source paths in baseline)")
-
-  (options, args) = parser.parse_args()
-  if len(args) == 0:
-    parser.error("no filename specified")
-  filenames = args
-
-  analyzer = MemcheckAnalyzer(options.source_dir, use_gdb=True)
-  return analyzer.Report(filenames, None)
-
-
-if __name__ == "__main__":
-  sys.exit(_main())
diff --git a/tools_libyuv/valgrind/valgrind.sh b/tools_libyuv/valgrind/valgrind.sh
deleted file mode 100755
index 7f3f7926..00000000
--- a/tools_libyuv/valgrind/valgrind.sh
+++ /dev/null
@@ -1,110 +0,0 @@
-#!/bin/bash
-
-# Copyright (c) 2017 The LibYuv Project Authors. All rights reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-# This is a small script for manually launching valgrind, along with passing
-# it the suppression file, and some helpful arguments (automatically attaching
-# the debugger on failures, etc).  Run it from your repo root, something like:
-#  $ sh ./tools/valgrind/valgrind.sh ./out/Debug/chrome
-#
-# This is mostly intended for running the chrome browser interactively.
-# To run unit tests, you probably want to run chrome_tests.sh instead.
-# That's the script used by the valgrind buildbot.
-
-export THISDIR=`dirname $0`
-
-setup_memcheck() {
-  RUN_COMMAND="valgrind"
-
-  # Prompt to attach gdb when there was an error detected.
-  DEFAULT_TOOL_FLAGS=("--db-command=gdb -nw %f %p" "--db-attach=yes" \
-                      # Keep the registers in gdb in sync with the code.
-                      "--vex-iropt-register-updates=allregs-at-mem-access" \
-                      # Overwrite newly allocated or freed objects
-                      # with 0x41 to catch inproper use.
-                      "--malloc-fill=41" "--free-fill=41" \
-                      # Increase the size of stacks being tracked.
-                      "--num-callers=30")
-}
-
-setup_unknown() {
-  echo "Unknown tool \"$TOOL_NAME\" specified, the result is not guaranteed"
-  DEFAULT_TOOL_FLAGS=()
-}
-
-set -e
-
-if [ $# -eq 0 ]; then
-  echo "usage: <command to run> <arguments ...>"
-  exit 1
-fi
-
-TOOL_NAME="memcheck"
-declare -a DEFAULT_TOOL_FLAGS[0]
-
-# Select a tool different from memcheck with --tool=TOOL as a first argument
-TMP_STR=`echo $1 | sed 's/^\-\-tool=//'`
-if [ "$TMP_STR" != "$1" ]; then
-  TOOL_NAME="$TMP_STR"
-  shift
-fi
-
-if echo "$@" | grep "\-\-tool" ; then
-  echo "--tool=TOOL must be the first argument" >&2
-  exit 1
-fi
-
-case $TOOL_NAME in
-  memcheck*)  setup_memcheck "$1";;
-  *)          setup_unknown;;
-esac
-
-
-SUPPRESSIONS="$THISDIR/$TOOL_NAME/suppressions.txt"
-
-CHROME_VALGRIND=`sh $THISDIR/locate_valgrind.sh`
-if [ "$CHROME_VALGRIND" = "" ]
-then
-  # locate_valgrind.sh failed
-  exit 1
-fi
-echo "Using valgrind binaries from ${CHROME_VALGRIND}"
-
-set -x
-PATH="${CHROME_VALGRIND}/bin:$PATH"
-# We need to set these variables to override default lib paths hard-coded into
-# Valgrind binary.
-export VALGRIND_LIB="$CHROME_VALGRIND/lib/valgrind"
-export VALGRIND_LIB_INNER="$CHROME_VALGRIND/lib/valgrind"
-
-# G_SLICE=always-malloc: make glib use system malloc
-# NSS_DISABLE_UNLOAD=1: make nss skip dlclosing dynamically loaded modules,
-# which would result in "obj:*" in backtraces.
-# NSS_DISABLE_ARENA_FREE_LIST=1: make nss use system malloc
-# G_DEBUG=fatal_warnings: make  GTK abort on any critical or warning assertions.
-# If it crashes on you in the Options menu, you hit bug 19751,
-# comment out the G_DEBUG=fatal_warnings line.
-#
-# GTEST_DEATH_TEST_USE_FORK=1: make gtest death tests valgrind-friendly
-#
-# When everyone has the latest valgrind, we might want to add
-#  --show-possibly-lost=no
-# to ignore possible but not definite leaks.
-
-G_SLICE=always-malloc \
-NSS_DISABLE_UNLOAD=1 \
-NSS_DISABLE_ARENA_FREE_LIST=1 \
-G_DEBUG=fatal_warnings \
-GTEST_DEATH_TEST_USE_FORK=1 \
-$RUN_COMMAND \
-  --trace-children=yes \
-  --leak-check=yes \
-  --suppressions="$SUPPRESSIONS" \
-  "${DEFAULT_TOOL_FLAGS[@]}" \
-  "$@"
diff --git a/tools_libyuv/valgrind/valgrind_test.py b/tools_libyuv/valgrind/valgrind_test.py
deleted file mode 100755
index 0fd3d97f..00000000
--- a/tools_libyuv/valgrind/valgrind_test.py
+++ /dev/null
@@ -1,517 +0,0 @@
-#!/usr/bin/env python
-# Copyright (c) 2017 The WebRTC project authors. All Rights Reserved.
-#
-# Use of this source code is governed by a BSD-style license
-# that can be found in the LICENSE file in the root of the source
-# tree. An additional intellectual property rights grant can be found
-# in the file PATENTS.  All contributing project authors may
-# be found in the AUTHORS file in the root of the source tree.
-
-"""Runs an exe through Valgrind and puts the intermediate files in a
-directory.
-"""
-
-import datetime
-import glob
-import logging
-import optparse
-import os
-import re
-import shutil
-import stat
-import subprocess
-import sys
-import tempfile
-
-import common
-
-import memcheck_analyze
-
-class BaseTool(object):
-  """Abstract class for running dynamic error detection tools.
-
-  Always subclass this and implement ToolCommand with framework- and
-  tool-specific stuff.
-  """
-
-  def __init__(self):
-    temp_parent_dir = None
-    self.log_parent_dir = ""
-    if common.IsWindows():
-      # gpu process on Windows Vista+ runs at Low Integrity and can only
-      # write to certain directories (http://crbug.com/119131)
-      #
-      # TODO(bruening): if scripts die in middle and don't clean up temp
-      # dir, we'll accumulate files in profile dir.  should remove
-      # really old files automatically.
-      profile = os.getenv("USERPROFILE")
-      if profile:
-        self.log_parent_dir = profile + "\\AppData\\LocalLow\\"
-        if os.path.exists(self.log_parent_dir):
-          self.log_parent_dir = common.NormalizeWindowsPath(self.log_parent_dir)
-          temp_parent_dir = self.log_parent_dir
-    # Generated every time (even when overridden)
-    self.temp_dir = tempfile.mkdtemp(prefix="vg_logs_", dir=temp_parent_dir)
-    self.log_dir = self.temp_dir # overridable by --keep_logs
-    self.option_parser_hooks = []
-    # TODO(glider): we may not need some of the env vars on some of the
-    # platforms.
-    self._env = {
-      "G_SLICE" : "always-malloc",
-      "NSS_DISABLE_UNLOAD" : "1",
-      "NSS_DISABLE_ARENA_FREE_LIST" : "1",
-      "GTEST_DEATH_TEST_USE_FORK": "1",
-    }
-
-  def ToolName(self):
-    raise NotImplementedError, "This method should be implemented " \
-                               "in the tool-specific subclass"
-
-  def Analyze(self, check_sanity=False):
-    raise NotImplementedError, "This method should be implemented " \
-                               "in the tool-specific subclass"
-
-  def RegisterOptionParserHook(self, hook):
-    # Frameworks and tools can add their own flags to the parser.
-    self.option_parser_hooks.append(hook)
-
-  def CreateOptionParser(self):
-    # Defines Chromium-specific flags.
-    self._parser = optparse.OptionParser("usage: %prog [options] <program to "
-                                         "test>")
-    self._parser.disable_interspersed_args()
-    self._parser.add_option("-t", "--timeout",
-                      dest="timeout", metavar="TIMEOUT", default=10000,
-                      help="timeout in seconds for the run (default 10000)")
-    self._parser.add_option("", "--build-dir",
-                            help="the location of the compiler output")
-    self._parser.add_option("", "--source-dir",
-                            help="path to top of source tree for this build"
-                                 "(used to normalize source paths in baseline)")
-    self._parser.add_option("", "--gtest_filter", default="",
-                            help="which test case to run")
-    self._parser.add_option("", "--gtest_repeat",
-                            help="how many times to run each test")
-    self._parser.add_option("", "--gtest_print_time", action="store_true",
-                            default=False,
-                            help="show how long each test takes")
-    self._parser.add_option("", "--ignore_exit_code", action="store_true",
-                            default=False,
-                            help="ignore exit code of the test "
-                                 "(e.g. test failures)")
-    self._parser.add_option("", "--keep_logs", action="store_true",
-                            default=False,
-                            help="store memory tool logs in the <tool>.logs "
-                                 "directory instead of /tmp.\nThis can be "
-                                 "useful for tool developers/maintainers.\n"
-                                 "Please note that the <tool>.logs directory "
-                                 "will be clobbered on tool startup.")
-
-    # To add framework- or tool-specific flags, please add a hook using
-    # RegisterOptionParserHook in the corresponding subclass.
-    # See ValgrindTool for an example.
-    for hook in self.option_parser_hooks:
-      hook(self, self._parser)
-
-  def ParseArgv(self, args):
-    self.CreateOptionParser()
-
-    # self._tool_flags will store those tool flags which we don't parse
-    # manually in this script.
-    self._tool_flags = []
-    known_args = []
-
-    """ We assume that the first argument not starting with "-" is a program
-    name and all the following flags should be passed to the program.
-    TODO(timurrrr): customize optparse instead
-    """
-    while len(args) > 0 and args[0][:1] == "-":
-      arg = args[0]
-      if (arg == "--"):
-        break
-      if self._parser.has_option(arg.split("=")[0]):
-        known_args += [arg]
-      else:
-        self._tool_flags += [arg]
-      args = args[1:]
-
-    if len(args) > 0:
-      known_args += args
-
-    self._options, self._args = self._parser.parse_args(known_args)
-
-    self._timeout = int(self._options.timeout)
-    self._source_dir = self._options.source_dir
-    if self._options.keep_logs:
-      # log_parent_dir has trailing slash if non-empty
-      self.log_dir = self.log_parent_dir + "%s.logs" % self.ToolName()
-      if os.path.exists(self.log_dir):
-        shutil.rmtree(self.log_dir)
-      os.mkdir(self.log_dir)
-      logging.info("Logs are in " + self.log_dir)
-
-    self._ignore_exit_code = self._options.ignore_exit_code
-    if self._options.gtest_filter != "":
-      self._args.append("--gtest_filter=%s" % self._options.gtest_filter)
-    if self._options.gtest_repeat:
-      self._args.append("--gtest_repeat=%s" % self._options.gtest_repeat)
-    if self._options.gtest_print_time:
-      self._args.append("--gtest_print_time")
-
-    return True
-
-  def Setup(self, args):
-    return self.ParseArgv(args)
-
-  def ToolCommand(self):
-    raise NotImplementedError, "This method should be implemented " \
-                               "in the tool-specific subclass"
-
-  def Cleanup(self):
-    # You may override it in the tool-specific subclass
-    pass
-
-  def Execute(self):
-    """ Execute the app to be tested after successful instrumentation.
-    Full execution command-line provided by subclassers via proc."""
-    logging.info("starting execution...")
-    proc = self.ToolCommand()
-    for var in self._env:
-      common.PutEnvAndLog(var, self._env[var])
-    return common.RunSubprocess(proc, self._timeout)
-
-  def RunTestsAndAnalyze(self, check_sanity):
-    exec_retcode = self.Execute()
-    analyze_retcode = self.Analyze(check_sanity)
-
-    if analyze_retcode:
-      logging.error("Analyze failed.")
-      logging.info("Search the log for '[ERROR]' to see the error reports.")
-      return analyze_retcode
-
-    if exec_retcode:
-      if self._ignore_exit_code:
-        logging.info("Test execution failed, but the exit code is ignored.")
-      else:
-        logging.error("Test execution failed.")
-        return exec_retcode
-    else:
-      logging.info("Test execution completed successfully.")
-
-    if not analyze_retcode:
-      logging.info("Analysis completed successfully.")
-
-    return 0
-
-  def Main(self, args, check_sanity, min_runtime_in_seconds):
-    """Call this to run through the whole process: Setup, Execute, Analyze"""
-    start_time = datetime.datetime.now()
-    retcode = -1
-    if self.Setup(args):
-      retcode = self.RunTestsAndAnalyze(check_sanity)
-      shutil.rmtree(self.temp_dir, ignore_errors=True)
-      self.Cleanup()
-    else:
-      logging.error("Setup failed")
-    end_time = datetime.datetime.now()
-    runtime_in_seconds = (end_time - start_time).seconds
-    hours = runtime_in_seconds / 3600
-    seconds = runtime_in_seconds % 3600
-    minutes = seconds / 60
-    seconds = seconds % 60
-    logging.info("elapsed time: %02d:%02d:%02d" % (hours, minutes, seconds))
-    if (min_runtime_in_seconds > 0 and
-        runtime_in_seconds < min_runtime_in_seconds):
-      logging.error("Layout tests finished too quickly. "
-                    "It should have taken at least %d seconds. "
-                    "Something went wrong?" % min_runtime_in_seconds)
-      retcode = -1
-    return retcode
-
-  def Run(self, args, module, min_runtime_in_seconds=0):
-    MODULES_TO_SANITY_CHECK = ["base"]
-
-    check_sanity = module in MODULES_TO_SANITY_CHECK
-    return self.Main(args, check_sanity, min_runtime_in_seconds)
-
-
-class ValgrindTool(BaseTool):
-  """Abstract class for running Valgrind tools.
-
-  Always subclass this and implement ToolSpecificFlags() and
-  ExtendOptionParser() for tool-specific stuff.
-  """
-  def __init__(self):
-    super(ValgrindTool, self).__init__()
-    self.RegisterOptionParserHook(ValgrindTool.ExtendOptionParser)
-
-  def UseXML(self):
-    # Override if tool prefers nonxml output
-    return True
-
-  def ExtendOptionParser(self, parser):
-    parser.add_option("", "--suppressions", default=[],
-                            action="append",
-                            help="path to a valgrind suppression file")
-    parser.add_option("", "--indirect", action="store_true",
-                            default=False,
-                            help="set BROWSER_WRAPPER rather than "
-                                 "running valgrind directly")
-    parser.add_option("", "--indirect_webkit_layout", action="store_true",
-                            default=False,
-                            help="set --wrapper rather than running Dr. Memory "
-                                 "directly.")
-    parser.add_option("", "--trace_children", action="store_true",
-                            default=False,
-                            help="also trace child processes")
-    parser.add_option("", "--num-callers",
-                            dest="num_callers", default=30,
-                            help="number of callers to show in stack traces")
-    parser.add_option("", "--generate_dsym", action="store_true",
-                          default=False,
-                          help="Generate .dSYM file on Mac if needed. Slow!")
-
-  def Setup(self, args):
-    if not BaseTool.Setup(self, args):
-      return False
-    return True
-
-  def ToolCommand(self):
-    """Get the valgrind command to run."""
-    # Note that self._args begins with the exe to be run.
-    tool_name = self.ToolName()
-
-    # Construct the valgrind command.
-    if 'CHROME_VALGRIND' in os.environ:
-      path = os.path.join(os.environ['CHROME_VALGRIND'], "bin", "valgrind")
-    else:
-      path = "valgrind"
-    proc = [path, "--tool=%s" % tool_name]
-
-    proc += ["--num-callers=%i" % int(self._options.num_callers)]
-
-    if self._options.trace_children:
-      proc += ["--trace-children=yes"]
-      proc += ["--trace-children-skip='*dbus-daemon*'"]
-      proc += ["--trace-children-skip='*dbus-launch*'"]
-      proc += ["--trace-children-skip='*perl*'"]
-      proc += ["--trace-children-skip='*python*'"]
-      # This is really Python, but for some reason Valgrind follows it.
-      proc += ["--trace-children-skip='*lsb_release*'"]
-
-    proc += self.ToolSpecificFlags()
-    proc += self._tool_flags
-
-    suppression_count = 0
-    for suppression_file in self._options.suppressions:
-      if os.path.exists(suppression_file):
-        suppression_count += 1
-        proc += ["--suppressions=%s" % suppression_file]
-
-    if not suppression_count:
-      logging.warning("WARNING: NOT USING SUPPRESSIONS!")
-
-    logfilename = self.log_dir + ("/%s." % tool_name) + "%p"
-    if self.UseXML():
-      proc += ["--xml=yes", "--xml-file=" + logfilename]
-    else:
-      proc += ["--log-file=" + logfilename]
-
-    # The Valgrind command is constructed.
-
-    # Handle --indirect_webkit_layout separately.
-    if self._options.indirect_webkit_layout:
-      # Need to create the wrapper before modifying |proc|.
-      wrapper = self.CreateBrowserWrapper(proc, webkit=True)
-      proc = self._args
-      proc.append("--wrapper")
-      proc.append(wrapper)
-      return proc
-
-    if self._options.indirect:
-      wrapper = self.CreateBrowserWrapper(proc)
-      os.environ["BROWSER_WRAPPER"] = wrapper
-      logging.info('export BROWSER_WRAPPER=' + wrapper)
-      proc = []
-    proc += self._args
-    return proc
-
-  def ToolSpecificFlags(self):
-    raise NotImplementedError, "This method should be implemented " \
-                               "in the tool-specific subclass"
-
-  def CreateBrowserWrapper(self, proc, webkit=False):
-    """The program being run invokes Python or something else that can't stand
-    to be valgrinded, and also invokes the Chrome browser. In this case, use a
-    magic wrapper to only valgrind the Chrome browser. Build the wrapper here.
-    Returns the path to the wrapper. It's up to the caller to use the wrapper
-    appropriately.
-    """
-    command = " ".join(proc)
-    # Add the PID of the browser wrapper to the logfile names so we can
-    # separate log files for different UI tests at the analyze stage.
-    command = command.replace("%p", "$$.%p")
-
-    (fd, indirect_fname) = tempfile.mkstemp(dir=self.log_dir,
-                                            prefix="browser_wrapper.",
-                                            text=True)
-    f = os.fdopen(fd, "w")
-    f.write('#!/bin/bash\n'
-            'echo "Started Valgrind wrapper for this test, PID=$$" >&2\n')
-
-    f.write('DIR=`dirname $0`\n'
-            'TESTNAME_FILE=$DIR/testcase.$$.name\n\n')
-
-    if webkit:
-      # Webkit layout_tests pass the URL as the first line of stdin.
-      f.write('tee $TESTNAME_FILE | %s "$@"\n' % command)
-    else:
-      # Try to get the test case name by looking at the program arguments.
-      # i.e. Chromium ui_tests used --test-name arg.
-      # TODO(timurrrr): This doesn't handle "--test-name Test.Name"
-      # TODO(timurrrr): ui_tests are dead. Where do we use the non-webkit
-      # wrapper now? browser_tests? What do they do?
-      f.write('for arg in $@\ndo\n'
-              '  if [[ "$arg" =~ --test-name=(.*) ]]\n  then\n'
-              '    echo ${BASH_REMATCH[1]} >$TESTNAME_FILE\n'
-              '  fi\n'
-              'done\n\n'
-              '%s "$@"\n' % command)
-
-    f.close()
-    os.chmod(indirect_fname, stat.S_IRUSR|stat.S_IXUSR)
-    return indirect_fname
-
-  def CreateAnalyzer(self):
-    raise NotImplementedError, "This method should be implemented " \
-                               "in the tool-specific subclass"
-
-  def GetAnalyzeResults(self, check_sanity=False):
-    # Glob all the files in the log directory
-    filenames = glob.glob(self.log_dir + "/" + self.ToolName() + ".*")
-
-    # If we have browser wrapper, the logfiles are named as
-    # "toolname.wrapper_PID.valgrind_PID".
-    # Let's extract the list of wrapper_PIDs and name it ppids
-    ppids = set([int(f.split(".")[-2]) \
-                for f in filenames if re.search("\.[0-9]+\.[0-9]+$", f)])
-
-    analyzer = self.CreateAnalyzer()
-    if len(ppids) == 0:
-      # Fast path - no browser wrapper was set.
-      return analyzer.Report(filenames, None, check_sanity)
-
-    ret = 0
-    for ppid in ppids:
-      testcase_name = None
-      try:
-        f = open(self.log_dir + ("/testcase.%d.name" % ppid))
-        testcase_name = f.read().strip()
-        f.close()
-        wk_layout_prefix="third_party/WebKit/LayoutTests/"
-        wk_prefix_at = testcase_name.rfind(wk_layout_prefix)
-        if wk_prefix_at != -1:
-          testcase_name = testcase_name[wk_prefix_at + len(wk_layout_prefix):]
-      except IOError:
-        pass
-      print "====================================================="
-      print " Below is the report for valgrind wrapper PID=%d." % ppid
-      if testcase_name:
-        print " It was used while running the `%s` test." % testcase_name
-      else:
-        print " You can find the corresponding test"
-        print " by searching the above log for 'PID=%d'" % ppid
-      sys.stdout.flush()
-
-      ppid_filenames = [f for f in filenames \
-                        if re.search("\.%d\.[0-9]+$" % ppid, f)]
-      # check_sanity won't work with browser wrappers
-      assert check_sanity == False
-      ret |= analyzer.Report(ppid_filenames, testcase_name)
-      print "====================================================="
-      sys.stdout.flush()
-
-    if ret != 0:
-      print ""
-      print "The Valgrind reports are grouped by test names."
-      print "Each test has its PID printed in the log when the test was run"
-      print "and at the beginning of its Valgrind report."
-      print "Hint: you can search for the reports by Ctrl+F -> `=#`"
-      sys.stdout.flush()
-
-    return ret
-
-
-# TODO(timurrrr): Split into a separate file.
-class Memcheck(ValgrindTool):
-  """Memcheck
-  Dynamic memory error detector for Linux & Mac
-
-  http://valgrind.org/info/tools.html#memcheck
-  """
-
-  def __init__(self):
-    super(Memcheck, self).__init__()
-    self.RegisterOptionParserHook(Memcheck.ExtendOptionParser)
-
-  def ToolName(self):
-    return "memcheck"
-
-  def ExtendOptionParser(self, parser):
-    parser.add_option("--leak-check", "--leak_check", type="string",
-                      default="yes",  # --leak-check=yes is equivalent of =full
-                      help="perform leak checking at the end of the run")
-    parser.add_option("", "--show_all_leaks", action="store_true",
-                      default=False,
-                      help="also show less blatant leaks")
-    parser.add_option("", "--track_origins", action="store_true",
-                      default=False,
-                      help="Show whence uninitialized bytes came. 30% slower.")
-
-  def ToolSpecificFlags(self):
-    ret = ["--gen-suppressions=all", "--demangle=no"]
-    ret += ["--leak-check=%s" % self._options.leak_check]
-
-    if self._options.show_all_leaks:
-      ret += ["--show-reachable=yes"]
-    else:
-      ret += ["--show-possibly-lost=no"]
-
-    if self._options.track_origins:
-      ret += ["--track-origins=yes"]
-
-    # TODO(glider): this is a temporary workaround for http://crbug.com/51716
-    # Let's see whether it helps.
-    if common.IsMac():
-      ret += ["--smc-check=all"]
-
-    return ret
-
-  def CreateAnalyzer(self):
-    use_gdb = common.IsMac()
-    return memcheck_analyze.MemcheckAnalyzer(self._source_dir,
-                                            self._options.show_all_leaks,
-                                            use_gdb=use_gdb)
-
-  def Analyze(self, check_sanity=False):
-    ret = self.GetAnalyzeResults(check_sanity)
-
-    if ret != 0:
-      logging.info("Please see http://dev.chromium.org/developers/how-tos/"
-                   "using-valgrind for the info on Memcheck/Valgrind")
-    return ret
-
-
-class ToolFactory:
-  def Create(self, tool_name):
-    if tool_name == "memcheck":
-      return Memcheck()
-    try:
-      platform_name = common.PlatformNames()[0]
-    except common.NotImplementedError:
-      platform_name = sys.platform + "(Unknown)"
-    raise RuntimeError, "Unknown tool (tool=%s, platform=%s)" % (tool_name,
-                                                                 platform_name)
-
-def CreateTool(tool):
-  return ToolFactory().Create(tool)
diff --git a/unit_test/color_test.cc b/unit_test/color_test.cc
index a81ab19a..f6b1f6f5 100644
--- a/unit_test/color_test.cc
+++ b/unit_test/color_test.cc
@@ -23,7 +23,7 @@ namespace libyuv {
 // TODO(fbarchard): clang x86 has a higher accuracy YUV to RGB.
 // Port to Visual C and other CPUs
 #if !defined(LIBYUV_DISABLE_X86) && \
-    (defined(__x86_64__) || (defined(__i386__) && !defined(_MSC_VER)))
+    (defined(__x86_64__) || defined(__i386__))
 #define ERROR_FULL 5
 #define ERROR_J420 4
 #else
@@ -32,7 +32,11 @@ namespace libyuv {
 #endif
 #define ERROR_R 1
 #define ERROR_G 1
-#define ERROR_B 3
+#ifdef LIBYUV_UNLIMITED_DATA
+#define ERROR_B 1
+#else
+#define ERROR_B 18
+#endif
 
 #define TESTCS(TESTNAME, YUVTOARGB, ARGBTOYUV, HS1, HS, HN, DIFF)              \
   TEST_F(LibYUVColorTest, TESTNAME) {                                          \
@@ -498,7 +502,11 @@ TEST_F(LibYUVColorTest, TestYUV) {
   YUVToRGB(240, 0, 0, &r1, &g1, &b1);
   EXPECT_EQ(57, r1);
   EXPECT_EQ(255, g1);
+#ifdef LIBYUV_UNLIMITED_DATA
+  EXPECT_EQ(3, b1);
+#else
   EXPECT_EQ(5, b1);
+#endif
 
   for (int i = 0; i < 256; ++i) {
     YUVToRGBReference(i, 128, 128, &r0, &g0, &b0);
@@ -655,9 +663,9 @@ TEST_F(LibYUVColorTest, TestFullYUVJ) {
         int y = RANDOM256(y2);
         YUVJToRGBReference(y, u, v, &r0, &g0, &b0);
         YUVJToRGB(y, u, v, &r1, &g1, &b1);
-        EXPECT_NEAR(r0, r1, 1);
-        EXPECT_NEAR(g0, g1, 1);
-        EXPECT_NEAR(b0, b1, 1);
+        EXPECT_NEAR(r0, r1, ERROR_R);
+        EXPECT_NEAR(g0, g1, ERROR_G);
+        EXPECT_NEAR(b0, b1, ERROR_B);
         ++rh[r1 - r0 + 128];
         ++gh[g1 - g0 + 128];
         ++bh[b1 - b0 + 128];
@@ -687,8 +695,7 @@ TEST_F(LibYUVColorTest, TestFullYUVH) {
         YUVHToRGB(y, u, v, &r1, &g1, &b1);
         EXPECT_NEAR(r0, r1, ERROR_R);
         EXPECT_NEAR(g0, g1, ERROR_G);
-        // TODO(crbug.com/libyuv/862): Reduce the errors in the B channel.
-        EXPECT_NEAR(b0, b1, 15);
+        EXPECT_NEAR(b0, b1, ERROR_B);
         ++rh[r1 - r0 + 128];
         ++gh[g1 - g0 + 128];
         ++bh[b1 - b0 + 128];
@@ -716,9 +723,9 @@ TEST_F(LibYUVColorTest, TestFullYUVF) {
         int y = RANDOM256(y2);
         YUVFToRGBReference(y, u, v, &r0, &g0, &b0);
         YUVFToRGB(y, u, v, &r1, &g1, &b1);
-        EXPECT_NEAR(r0, r1, 5);
-        EXPECT_NEAR(g0, g1, 5);
-        EXPECT_NEAR(b0, b1, 5);
+        EXPECT_NEAR(r0, r1, ERROR_R);
+        EXPECT_NEAR(g0, g1, ERROR_G);
+        EXPECT_NEAR(b0, b1, ERROR_B);
         ++rh[r1 - r0 + 128];
         ++gh[g1 - g0 + 128];
         ++bh[b1 - b0 + 128];
@@ -748,8 +755,7 @@ TEST_F(LibYUVColorTest, TestFullYUVU) {
         YUVUToRGB(y, u, v, &r1, &g1, &b1);
         EXPECT_NEAR(r0, r1, ERROR_R);
         EXPECT_NEAR(g0, g1, ERROR_G);
-        // TODO(crbug.com/libyuv/863): Reduce the errors in the B channel.
-        EXPECT_NEAR(b0, b1, 18);
+        EXPECT_NEAR(b0, b1, ERROR_B);
         ++rh[r1 - r0 + 128];
         ++gh[g1 - g0 + 128];
         ++bh[b1 - b0 + 128];
diff --git a/unit_test/compare_test.cc b/unit_test/compare_test.cc
index bd99cdd3..c29562cb 100644
--- a/unit_test/compare_test.cc
+++ b/unit_test/compare_test.cc
@@ -344,7 +344,7 @@ static const int kMaxOptCount = (1 << (32 - 3)) - 64;  // 536870848
 
 TEST_F(LibYUVCompareTest, TestHammingDistance_Opt) {
   uint32_t h1 = 0;
-  const int kMaxWidth = (benchmark_width_ * benchmark_height_ + 31) & ~31;
+  const int kMaxWidth = (benchmark_width_ * benchmark_height_ + 63) & ~63;
   align_buffer_page_end(src_a, kMaxWidth);
   align_buffer_page_end(src_b, kMaxWidth);
   memset(src_a, 255u, kMaxWidth);
diff --git a/unit_test/convert_test.cc b/unit_test/convert_test.cc
index 8b4f4231..8d142529 100644
--- a/unit_test/convert_test.cc
+++ b/unit_test/convert_test.cc
@@ -62,7 +62,7 @@ namespace libyuv {
                   "DST_SUBSAMP_X unsupported");                               \
     static_assert(DST_SUBSAMP_Y == 1 || DST_SUBSAMP_Y == 2,                   \
                   "DST_SUBSAMP_Y unsupported");                               \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = benchmark_height_;                                    \
     const int kSrcHalfWidth = SUBSAMPLE(kWidth, SRC_SUBSAMP_X);               \
     const int kSrcHalfHeight = SUBSAMPLE(kHeight, SRC_SUBSAMP_Y);             \
@@ -137,7 +137,7 @@ namespace libyuv {
                       DST_SUBSAMP_X, DST_SUBSAMP_Y, SRC_DEPTH)                 \
   TESTPLANARTOPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X, SRC_SUBSAMP_Y, \
                  FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X, DST_SUBSAMP_Y,     \
-                 benchmark_width_ - 4, _Any, +, 0, SRC_DEPTH)                  \
+                 benchmark_width_ + 1, _Any, +, 0, SRC_DEPTH)                  \
   TESTPLANARTOPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X, SRC_SUBSAMP_Y, \
                  FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X, DST_SUBSAMP_Y,     \
                  benchmark_width_, _Unaligned, +, 1, SRC_DEPTH)                \
@@ -184,7 +184,7 @@ TESTPLANARTOP(I412, uint16_t, 2, 1, 1, I444, uint8_t, 1, 1, 1, 12)
                         SRC_SUBSAMP_Y, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y,      \
                         W1280, N, NEG, OFF, PN, OFF_U, OFF_V)                 \
   TEST_F(LibYUVConvertTest, SRC_FMT_PLANAR##To##FMT_PLANAR##_##PN##N) {       \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = benchmark_height_;                                    \
     const int kSizeUV =                                                       \
         SUBSAMPLE(kWidth, SRC_SUBSAMP_X) * SUBSAMPLE(kHeight, SRC_SUBSAMP_Y); \
@@ -270,7 +270,7 @@ TESTPLANARTOP(I412, uint16_t, 2, 1, 1, I444, uint8_t, 1, 1, 1, 12)
                        SRC_SUBSAMP_X, SRC_SUBSAMP_Y, FMT_PLANAR, SUBSAMP_X,    \
                        SUBSAMP_Y)                                              \
   TESTAPLANARTOPI(SRC_FMT_PLANAR, PIXEL_STRIDE, SRC_SUBSAMP_X, SRC_SUBSAMP_Y,  \
-                  FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, benchmark_width_ - 4,      \
+                  FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, benchmark_width_ + 1,      \
                   _Any, +, 0, PN, OFF_U, OFF_V)                                \
   TESTAPLANARTOPI(SRC_FMT_PLANAR, PIXEL_STRIDE, SRC_SUBSAMP_X, SRC_SUBSAMP_Y,  \
                   FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, benchmark_width_,          \
@@ -318,7 +318,7 @@ int I400ToNV21(const uint8_t* src_y,
                   "DST_SUBSAMP_X unsupported");                               \
     static_assert(DST_SUBSAMP_Y == 1 || DST_SUBSAMP_Y == 2,                   \
                   "DST_SUBSAMP_Y unsupported");                               \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = benchmark_height_;                                    \
     const int kSrcHalfWidth = SUBSAMPLE(kWidth, SRC_SUBSAMP_X);               \
     const int kSrcHalfHeight = SUBSAMPLE(kHeight, SRC_SUBSAMP_Y);             \
@@ -386,7 +386,7 @@ int I400ToNV21(const uint8_t* src_y,
                        DST_SUBSAMP_X, DST_SUBSAMP_Y, SRC_DEPTH)               \
   TESTPLANARTOBPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X,              \
                   SRC_SUBSAMP_Y, FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X,   \
-                  DST_SUBSAMP_Y, benchmark_width_ - 4, _Any, +, 0, SRC_DEPTH) \
+                  DST_SUBSAMP_Y, benchmark_width_ + 1, _Any, +, 0, SRC_DEPTH) \
   TESTPLANARTOBPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X,              \
                   SRC_SUBSAMP_Y, FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X,   \
                   DST_SUBSAMP_Y, benchmark_width_, _Unaligned, +, 1,          \
@@ -424,7 +424,7 @@ TESTPLANARTOBP(I212, uint16_t, 2, 2, 1, P212, uint16_t, 2, 2, 1, 12)
                   "DST_SUBSAMP_X unsupported");                                \
     static_assert(DST_SUBSAMP_Y == 1 || DST_SUBSAMP_Y == 2,                    \
                   "DST_SUBSAMP_Y unsupported");                                \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = benchmark_height_;                                     \
     const int kSrcHalfWidth = SUBSAMPLE(kWidth, SRC_SUBSAMP_X);                \
     const int kSrcHalfHeight = SUBSAMPLE(kHeight, SRC_SUBSAMP_Y);              \
@@ -493,7 +493,7 @@ TESTPLANARTOBP(I212, uint16_t, 2, 2, 1, P212, uint16_t, 2, 2, 1, 12)
                          DST_SUBSAMP_X, DST_SUBSAMP_Y, SRC_DEPTH)              \
   TESTBIPLANARTOBPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X,             \
                     SRC_SUBSAMP_Y, FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X,  \
-                    DST_SUBSAMP_Y, benchmark_width_ - 4, _Any, +, 0, 1,        \
+                    DST_SUBSAMP_Y, benchmark_width_ + 1, _Any, +, 0, 1,        \
                     SRC_DEPTH)                                                 \
   TESTBIPLANARTOBPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X,             \
                     SRC_SUBSAMP_Y, FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X,  \
@@ -537,7 +537,7 @@ TESTBIPLANARTOBP(P216, uint16_t, 2, 2, 1, P416, uint16_t, 2, 1, 1, 12)
                   "DST_SUBSAMP_X unsupported");                               \
     static_assert(DST_SUBSAMP_Y == 1 || DST_SUBSAMP_Y == 2,                   \
                   "DST_SUBSAMP_Y unsupported");                               \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = benchmark_height_;                                    \
     const int kSrcHalfWidth = SUBSAMPLE(kWidth, SRC_SUBSAMP_X);               \
     const int kSrcHalfHeight = SUBSAMPLE(kHeight, SRC_SUBSAMP_Y);             \
@@ -606,7 +606,7 @@ TESTBIPLANARTOBP(P216, uint16_t, 2, 2, 1, P416, uint16_t, 2, 1, 1, 12)
                         DST_SUBSAMP_X, DST_SUBSAMP_Y, SRC_DEPTH)               \
   TESTBIPLANARTOPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X,              \
                    SRC_SUBSAMP_Y, FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X,   \
-                   DST_SUBSAMP_Y, benchmark_width_ - 4, _Any, +, 0, SRC_DEPTH) \
+                   DST_SUBSAMP_Y, benchmark_width_ + 1, _Any, +, 0, SRC_DEPTH) \
   TESTBIPLANARTOPI(SRC_FMT_PLANAR, SRC_T, SRC_BPC, SRC_SUBSAMP_X,              \
                    SRC_SUBSAMP_Y, FMT_PLANAR, DST_T, DST_BPC, DST_SUBSAMP_X,   \
                    DST_SUBSAMP_Y, benchmark_width_, _Unaligned, +, 1,          \
@@ -654,7 +654,7 @@ TESTBIPLANARTOP(NV21, uint8_t, 1, 2, 2, I420, uint8_t, 1, 2, 2, 8)
 #define TESTPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN, \
                        YALIGN, W1280, N, NEG, OFF)                            \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##N) {                       \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = ALIGNINT(benchmark_height_, YALIGN);                  \
     const int kStrideB = ALIGNINT(kWidth * BPP_B, ALIGN);                     \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                       \
@@ -702,7 +702,7 @@ TESTBIPLANARTOP(NV21, uint8_t, 1, 2, 2, I420, uint8_t, 1, 2, 2, 8)
 #define TESTPLANARTOB(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN, \
                       YALIGN)                                                \
   TESTPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
-                 YALIGN, benchmark_width_ - 4, _Any, +, 0)                   \
+                 YALIGN, benchmark_width_ + 1, _Any, +, 0)                   \
   TESTPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
                  YALIGN, benchmark_width_, _Unaligned, +, 1)                 \
   TESTPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
@@ -769,12 +769,14 @@ TESTPLANARTOB(J420, 2, 2, J400, 1, 1, 1)
 #ifdef LITTLE_ENDIAN_ONLY_TEST
 TESTPLANARTOB(I420, 2, 2, AR30, 4, 4, 1)
 TESTPLANARTOB(H420, 2, 2, AR30, 4, 4, 1)
+TESTPLANARTOB(I420, 2, 2, AB30, 4, 4, 1)
+TESTPLANARTOB(H420, 2, 2, AB30, 4, 4, 1)
 #endif
 
 #define TESTQPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN, \
                         YALIGN, W1280, N, NEG, OFF, ATTEN)                     \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##N) {                        \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = ALIGNINT(benchmark_height_, YALIGN);                   \
     const int kStrideB = ALIGNINT(kWidth * BPP_B, ALIGN);                      \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                        \
@@ -821,7 +823,7 @@ TESTPLANARTOB(H420, 2, 2, AR30, 4, 4, 1)
 #define TESTQPLANARTOB(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN, \
                        YALIGN)                                                \
   TESTQPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
-                  YALIGN, benchmark_width_ - 4, _Any, +, 0, 0)                \
+                  YALIGN, benchmark_width_ + 1, _Any, +, 0, 0)                \
   TESTQPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
                   YALIGN, benchmark_width_, _Unaligned, +, 1, 0)              \
   TESTQPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
@@ -962,7 +964,7 @@ TESTQPLANARTOB(V444Alpha, 1, 1, ABGR, 4, 4, 1)
 #define TESTBIPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, FMT_C,       \
                          BPP_B, W1280, N, NEG, OFF)                            \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##N) {                        \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = benchmark_height_;                                     \
     const int kStrideB = kWidth * BPP_B;                                       \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                        \
@@ -1015,7 +1017,7 @@ TESTQPLANARTOB(V444Alpha, 1, 1, ABGR, 4, 4, 1)
 
 #define TESTBIPLANARTOB(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, FMT_C, BPP_B) \
   TESTBIPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, FMT_C, BPP_B,      \
-                   benchmark_width_ - 4, _Any, +, 0)                           \
+                   benchmark_width_ + 1, _Any, +, 0)                           \
   TESTBIPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, FMT_C, BPP_B,      \
                    benchmark_width_, _Unaligned, +, 1)                         \
   TESTBIPLANARTOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, FMT_C, BPP_B,      \
@@ -1070,7 +1072,7 @@ TESTBIPLANARTOB(NV12, 2, 2, RGB565, RGB565, 2)
 #define TESTATOPLANARI(FMT_A, BPP_A, YALIGN, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, \
                        W1280, N, NEG, OFF)                                     \
   TEST_F(LibYUVConvertTest, FMT_A##To##FMT_PLANAR##N) {                        \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = ALIGNINT(benchmark_height_, YALIGN);                   \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                        \
     const int kStride = (kStrideUV * SUBSAMP_X * 8 * BPP_A + 7) / 8;           \
@@ -1117,7 +1119,7 @@ TESTBIPLANARTOB(NV12, 2, 2, RGB565, RGB565, 2)
 
 #define TESTATOPLANAR(FMT_A, BPP_A, YALIGN, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y) \
   TESTATOPLANARI(FMT_A, BPP_A, YALIGN, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y,      \
-                 benchmark_width_ - 4, _Any, +, 0)                            \
+                 benchmark_width_ + 1, _Any, +, 0)                            \
   TESTATOPLANARI(FMT_A, BPP_A, YALIGN, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y,      \
                  benchmark_width_, _Unaligned, +, 1)                          \
   TESTATOPLANARI(FMT_A, BPP_A, YALIGN, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y,      \
@@ -1140,6 +1142,7 @@ TESTATOPLANAR(BGRA, 4, 1, I420, 2, 2)
 TESTATOPLANAR(I400, 1, 1, I420, 2, 2)
 TESTATOPLANAR(J400, 1, 1, J420, 2, 2)
 TESTATOPLANAR(RAW, 3, 1, I420, 2, 2)
+TESTATOPLANAR(RAW, 3, 1, J420, 2, 2)
 TESTATOPLANAR(RGB24, 3, 1, I420, 2, 2)
 TESTATOPLANAR(RGB24, 3, 1, J420, 2, 2)
 TESTATOPLANAR(RGBA, 4, 1, I420, 2, 2)
@@ -1151,7 +1154,7 @@ TESTATOPLANAR(YUY2, 2, 1, I422, 2, 1)
 #define TESTATOBIPLANARI(FMT_A, SUB_A, BPP_A, FMT_PLANAR, SUBSAMP_X,          \
                          SUBSAMP_Y, W1280, N, NEG, OFF)                       \
   TEST_F(LibYUVConvertTest, FMT_A##To##FMT_PLANAR##N) {                       \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = benchmark_height_;                                    \
     const int kStride = SUBSAMPLE(kWidth, SUB_A) * BPP_A;                     \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                       \
@@ -1197,7 +1200,7 @@ TESTATOPLANAR(YUY2, 2, 1, I422, 2, 1)
 
 #define TESTATOBIPLANAR(FMT_A, SUB_A, BPP_A, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y) \
   TESTATOBIPLANARI(FMT_A, SUB_A, BPP_A, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y,      \
-                   benchmark_width_ - 4, _Any, +, 0)                           \
+                   benchmark_width_ + 1, _Any, +, 0)                           \
   TESTATOBIPLANARI(FMT_A, SUB_A, BPP_A, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y,      \
                    benchmark_width_, _Unaligned, +, 1)                         \
   TESTATOBIPLANARI(FMT_A, SUB_A, BPP_A, FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y,      \
@@ -1214,152 +1217,166 @@ TESTATOBIPLANAR(UYVY, 2, 4, NV12, 2, 2)
 TESTATOBIPLANAR(AYUV, 1, 4, NV12, 2, 2)
 TESTATOBIPLANAR(AYUV, 1, 4, NV21, 2, 2)
 
-#define TESTATOBI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,  \
-                  HEIGHT_B, W1280, N, NEG, OFF)                              \
-  TEST_F(LibYUVConvertTest, FMT_A##To##FMT_B##N) {                           \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                          \
-    const int kHeight = benchmark_height_;                                   \
-    const int kHeightA = (kHeight + HEIGHT_A - 1) / HEIGHT_A * HEIGHT_A;     \
-    const int kHeightB = (kHeight + HEIGHT_B - 1) / HEIGHT_B * HEIGHT_B;     \
-    const int kStrideA =                                                     \
-        (kWidth * BPP_A + STRIDE_A - 1) / STRIDE_A * STRIDE_A;               \
-    const int kStrideB =                                                     \
-        (kWidth * BPP_B + STRIDE_B - 1) / STRIDE_B * STRIDE_B;               \
-    align_buffer_page_end(src_argb, kStrideA* kHeightA + OFF);               \
-    align_buffer_page_end(dst_argb_c, kStrideB* kHeightB);                   \
-    align_buffer_page_end(dst_argb_opt, kStrideB* kHeightB);                 \
-    for (int i = 0; i < kStrideA * kHeightA; ++i) {                          \
-      src_argb[i + OFF] = (fastrand() & 0xff);                               \
-    }                                                                        \
-    memset(dst_argb_c, 1, kStrideB* kHeightB);                               \
-    memset(dst_argb_opt, 101, kStrideB* kHeightB);                           \
-    MaskCpuFlags(disable_cpu_flags_);                                        \
-    FMT_A##To##FMT_B(src_argb + OFF, kStrideA, dst_argb_c, kStrideB, kWidth, \
-                     NEG kHeight);                                           \
-    MaskCpuFlags(benchmark_cpu_info_);                                       \
-    for (int i = 0; i < benchmark_iterations_; ++i) {                        \
-      FMT_A##To##FMT_B(src_argb + OFF, kStrideA, dst_argb_opt, kStrideB,     \
-                       kWidth, NEG kHeight);                                 \
-    }                                                                        \
-    for (int i = 0; i < kStrideB * kHeightB; ++i) {                          \
-      EXPECT_EQ(dst_argb_c[i], dst_argb_opt[i]);                             \
-    }                                                                        \
-    free_aligned_buffer_page_end(src_argb);                                  \
-    free_aligned_buffer_page_end(dst_argb_c);                                \
-    free_aligned_buffer_page_end(dst_argb_opt);                              \
+#define TESTATOBI(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B, TYPE_B,     \
+                  EPP_B, STRIDE_B, HEIGHT_B, W1280, N, NEG, OFF)               \
+  TEST_F(LibYUVConvertTest, FMT_A##To##FMT_B##N) {                             \
+    const int kWidth = W1280;                                                  \
+    const int kHeight = benchmark_height_;                                     \
+    const int kHeightA = (kHeight + HEIGHT_A - 1) / HEIGHT_A * HEIGHT_A;       \
+    const int kHeightB = (kHeight + HEIGHT_B - 1) / HEIGHT_B * HEIGHT_B;       \
+    const int kStrideA =                                                       \
+        (kWidth * EPP_A + STRIDE_A - 1) / STRIDE_A * STRIDE_A;                 \
+    const int kStrideB =                                                       \
+        (kWidth * EPP_B + STRIDE_B - 1) / STRIDE_B * STRIDE_B;                 \
+    align_buffer_page_end(src_argb,                                            \
+                          kStrideA* kHeightA*(int)sizeof(TYPE_A) + OFF);       \
+    align_buffer_page_end(dst_argb_c, kStrideB* kHeightB*(int)sizeof(TYPE_B)); \
+    align_buffer_page_end(dst_argb_opt,                                        \
+                          kStrideB* kHeightB*(int)sizeof(TYPE_B));             \
+    for (int i = 0; i < kStrideA * kHeightA * (int)sizeof(TYPE_A); ++i) {      \
+      src_argb[i + OFF] = (fastrand() & 0xff);                                 \
+    }                                                                          \
+    memset(dst_argb_c, 1, kStrideB* kHeightB);                                 \
+    memset(dst_argb_opt, 101, kStrideB* kHeightB);                             \
+    MaskCpuFlags(disable_cpu_flags_);                                          \
+    FMT_A##To##FMT_B((TYPE_A*)(src_argb + OFF), kStrideA, (TYPE_B*)dst_argb_c, \
+                     kStrideB, kWidth, NEG kHeight);                           \
+    MaskCpuFlags(benchmark_cpu_info_);                                         \
+    for (int i = 0; i < benchmark_iterations_; ++i) {                          \
+      FMT_A##To##FMT_B((TYPE_A*)(src_argb + OFF), kStrideA,                    \
+                       (TYPE_B*)dst_argb_opt, kStrideB, kWidth, NEG kHeight);  \
+    }                                                                          \
+    for (int i = 0; i < kStrideB * kHeightB * (int)sizeof(TYPE_B); ++i) {      \
+      EXPECT_EQ(dst_argb_c[i], dst_argb_opt[i]);                               \
+    }                                                                          \
+    free_aligned_buffer_page_end(src_argb);                                    \
+    free_aligned_buffer_page_end(dst_argb_c);                                  \
+    free_aligned_buffer_page_end(dst_argb_opt);                                \
+  }
+
+#define TESTATOBRANDOM(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B,        \
+                       TYPE_B, EPP_B, STRIDE_B, HEIGHT_B)                      \
+  TEST_F(LibYUVConvertTest, FMT_A##To##FMT_B##_Random) {                       \
+    for (int times = 0; times < benchmark_iterations_; ++times) {              \
+      const int kWidth = (fastrand() & 63) + 1;                                \
+      const int kHeight = (fastrand() & 31) + 1;                               \
+      const int kHeightA = (kHeight + HEIGHT_A - 1) / HEIGHT_A * HEIGHT_A;     \
+      const int kHeightB = (kHeight + HEIGHT_B - 1) / HEIGHT_B * HEIGHT_B;     \
+      const int kStrideA =                                                     \
+          (kWidth * EPP_A + STRIDE_A - 1) / STRIDE_A * STRIDE_A;               \
+      const int kStrideB =                                                     \
+          (kWidth * EPP_B + STRIDE_B - 1) / STRIDE_B * STRIDE_B;               \
+      align_buffer_page_end(src_argb, kStrideA* kHeightA*(int)sizeof(TYPE_A)); \
+      align_buffer_page_end(dst_argb_c,                                        \
+                            kStrideB* kHeightB*(int)sizeof(TYPE_B));           \
+      align_buffer_page_end(dst_argb_opt,                                      \
+                            kStrideB* kHeightB*(int)sizeof(TYPE_B));           \
+      for (int i = 0; i < kStrideA * kHeightA * (int)sizeof(TYPE_A); ++i) {    \
+        src_argb[i] = 0xfe;                                                    \
+      }                                                                        \
+      memset(dst_argb_c, 123, kStrideB* kHeightB);                             \
+      memset(dst_argb_opt, 123, kStrideB* kHeightB);                           \
+      MaskCpuFlags(disable_cpu_flags_);                                        \
+      FMT_A##To##FMT_B((TYPE_A*)src_argb, kStrideA, (TYPE_B*)dst_argb_c,       \
+                       kStrideB, kWidth, kHeight);                             \
+      MaskCpuFlags(benchmark_cpu_info_);                                       \
+      FMT_A##To##FMT_B((TYPE_A*)src_argb, kStrideA, (TYPE_B*)dst_argb_opt,     \
+                       kStrideB, kWidth, kHeight);                             \
+      for (int i = 0; i < kStrideB * kHeightB * (int)sizeof(TYPE_B); ++i) {    \
+        EXPECT_EQ(dst_argb_c[i], dst_argb_opt[i]);                             \
+      }                                                                        \
+      free_aligned_buffer_page_end(src_argb);                                  \
+      free_aligned_buffer_page_end(dst_argb_c);                                \
+      free_aligned_buffer_page_end(dst_argb_opt);                              \
+    }                                                                          \
   }
 
-#define TESTATOBRANDOM(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B,     \
-                       STRIDE_B, HEIGHT_B)                                 \
-  TEST_F(LibYUVConvertTest, FMT_A##To##FMT_B##_Random) {                   \
-    for (int times = 0; times < benchmark_iterations_; ++times) {          \
-      const int kWidth = (fastrand() & 63) + 1;                            \
-      const int kHeight = (fastrand() & 31) + 1;                           \
-      const int kHeightA = (kHeight + HEIGHT_A - 1) / HEIGHT_A * HEIGHT_A; \
-      const int kHeightB = (kHeight + HEIGHT_B - 1) / HEIGHT_B * HEIGHT_B; \
-      const int kStrideA =                                                 \
-          (kWidth * BPP_A + STRIDE_A - 1) / STRIDE_A * STRIDE_A;           \
-      const int kStrideB =                                                 \
-          (kWidth * BPP_B + STRIDE_B - 1) / STRIDE_B * STRIDE_B;           \
-      align_buffer_page_end(src_argb, kStrideA* kHeightA);                 \
-      align_buffer_page_end(dst_argb_c, kStrideB* kHeightB);               \
-      align_buffer_page_end(dst_argb_opt, kStrideB* kHeightB);             \
-      for (int i = 0; i < kStrideA * kHeightA; ++i) {                      \
-        src_argb[i] = (fastrand() & 0xff);                                 \
-      }                                                                    \
-      memset(dst_argb_c, 123, kStrideB* kHeightB);                         \
-      memset(dst_argb_opt, 123, kStrideB* kHeightB);                       \
-      MaskCpuFlags(disable_cpu_flags_);                                    \
-      FMT_A##To##FMT_B(src_argb, kStrideA, dst_argb_c, kStrideB, kWidth,   \
-                       kHeight);                                           \
-      MaskCpuFlags(benchmark_cpu_info_);                                   \
-      FMT_A##To##FMT_B(src_argb, kStrideA, dst_argb_opt, kStrideB, kWidth, \
-                       kHeight);                                           \
-      for (int i = 0; i < kStrideB * kHeightB; ++i) {                      \
-        EXPECT_EQ(dst_argb_c[i], dst_argb_opt[i]);                         \
-      }                                                                    \
-      free_aligned_buffer_page_end(src_argb);                              \
-      free_aligned_buffer_page_end(dst_argb_c);                            \
-      free_aligned_buffer_page_end(dst_argb_opt);                          \
-    }                                                                      \
-  }
-
-#define TESTATOB(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B, \
-                 HEIGHT_B)                                                 \
-  TESTATOBI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,      \
-            HEIGHT_B, benchmark_width_ - 4, _Any, +, 0)                    \
-  TESTATOBI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,      \
-            HEIGHT_B, benchmark_width_, _Unaligned, +, 1)                  \
-  TESTATOBI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,      \
-            HEIGHT_B, benchmark_width_, _Invert, -, 0)                     \
-  TESTATOBI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,      \
-            HEIGHT_B, benchmark_width_, _Opt, +, 0)                        \
-  TESTATOBRANDOM(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B, \
-                 HEIGHT_B)
-
-TESTATOB(AB30, 4, 4, 1, ABGR, 4, 4, 1)
-TESTATOB(AB30, 4, 4, 1, ARGB, 4, 4, 1)
+#define TESTATOB(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B, TYPE_B,   \
+                 EPP_B, STRIDE_B, HEIGHT_B)                                 \
+  TESTATOBI(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B, TYPE_B, EPP_B, \
+            STRIDE_B, HEIGHT_B, benchmark_width_ + 1, _Any, +, 0)           \
+  TESTATOBI(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B, TYPE_B, EPP_B, \
+            STRIDE_B, HEIGHT_B, benchmark_width_, _Unaligned, +, 1)         \
+  TESTATOBI(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B, TYPE_B, EPP_B, \
+            STRIDE_B, HEIGHT_B, benchmark_width_, _Invert, -, 0)            \
+  TESTATOBI(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B, TYPE_B, EPP_B, \
+            STRIDE_B, HEIGHT_B, benchmark_width_, _Opt, +, 0)               \
+  TESTATOBRANDOM(FMT_A, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, FMT_B, TYPE_B,   \
+                 EPP_B, STRIDE_B, HEIGHT_B)
+
+TESTATOB(AB30, uint8_t, 4, 4, 1, ABGR, uint8_t, 4, 4, 1)
+TESTATOB(AB30, uint8_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
 #ifdef LITTLE_ENDIAN_ONLY_TEST
-TESTATOB(ABGR, 4, 4, 1, AR30, 4, 4, 1)
+TESTATOB(ABGR, uint8_t, 4, 4, 1, AR30, uint8_t, 4, 4, 1)
 #endif
-TESTATOB(ABGR, 4, 4, 1, ARGB, 4, 4, 1)
+TESTATOB(ABGR, uint8_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
 #ifdef LITTLE_ENDIAN_ONLY_TEST
-TESTATOB(AR30, 4, 4, 1, AB30, 4, 4, 1)
+TESTATOB(AR30, uint8_t, 4, 4, 1, AB30, uint8_t, 4, 4, 1)
 #endif
-TESTATOB(AR30, 4, 4, 1, ABGR, 4, 4, 1)
+TESTATOB(AR30, uint8_t, 4, 4, 1, ABGR, uint8_t, 4, 4, 1)
 #ifdef LITTLE_ENDIAN_ONLY_TEST
-TESTATOB(AR30, 4, 4, 1, AR30, 4, 4, 1)
-TESTATOB(AR30, 4, 4, 1, ARGB, 4, 4, 1)
+TESTATOB(AR30, uint8_t, 4, 4, 1, AR30, uint8_t, 4, 4, 1)
+TESTATOB(AR30, uint8_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
 #endif
-TESTATOB(ARGB, 4, 4, 1, ABGR, 4, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, ABGR, uint8_t, 4, 4, 1)
 #ifdef LITTLE_ENDIAN_ONLY_TEST
-TESTATOB(ARGB, 4, 4, 1, AR30, 4, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, AR30, uint8_t, 4, 4, 1)
 #endif
-TESTATOB(ARGB, 4, 4, 1, ARGB, 4, 4, 1)
-TESTATOB(ARGB, 4, 4, 1, ARGB1555, 2, 2, 1)
-TESTATOB(ARGB, 4, 4, 1, ARGB4444, 2, 2, 1)
-TESTATOB(ARGB, 4, 4, 1, ARGBMirror, 4, 4, 1)
-TESTATOB(ARGB, 4, 4, 1, BGRA, 4, 4, 1)
-TESTATOB(ARGB, 4, 4, 1, I400, 1, 1, 1)
-TESTATOB(ARGB, 4, 4, 1, J400, 1, 1, 1)
-TESTATOB(RGBA, 4, 4, 1, J400, 1, 1, 1)
-TESTATOB(ARGB, 4, 4, 1, RAW, 3, 3, 1)
-TESTATOB(ARGB, 4, 4, 1, RGB24, 3, 3, 1)
-TESTATOB(ABGR, 4, 4, 1, RAW, 3, 3, 1)
-TESTATOB(ABGR, 4, 4, 1, RGB24, 3, 3, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, ARGB1555, uint8_t, 2, 2, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, ARGB4444, uint8_t, 2, 2, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, ARGBMirror, uint8_t, 4, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, BGRA, uint8_t, 4, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, I400, uint8_t, 1, 1, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, J400, uint8_t, 1, 1, 1)
+TESTATOB(RGBA, uint8_t, 4, 4, 1, J400, uint8_t, 1, 1, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, RAW, uint8_t, 3, 3, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, RGB24, uint8_t, 3, 3, 1)
+TESTATOB(ABGR, uint8_t, 4, 4, 1, RAW, uint8_t, 3, 3, 1)
+TESTATOB(ABGR, uint8_t, 4, 4, 1, RGB24, uint8_t, 3, 3, 1)
 #ifdef LITTLE_ENDIAN_ONLY_TEST
-TESTATOB(ARGB, 4, 4, 1, RGB565, 2, 2, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, RGB565, uint8_t, 2, 2, 1)
 #endif
-TESTATOB(ARGB, 4, 4, 1, RGBA, 4, 4, 1)
-TESTATOB(ARGB, 4, 4, 1, UYVY, 2, 4, 1)
-TESTATOB(ARGB, 4, 4, 1, YUY2, 2, 4, 1)  // 4
-TESTATOB(ARGB1555, 2, 2, 1, ARGB, 4, 4, 1)
-TESTATOB(ARGB4444, 2, 2, 1, ARGB, 4, 4, 1)
-TESTATOB(BGRA, 4, 4, 1, ARGB, 4, 4, 1)
-TESTATOB(I400, 1, 1, 1, ARGB, 4, 4, 1)
-TESTATOB(I400, 1, 1, 1, I400, 1, 1, 1)
-TESTATOB(I400, 1, 1, 1, I400Mirror, 1, 1, 1)
-TESTATOB(J400, 1, 1, 1, ARGB, 4, 4, 1)
-TESTATOB(J400, 1, 1, 1, J400, 1, 1, 1)
-TESTATOB(RAW, 3, 3, 1, ARGB, 4, 4, 1)
-TESTATOB(RAW, 3, 3, 1, RGBA, 4, 4, 1)
-TESTATOB(RAW, 3, 3, 1, RGB24, 3, 3, 1)
-TESTATOB(RGB24, 3, 3, 1, ARGB, 4, 4, 1)
-TESTATOB(RGB24, 3, 3, 1, J400, 1, 1, 1)
-TESTATOB(RGB24, 3, 3, 1, RGB24Mirror, 3, 3, 1)
-TESTATOB(RAW, 3, 3, 1, J400, 1, 1, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, RGBA, uint8_t, 4, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, UYVY, uint8_t, 2, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, YUY2, uint8_t, 2, 4, 1)  // 4
+TESTATOB(ARGB1555, uint8_t, 2, 2, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(ARGB4444, uint8_t, 2, 2, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(BGRA, uint8_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(I400, uint8_t, 1, 1, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(I400, uint8_t, 1, 1, 1, I400, uint8_t, 1, 1, 1)
+TESTATOB(I400, uint8_t, 1, 1, 1, I400Mirror, uint8_t, 1, 1, 1)
+TESTATOB(J400, uint8_t, 1, 1, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(J400, uint8_t, 1, 1, 1, J400, uint8_t, 1, 1, 1)
+TESTATOB(RAW, uint8_t, 3, 3, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(RAW, uint8_t, 3, 3, 1, RGBA, uint8_t, 4, 4, 1)
+TESTATOB(RAW, uint8_t, 3, 3, 1, RGB24, uint8_t, 3, 3, 1)
+TESTATOB(RGB24, uint8_t, 3, 3, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(RGB24, uint8_t, 3, 3, 1, J400, uint8_t, 1, 1, 1)
+TESTATOB(RGB24, uint8_t, 3, 3, 1, RGB24Mirror, uint8_t, 3, 3, 1)
+TESTATOB(RAW, uint8_t, 3, 3, 1, J400, uint8_t, 1, 1, 1)
 #ifdef LITTLE_ENDIAN_ONLY_TEST
-TESTATOB(RGB565, 2, 2, 1, ARGB, 4, 4, 1)
+TESTATOB(RGB565, uint8_t, 2, 2, 1, ARGB, uint8_t, 4, 4, 1)
 #endif
-TESTATOB(RGBA, 4, 4, 1, ARGB, 4, 4, 1)
-TESTATOB(UYVY, 2, 4, 1, ARGB, 4, 4, 1)
-TESTATOB(YUY2, 2, 4, 1, ARGB, 4, 4, 1)
-TESTATOB(YUY2, 2, 4, 1, Y, 1, 1, 1)
+TESTATOB(RGBA, uint8_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(UYVY, uint8_t, 2, 4, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(YUY2, uint8_t, 2, 4, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(YUY2, uint8_t, 2, 4, 1, Y, uint8_t, 1, 1, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, AR64, uint16_t, 4, 4, 1)
+TESTATOB(ARGB, uint8_t, 4, 4, 1, AB64, uint16_t, 4, 4, 1)
+TESTATOB(ABGR, uint8_t, 4, 4, 1, AR64, uint16_t, 4, 4, 1)
+TESTATOB(ABGR, uint8_t, 4, 4, 1, AB64, uint16_t, 4, 4, 1)
+TESTATOB(AR64, uint16_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(AB64, uint16_t, 4, 4, 1, ARGB, uint8_t, 4, 4, 1)
+TESTATOB(AR64, uint16_t, 4, 4, 1, ABGR, uint8_t, 4, 4, 1)
+TESTATOB(AB64, uint16_t, 4, 4, 1, ABGR, uint8_t, 4, 4, 1)
+TESTATOB(AR64, uint16_t, 4, 4, 1, AB64, uint16_t, 4, 4, 1)
+TESTATOB(AB64, uint16_t, 4, 4, 1, AR64, uint16_t, 4, 4, 1)
 
 #define TESTATOBDI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B, \
                    HEIGHT_B, W1280, N, NEG, OFF)                             \
   TEST_F(LibYUVConvertTest, FMT_A##To##FMT_B##Dither##N) {                   \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                          \
+    const int kWidth = W1280;                                                \
     const int kHeight = benchmark_height_;                                   \
     const int kHeightA = (kHeight + HEIGHT_A - 1) / HEIGHT_A * HEIGHT_A;     \
     const int kHeightB = (kHeight + HEIGHT_B - 1) / HEIGHT_B * HEIGHT_B;     \
@@ -1429,7 +1446,7 @@ TESTATOB(YUY2, 2, 4, 1, Y, 1, 1, 1)
 #define TESTATOBD(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B, \
                   HEIGHT_B)                                                 \
   TESTATOBDI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,      \
-             HEIGHT_B, benchmark_width_ - 4, _Any, +, 0)                    \
+             HEIGHT_B, benchmark_width_ + 1, _Any, +, 0)                    \
   TESTATOBDI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,      \
              HEIGHT_B, benchmark_width_, _Unaligned, +, 1)                  \
   TESTATOBDI(FMT_A, BPP_A, STRIDE_A, HEIGHT_A, FMT_B, BPP_B, STRIDE_B,      \
@@ -1443,35 +1460,39 @@ TESTATOB(YUY2, 2, 4, 1, Y, 1, 1, 1)
 TESTATOBD(ARGB, 4, 4, 1, RGB565, 2, 2, 1)
 #endif
 
-#define TESTSYMI(FMT_ATOB, BPP_A, STRIDE_A, HEIGHT_A, W1280, N, NEG, OFF)      \
+#define TESTSYMI(FMT_ATOB, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, W1280, N, NEG,   \
+                 OFF)                                                          \
   TEST_F(LibYUVConvertTest, FMT_ATOB##_Symetric##N) {                          \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = benchmark_height_;                                     \
     const int kHeightA = (kHeight + HEIGHT_A - 1) / HEIGHT_A * HEIGHT_A;       \
     const int kStrideA =                                                       \
-        (kWidth * BPP_A + STRIDE_A - 1) / STRIDE_A * STRIDE_A;                 \
-    align_buffer_page_end(src_argb, kStrideA* kHeightA + OFF);                 \
-    align_buffer_page_end(dst_argb_c, kStrideA* kHeightA);                     \
-    align_buffer_page_end(dst_argb_opt, kStrideA* kHeightA);                   \
-    for (int i = 0; i < kStrideA * kHeightA; ++i) {                            \
+        (kWidth * EPP_A + STRIDE_A - 1) / STRIDE_A * STRIDE_A;                 \
+    align_buffer_page_end(src_argb,                                            \
+                          kStrideA* kHeightA*(int)sizeof(TYPE_A) + OFF);       \
+    align_buffer_page_end(dst_argb_c, kStrideA* kHeightA*(int)sizeof(TYPE_A)); \
+    align_buffer_page_end(dst_argb_opt,                                        \
+                          kStrideA* kHeightA*(int)sizeof(TYPE_A));             \
+    for (int i = 0; i < kStrideA * kHeightA * (int)sizeof(TYPE_A); ++i) {      \
       src_argb[i + OFF] = (fastrand() & 0xff);                                 \
     }                                                                          \
     memset(dst_argb_c, 1, kStrideA* kHeightA);                                 \
     memset(dst_argb_opt, 101, kStrideA* kHeightA);                             \
     MaskCpuFlags(disable_cpu_flags_);                                          \
-    FMT_ATOB(src_argb + OFF, kStrideA, dst_argb_c, kStrideA, kWidth,           \
-             NEG kHeight);                                                     \
+    FMT_ATOB((TYPE_A*)(src_argb + OFF), kStrideA, (TYPE_A*)dst_argb_c,         \
+             kStrideA, kWidth, NEG kHeight);                                   \
     MaskCpuFlags(benchmark_cpu_info_);                                         \
     for (int i = 0; i < benchmark_iterations_; ++i) {                          \
-      FMT_ATOB(src_argb + OFF, kStrideA, dst_argb_opt, kStrideA, kWidth,       \
-               NEG kHeight);                                                   \
+      FMT_ATOB((TYPE_A*)(src_argb + OFF), kStrideA, (TYPE_A*)dst_argb_opt,     \
+               kStrideA, kWidth, NEG kHeight);                                 \
     }                                                                          \
     MaskCpuFlags(disable_cpu_flags_);                                          \
-    FMT_ATOB(dst_argb_c, kStrideA, dst_argb_c, kStrideA, kWidth, NEG kHeight); \
+    FMT_ATOB((TYPE_A*)dst_argb_c, kStrideA, (TYPE_A*)dst_argb_c, kStrideA,     \
+             kWidth, NEG kHeight);                                             \
     MaskCpuFlags(benchmark_cpu_info_);                                         \
-    FMT_ATOB(dst_argb_opt, kStrideA, dst_argb_opt, kStrideA, kWidth,           \
-             NEG kHeight);                                                     \
-    for (int i = 0; i < kStrideA * kHeightA; ++i) {                            \
+    FMT_ATOB((TYPE_A*)dst_argb_opt, kStrideA, (TYPE_A*)dst_argb_opt, kStrideA, \
+             kWidth, NEG kHeight);                                             \
+    for (int i = 0; i < kStrideA * kHeightA * (int)sizeof(TYPE_A); ++i) {      \
       EXPECT_EQ(src_argb[i + OFF], dst_argb_opt[i]);                           \
       EXPECT_EQ(dst_argb_c[i], dst_argb_opt[i]);                               \
     }                                                                          \
@@ -1480,18 +1501,20 @@ TESTATOBD(ARGB, 4, 4, 1, RGB565, 2, 2, 1)
     free_aligned_buffer_page_end(dst_argb_opt);                                \
   }
 
-#define TESTSYM(FMT_ATOB, BPP_A, STRIDE_A, HEIGHT_A)                           \
-  TESTSYMI(FMT_ATOB, BPP_A, STRIDE_A, HEIGHT_A, benchmark_width_ - 4, _Any, +, \
-           0)                                                                  \
-  TESTSYMI(FMT_ATOB, BPP_A, STRIDE_A, HEIGHT_A, benchmark_width_, _Unaligned,  \
-           +, 1)                                                               \
-  TESTSYMI(FMT_ATOB, BPP_A, STRIDE_A, HEIGHT_A, benchmark_width_, _Opt, +, 0)
+#define TESTSYM(FMT_ATOB, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A)                  \
+  TESTSYMI(FMT_ATOB, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, benchmark_width_ + 1, \
+           _Any, +, 0)                                                        \
+  TESTSYMI(FMT_ATOB, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, benchmark_width_,     \
+           _Unaligned, +, 1)                                                  \
+  TESTSYMI(FMT_ATOB, TYPE_A, EPP_A, STRIDE_A, HEIGHT_A, benchmark_width_,     \
+           _Opt, +, 0)
 
-TESTSYM(ARGBToARGB, 4, 4, 1)
-TESTSYM(ARGBToBGRA, 4, 4, 1)
-TESTSYM(ARGBToABGR, 4, 4, 1)
-TESTSYM(BGRAToARGB, 4, 4, 1)
-TESTSYM(ABGRToARGB, 4, 4, 1)
+TESTSYM(ARGBToARGB, uint8_t, 4, 4, 1)
+TESTSYM(ARGBToBGRA, uint8_t, 4, 4, 1)
+TESTSYM(ARGBToABGR, uint8_t, 4, 4, 1)
+TESTSYM(BGRAToARGB, uint8_t, 4, 4, 1)
+TESTSYM(ABGRToARGB, uint8_t, 4, 4, 1)
+TESTSYM(AB64ToAR64, uint16_t, 4, 4, 1)
 
 TEST_F(LibYUVConvertTest, Test565) {
   SIMD_ALIGNED(uint8_t orig_pixels[256][4]);
@@ -2355,7 +2378,11 @@ TEST_F(LibYUVConvertTest, TestMJPGToARGB) {
 
   // Test result matches known hash value.
   uint32_t dst_argb_hash = HashDjb2(dst_argb, width * height, 5381);
+#ifdef LIBYUV_UNLIMITED_DATA
+  EXPECT_EQ(dst_argb_hash, 3900633302u);
+#else
   EXPECT_EQ(dst_argb_hash, 2355976473u);
+#endif
 
   free_aligned_buffer_page_end(dst_argb);
 }
@@ -2664,7 +2691,7 @@ TEST_F(LibYUVConvertTest, TestDither) {
 #define TESTPLANARTOBID(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN, \
                         YALIGN, W1280, N, NEG, OFF, FMT_C, BPP_C)              \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##Dither##N) {                \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = ALIGNINT(benchmark_height_, YALIGN);                   \
     const int kStrideB = ALIGNINT(kWidth * BPP_B, ALIGN);                      \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                        \
@@ -2717,7 +2744,7 @@ TEST_F(LibYUVConvertTest, TestDither) {
 #define TESTPLANARTOBD(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN, \
                        YALIGN, FMT_C, BPP_C)                                  \
   TESTPLANARTOBID(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
-                  YALIGN, benchmark_width_ - 4, _Any, +, 0, FMT_C, BPP_C)     \
+                  YALIGN, benchmark_width_ + 1, _Any, +, 0, FMT_C, BPP_C)     \
   TESTPLANARTOBID(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
                   YALIGN, benchmark_width_, _Unaligned, +, 1, FMT_C, BPP_C)   \
   TESTPLANARTOBID(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,      \
@@ -2794,7 +2821,7 @@ TESTPTOB(TestUYVYToNV12, UYVYToI420, UYVYToNV12)
 #define TESTPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B, \
                        W1280, N, NEG, OFF, FMT_C, BPP_C)                      \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##_##FMT_C##N) {             \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = benchmark_height_;                                    \
     const int kStrideB = SUBSAMPLE(kWidth, SUB_B) * BPP_B;                    \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                       \
@@ -2842,7 +2869,7 @@ TESTPTOB(TestUYVYToNV12, UYVYToI420, UYVYToNV12)
 #define TESTPLANARTOE(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B, \
                       FMT_C, BPP_C)                                          \
   TESTPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B,      \
-                 benchmark_width_ - 4, _Any, +, 0, FMT_C, BPP_C)             \
+                 benchmark_width_ + 1, _Any, +, 0, FMT_C, BPP_C)             \
   TESTPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B,      \
                  benchmark_width_, _Unaligned, +, 1, FMT_C, BPP_C)           \
   TESTPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B,      \
@@ -2906,7 +2933,7 @@ TESTPLANARTOE(I422, 2, 1, UYVY, 2, 4, ARGB, 4)
 #define TESTQPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B, \
                         W1280, N, NEG, OFF, FMT_C, BPP_C, ATTEN)               \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##_##FMT_C##N) {              \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = benchmark_height_;                                     \
     const int kStrideB = SUBSAMPLE(kWidth, SUB_B) * BPP_B;                     \
     const int kSizeUV =                                                        \
@@ -2959,7 +2986,7 @@ TESTPLANARTOE(I422, 2, 1, UYVY, 2, 4, ARGB, 4)
 #define TESTQPLANARTOE(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B, \
                        FMT_C, BPP_C)                                          \
   TESTQPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B,      \
-                  benchmark_width_ - 4, _Any, +, 0, FMT_C, BPP_C, 0)          \
+                  benchmark_width_ + 1, _Any, +, 0, FMT_C, BPP_C, 0)          \
   TESTQPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B,      \
                   benchmark_width_, _Unaligned, +, 1, FMT_C, BPP_C, 0)        \
   TESTQPLANARTOEI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, SUB_B, BPP_B,      \
@@ -3007,7 +3034,7 @@ TESTQPLANARTOE(V444Alpha, 1, 1, ABGR, 1, 4, ARGB, 4)
 #define TESTPLANETOEI(FMT_A, SUB_A, BPP_A, FMT_B, SUB_B, BPP_B, W1280, N, NEG, \
                       OFF, FMT_C, BPP_C)                                       \
   TEST_F(LibYUVConvertTest, FMT_A##To##FMT_B##_##FMT_C##N) {                   \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = benchmark_height_;                                     \
     const int kStrideA = SUBSAMPLE(kWidth, SUB_A) * BPP_A;                     \
     const int kStrideB = SUBSAMPLE(kWidth, SUB_B) * BPP_B;                     \
@@ -3044,7 +3071,7 @@ TESTQPLANARTOE(V444Alpha, 1, 1, ABGR, 1, 4, ARGB, 4)
 
 #define TESTPLANETOE(FMT_A, SUB_A, BPP_A, FMT_B, SUB_B, BPP_B, FMT_C, BPP_C) \
   TESTPLANETOEI(FMT_A, SUB_A, BPP_A, FMT_B, SUB_B, BPP_B,                    \
-                benchmark_width_ - 4, _Any, +, 0, FMT_C, BPP_C)              \
+                benchmark_width_ + 1, _Any, +, 0, FMT_C, BPP_C)              \
   TESTPLANETOEI(FMT_A, SUB_A, BPP_A, FMT_B, SUB_B, BPP_B, benchmark_width_,  \
                 _Unaligned, +, 1, FMT_C, BPP_C)                              \
   TESTPLANETOEI(FMT_A, SUB_A, BPP_A, FMT_B, SUB_B, BPP_B, benchmark_width_,  \
@@ -3202,7 +3229,7 @@ TEST_F(LibYUVConvertTest, ABGRToAR30Row_Opt) {
 #define TESTPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_MASK, FMT_B,   \
                          BPP_B, ALIGN, YALIGN, W1280, N, NEG, SOFF, DOFF)     \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##N) {                       \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = ALIGNINT(benchmark_height_, YALIGN);                  \
     const int kStrideB = ALIGNINT(kWidth * BPP_B, ALIGN);                     \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                       \
@@ -3249,7 +3276,7 @@ TEST_F(LibYUVConvertTest, ABGRToAR30Row_Opt) {
 #define TESTPLANAR16TOB(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_MASK, FMT_B,   \
                         BPP_B, ALIGN, YALIGN)                                \
   TESTPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_MASK, FMT_B, BPP_B, \
-                   ALIGN, YALIGN, benchmark_width_ - 4, _Any, +, 0, 0)       \
+                   ALIGN, YALIGN, benchmark_width_ + 1, _Any, +, 0, 0)       \
   TESTPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_MASK, FMT_B, BPP_B, \
                    ALIGN, YALIGN, benchmark_width_, _Unaligned, +, 1, 1)     \
   TESTPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_MASK, FMT_B, BPP_B, \
@@ -3305,7 +3332,7 @@ TESTPLANAR16TOB(I012, 2, 2, 0xfff, AR30, 4, 4, 1)
 #define TESTQPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B,      \
                           ALIGN, YALIGN, W1280, N, NEG, OFF, ATTEN, S_DEPTH)   \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##N) {                        \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = ALIGNINT(benchmark_height_, YALIGN);                   \
     const int kStrideB = ALIGNINT(kWidth * BPP_B, ALIGN);                      \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X);                        \
@@ -3361,7 +3388,7 @@ TESTPLANAR16TOB(I012, 2, 2, 0xfff, AR30, 4, 4, 1)
 #define TESTQPLANAR16TOB(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B,    \
                          ALIGN, YALIGN, S_DEPTH)                            \
   TESTQPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,  \
-                    YALIGN, benchmark_width_ - 4, _Any, +, 0, 0, S_DEPTH)   \
+                    YALIGN, benchmark_width_ + 1, _Any, +, 0, 0, S_DEPTH)   \
   TESTQPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,  \
                     YALIGN, benchmark_width_, _Unaligned, +, 1, 0, S_DEPTH) \
   TESTQPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,  \
@@ -3523,7 +3550,7 @@ TESTQPLANAR16TOB(V410Alpha, 1, 1, ABGR, 4, 4, 1, 10)
 #define TESTBIPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B,     \
                            ALIGN, YALIGN, W1280, N, NEG, SOFF, DOFF, S_DEPTH)  \
   TEST_F(LibYUVConvertTest, FMT_PLANAR##To##FMT_B##N) {                        \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                            \
+    const int kWidth = W1280;                                                  \
     const int kHeight = ALIGNINT(benchmark_height_, YALIGN);                   \
     const int kStrideB = ALIGNINT(kWidth * BPP_B, ALIGN);                      \
     const int kStrideUV = SUBSAMPLE(kWidth, SUBSAMP_X) * 2;                    \
@@ -3567,7 +3594,7 @@ TESTQPLANAR16TOB(V410Alpha, 1, 1, ABGR, 4, 4, 1, 10)
 #define TESTBIPLANAR16TOB(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B,    \
                           ALIGN, YALIGN, S_DEPTH)                            \
   TESTBIPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,  \
-                     YALIGN, benchmark_width_ - 4, _Any, +, 0, 0, S_DEPTH)   \
+                     YALIGN, benchmark_width_ + 1, _Any, +, 0, 0, S_DEPTH)   \
   TESTBIPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,  \
                      YALIGN, benchmark_width_, _Unaligned, +, 1, 1, S_DEPTH) \
   TESTBIPLANAR16TOBI(FMT_PLANAR, SUBSAMP_X, SUBSAMP_Y, FMT_B, BPP_B, ALIGN,  \
diff --git a/unit_test/planar_test.cc b/unit_test/planar_test.cc
index fd1755cd..5c608421 100644
--- a/unit_test/planar_test.cc
+++ b/unit_test/planar_test.cc
@@ -155,7 +155,7 @@ static int TestAttenuateI(int width,
 }
 
 TEST_F(LibYUVPlanarTest, ARGBAttenuate_Any) {
-  int max_diff = TestAttenuateI(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestAttenuateI(benchmark_width_ + 1, benchmark_height_,
                                 benchmark_iterations_, disable_cpu_flags_,
                                 benchmark_cpu_info_, +1, 0);
   EXPECT_LE(max_diff, 2);
@@ -228,7 +228,7 @@ static int TestUnattenuateI(int width,
 }
 
 TEST_F(LibYUVPlanarTest, ARGBUnattenuate_Any) {
-  int max_diff = TestUnattenuateI(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestUnattenuateI(benchmark_width_ + 1, benchmark_height_,
                                   benchmark_iterations_, disable_cpu_flags_,
                                   benchmark_cpu_info_, +1, 0);
   EXPECT_LE(max_diff, 2);
@@ -1076,7 +1076,7 @@ TEST_F(LibYUVPlanarTest, TestInterpolatePlane) {
 #define TESTTERP(FMT_A, BPP_A, STRIDE_A, FMT_B, BPP_B, STRIDE_B, W1280, TERP, \
                  N, NEG, OFF)                                                 \
   TEST_F(LibYUVPlanarTest, ARGBInterpolate##TERP##N) {                        \
-    const int kWidth = ((W1280) > 0) ? (W1280) : 1;                           \
+    const int kWidth = W1280;                                                 \
     const int kHeight = benchmark_height_;                                    \
     const int kStrideA =                                                      \
         (kWidth * BPP_A + STRIDE_A - 1) / STRIDE_A * STRIDE_A;                \
@@ -1108,7 +1108,7 @@ TEST_F(LibYUVPlanarTest, TestInterpolatePlane) {
   }
 
 #define TESTINTERPOLATE(TERP)                                                \
-  TESTTERP(ARGB, 4, 1, ARGB, 4, 1, benchmark_width_ - 1, TERP, _Any, +, 0)   \
+  TESTTERP(ARGB, 4, 1, ARGB, 4, 1, benchmark_width_ + 1, TERP, _Any, +, 0)   \
   TESTTERP(ARGB, 4, 1, ARGB, 4, 1, benchmark_width_, TERP, _Unaligned, +, 1) \
   TESTTERP(ARGB, 4, 1, ARGB, 4, 1, benchmark_width_, TERP, _Invert, -, 0)    \
   TESTTERP(ARGB, 4, 1, ARGB, 4, 1, benchmark_width_, TERP, _Opt, +, 0)
@@ -1174,7 +1174,7 @@ static int TestBlend(int width,
 
 TEST_F(LibYUVPlanarTest, ARGBBlend_Any) {
   int max_diff =
-      TestBlend(benchmark_width_ - 4, benchmark_height_, benchmark_iterations_,
+      TestBlend(benchmark_width_ + 1, benchmark_height_, benchmark_iterations_,
                 disable_cpu_flags_, benchmark_cpu_info_, +1, 0, 1);
   EXPECT_LE(max_diff, 1);
 }
@@ -1280,7 +1280,7 @@ TEST_F(LibYUVPlanarTest, BlendPlane_Unaligned) {
                  disable_cpu_flags_, benchmark_cpu_info_, +1, 1);
 }
 TEST_F(LibYUVPlanarTest, BlendPlane_Any) {
-  TestBlendPlane(benchmark_width_ - 4, benchmark_height_, benchmark_iterations_,
+  TestBlendPlane(benchmark_width_ + 1, benchmark_height_, benchmark_iterations_,
                  disable_cpu_flags_, benchmark_cpu_info_, +1, 1);
 }
 TEST_F(LibYUVPlanarTest, BlendPlane_Invert) {
@@ -1375,7 +1375,7 @@ TEST_F(LibYUVPlanarTest, I420Blend_Unaligned) {
 
 // TODO(fbarchard): DISABLED because _Any uses C.  Avoid C and re-enable.
 TEST_F(LibYUVPlanarTest, DISABLED_I420Blend_Any) {
-  TestI420Blend(benchmark_width_ - 4, benchmark_height_, benchmark_iterations_,
+  TestI420Blend(benchmark_width_ + 1, benchmark_height_, benchmark_iterations_,
                 disable_cpu_flags_, benchmark_cpu_info_, +1, 0);
 }
 TEST_F(LibYUVPlanarTest, I420Blend_Invert) {
@@ -1524,7 +1524,7 @@ static int TestMultiply(int width,
 }
 
 TEST_F(LibYUVPlanarTest, ARGBMultiply_Any) {
-  int max_diff = TestMultiply(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestMultiply(benchmark_width_ + 1, benchmark_height_,
                               benchmark_iterations_, disable_cpu_flags_,
                               benchmark_cpu_info_, +1, 0);
   EXPECT_LE(max_diff, 1);
@@ -1599,7 +1599,7 @@ static int TestAdd(int width,
 
 TEST_F(LibYUVPlanarTest, ARGBAdd_Any) {
   int max_diff =
-      TestAdd(benchmark_width_ - 1, benchmark_height_, benchmark_iterations_,
+      TestAdd(benchmark_width_ + 1, benchmark_height_, benchmark_iterations_,
               disable_cpu_flags_, benchmark_cpu_info_, +1, 0);
   EXPECT_LE(max_diff, 1);
 }
@@ -1672,7 +1672,7 @@ static int TestSubtract(int width,
 }
 
 TEST_F(LibYUVPlanarTest, ARGBSubtract_Any) {
-  int max_diff = TestSubtract(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestSubtract(benchmark_width_ + 1, benchmark_height_,
                               benchmark_iterations_, disable_cpu_flags_,
                               benchmark_cpu_info_, +1, 0);
   EXPECT_LE(max_diff, 1);
@@ -1745,7 +1745,7 @@ static int TestSobel(int width,
 
 TEST_F(LibYUVPlanarTest, ARGBSobel_Any) {
   int max_diff =
-      TestSobel(benchmark_width_ - 1, benchmark_height_, benchmark_iterations_,
+      TestSobel(benchmark_width_ + 1, benchmark_height_, benchmark_iterations_,
                 disable_cpu_flags_, benchmark_cpu_info_, +1, 0);
   EXPECT_EQ(0, max_diff);
 }
@@ -1818,7 +1818,7 @@ static int TestSobelToPlane(int width,
 }
 
 TEST_F(LibYUVPlanarTest, ARGBSobelToPlane_Any) {
-  int max_diff = TestSobelToPlane(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestSobelToPlane(benchmark_width_ + 1, benchmark_height_,
                                   benchmark_iterations_, disable_cpu_flags_,
                                   benchmark_cpu_info_, +1, 0);
   EXPECT_EQ(0, max_diff);
@@ -1890,7 +1890,7 @@ static int TestSobelXY(int width,
 }
 
 TEST_F(LibYUVPlanarTest, ARGBSobelXY_Any) {
-  int max_diff = TestSobelXY(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestSobelXY(benchmark_width_ + 1, benchmark_height_,
                              benchmark_iterations_, disable_cpu_flags_,
                              benchmark_cpu_info_, +1, 0);
   EXPECT_EQ(0, max_diff);
@@ -1966,29 +1966,35 @@ static int TestBlur(int width,
   return max_diff;
 }
 
+#if defined(ENABLE_SLOW_TESTS) || defined(__x86_64__) || defined(__i386__)
+#define DISABLED_ARM(name) name
+#else
+#define DISABLED_ARM(name) DISABLED_##name
+#endif
+
 static const int kBlurSize = 55;
-TEST_F(LibYUVPlanarTest, ARGBBlur_Any) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlur_Any)) {
   int max_diff =
-      TestBlur(benchmark_width_ - 1, benchmark_height_, benchmark_iterations_,
+      TestBlur(benchmark_width_ + 1, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, +1, 0, kBlurSize);
   EXPECT_LE(max_diff, 1);
 }
 
-TEST_F(LibYUVPlanarTest, ARGBBlur_Unaligned) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlur_Unaligned)) {
   int max_diff =
       TestBlur(benchmark_width_, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, +1, 1, kBlurSize);
   EXPECT_LE(max_diff, 1);
 }
 
-TEST_F(LibYUVPlanarTest, ARGBBlur_Invert) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlur_Invert)) {
   int max_diff =
       TestBlur(benchmark_width_, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, -1, 0, kBlurSize);
   EXPECT_LE(max_diff, 1);
 }
 
-TEST_F(LibYUVPlanarTest, ARGBBlur_Opt) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlur_Opt)) {
   int max_diff =
       TestBlur(benchmark_width_, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, +1, 0, kBlurSize);
@@ -1996,35 +2002,35 @@ TEST_F(LibYUVPlanarTest, ARGBBlur_Opt) {
 }
 
 static const int kBlurSmallSize = 5;
-TEST_F(LibYUVPlanarTest, ARGBBlurSmall_Any) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlurSmall_Any)) {
   int max_diff =
-      TestBlur(benchmark_width_ - 1, benchmark_height_, benchmark_iterations_,
+      TestBlur(benchmark_width_ + 1, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, +1, 0, kBlurSmallSize);
   EXPECT_LE(max_diff, 1);
 }
 
-TEST_F(LibYUVPlanarTest, ARGBBlurSmall_Unaligned) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlurSmall_Unaligned)) {
   int max_diff =
       TestBlur(benchmark_width_, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, +1, 1, kBlurSmallSize);
   EXPECT_LE(max_diff, 1);
 }
 
-TEST_F(LibYUVPlanarTest, ARGBBlurSmall_Invert) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlurSmall_Invert)) {
   int max_diff =
       TestBlur(benchmark_width_, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, -1, 0, kBlurSmallSize);
   EXPECT_LE(max_diff, 1);
 }
 
-TEST_F(LibYUVPlanarTest, ARGBBlurSmall_Opt) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(ARGBBlurSmall_Opt)) {
   int max_diff =
       TestBlur(benchmark_width_, benchmark_height_, benchmark_iterations_,
                disable_cpu_flags_, benchmark_cpu_info_, +1, 0, kBlurSmallSize);
   EXPECT_LE(max_diff, 1);
 }
 
-TEST_F(LibYUVPlanarTest, TestARGBPolynomial) {
+TEST_F(LibYUVPlanarTest, DISABLED_ARM(TestARGBPolynomial)) {
   SIMD_ALIGNED(uint8_t orig_pixels[1280][4]);
   SIMD_ALIGNED(uint8_t dst_pixels_opt[1280][4]);
   SIMD_ALIGNED(uint8_t dst_pixels_c[1280][4]);
@@ -2398,8 +2404,7 @@ TEST_F(LibYUVPlanarTest, TestARGBCopyAlpha) {
 }
 
 TEST_F(LibYUVPlanarTest, TestARGBExtractAlpha) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 4);
   align_buffer_page_end(dst_pixels_opt, kPixels);
   align_buffer_page_end(dst_pixels_c, kPixels);
@@ -2427,8 +2432,7 @@ TEST_F(LibYUVPlanarTest, TestARGBExtractAlpha) {
 }
 
 TEST_F(LibYUVPlanarTest, TestARGBCopyYToAlpha) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(orig_pixels, kPixels);
   align_buffer_page_end(dst_pixels_opt, kPixels * 4);
   align_buffer_page_end(dst_pixels_c, kPixels * 4);
@@ -2505,7 +2509,7 @@ static int TestARGBRect(int width,
 }
 
 TEST_F(LibYUVPlanarTest, ARGBRect_Any) {
-  int max_diff = TestARGBRect(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestARGBRect(benchmark_width_ + 1, benchmark_height_,
                               benchmark_iterations_, disable_cpu_flags_,
                               benchmark_cpu_info_, +1, 0, 4);
   EXPECT_EQ(0, max_diff);
@@ -2533,7 +2537,7 @@ TEST_F(LibYUVPlanarTest, ARGBRect_Opt) {
 }
 
 TEST_F(LibYUVPlanarTest, SetPlane_Any) {
-  int max_diff = TestARGBRect(benchmark_width_ - 1, benchmark_height_,
+  int max_diff = TestARGBRect(benchmark_width_ + 1, benchmark_height_,
                               benchmark_iterations_, disable_cpu_flags_,
                               benchmark_cpu_info_, +1, 0, 1);
   EXPECT_EQ(0, max_diff);
@@ -2561,35 +2565,25 @@ TEST_F(LibYUVPlanarTest, SetPlane_Opt) {
 }
 
 TEST_F(LibYUVPlanarTest, MergeUVPlane_Opt) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
-  align_buffer_page_end(src_pixels, kPixels * 2);
-  align_buffer_page_end(tmp_pixels_u, kPixels);
-  align_buffer_page_end(tmp_pixels_v, kPixels);
+  const int kPixels = benchmark_width_ * benchmark_height_;
+  align_buffer_page_end(src_pixels_u, kPixels);
+  align_buffer_page_end(src_pixels_v, kPixels);
   align_buffer_page_end(dst_pixels_opt, kPixels * 2);
   align_buffer_page_end(dst_pixels_c, kPixels * 2);
 
-  MemRandomize(src_pixels, kPixels * 2);
-  MemRandomize(tmp_pixels_u, kPixels);
-  MemRandomize(tmp_pixels_v, kPixels);
+  MemRandomize(src_pixels_u, kPixels);
+  MemRandomize(src_pixels_v, kPixels);
   MemRandomize(dst_pixels_opt, kPixels * 2);
   MemRandomize(dst_pixels_c, kPixels * 2);
 
   MaskCpuFlags(disable_cpu_flags_);
-  SplitUVPlane(src_pixels, benchmark_width_ * 2, tmp_pixels_u, benchmark_width_,
-               tmp_pixels_v, benchmark_width_, benchmark_width_,
-               benchmark_height_);
-  MergeUVPlane(tmp_pixels_u, benchmark_width_, tmp_pixels_v, benchmark_width_,
+  MergeUVPlane(src_pixels_u, benchmark_width_, src_pixels_v, benchmark_width_,
                dst_pixels_c, benchmark_width_ * 2, benchmark_width_,
                benchmark_height_);
   MaskCpuFlags(benchmark_cpu_info_);
 
-  SplitUVPlane(src_pixels, benchmark_width_ * 2, tmp_pixels_u, benchmark_width_,
-               tmp_pixels_v, benchmark_width_, benchmark_width_,
-               benchmark_height_);
-
   for (int i = 0; i < benchmark_iterations_; ++i) {
-    MergeUVPlane(tmp_pixels_u, benchmark_width_, tmp_pixels_v, benchmark_width_,
+    MergeUVPlane(src_pixels_u, benchmark_width_, src_pixels_v, benchmark_width_,
                  dst_pixels_opt, benchmark_width_ * 2, benchmark_width_,
                  benchmark_height_);
   }
@@ -2598,60 +2592,127 @@ TEST_F(LibYUVPlanarTest, MergeUVPlane_Opt) {
     EXPECT_EQ(dst_pixels_c[i], dst_pixels_opt[i]);
   }
 
-  free_aligned_buffer_page_end(src_pixels);
-  free_aligned_buffer_page_end(tmp_pixels_u);
-  free_aligned_buffer_page_end(tmp_pixels_v);
+  free_aligned_buffer_page_end(src_pixels_u);
+  free_aligned_buffer_page_end(src_pixels_v);
+  free_aligned_buffer_page_end(dst_pixels_opt);
+  free_aligned_buffer_page_end(dst_pixels_c);
+}
+
+// 16 bit channel split and merge
+TEST_F(LibYUVPlanarTest, MergeUVPlane_16_Opt) {
+  const int kPixels = benchmark_width_ * benchmark_height_;
+  align_buffer_page_end(src_pixels_u, kPixels * 2);
+  align_buffer_page_end(src_pixels_v, kPixels * 2);
+  align_buffer_page_end(dst_pixels_opt, kPixels * 2 * 2);
+  align_buffer_page_end(dst_pixels_c, kPixels * 2 * 2);
+  MemRandomize(src_pixels_u, kPixels * 2);
+  MemRandomize(src_pixels_v, kPixels * 2);
+  MemRandomize(dst_pixels_opt, kPixels * 2 * 2);
+  MemRandomize(dst_pixels_c, kPixels * 2 * 2);
+
+  MaskCpuFlags(disable_cpu_flags_);
+  MergeUVPlane_16((const uint16_t*)src_pixels_u, benchmark_width_,
+                  (const uint16_t*)src_pixels_v, benchmark_width_,
+                  (uint16_t*)dst_pixels_c, benchmark_width_ * 2,
+                  benchmark_width_, benchmark_height_, 12);
+  MaskCpuFlags(benchmark_cpu_info_);
+
+  for (int i = 0; i < benchmark_iterations_; ++i) {
+    MergeUVPlane_16((const uint16_t*)src_pixels_u, benchmark_width_,
+                    (const uint16_t*)src_pixels_v, benchmark_width_,
+                    (uint16_t*)dst_pixels_opt, benchmark_width_ * 2,
+                    benchmark_width_, benchmark_height_, 12);
+  }
+
+  for (int i = 0; i < kPixels * 2 * 2; ++i) {
+    EXPECT_EQ(dst_pixels_c[i], dst_pixels_opt[i]);
+  }
+  free_aligned_buffer_page_end(src_pixels_u);
+  free_aligned_buffer_page_end(src_pixels_v);
   free_aligned_buffer_page_end(dst_pixels_opt);
   free_aligned_buffer_page_end(dst_pixels_c);
 }
 
 TEST_F(LibYUVPlanarTest, SplitUVPlane_Opt) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 2);
-  align_buffer_page_end(tmp_pixels_u, kPixels);
-  align_buffer_page_end(tmp_pixels_v, kPixels);
-  align_buffer_page_end(dst_pixels_opt, kPixels * 2);
-  align_buffer_page_end(dst_pixels_c, kPixels * 2);
+  align_buffer_page_end(dst_pixels_u_c, kPixels);
+  align_buffer_page_end(dst_pixels_v_c, kPixels);
+  align_buffer_page_end(dst_pixels_u_opt, kPixels);
+  align_buffer_page_end(dst_pixels_v_opt, kPixels);
 
   MemRandomize(src_pixels, kPixels * 2);
-  MemRandomize(tmp_pixels_u, kPixels);
-  MemRandomize(tmp_pixels_v, kPixels);
-  MemRandomize(dst_pixels_opt, kPixels * 2);
-  MemRandomize(dst_pixels_c, kPixels * 2);
+  MemRandomize(dst_pixels_u_c, kPixels);
+  MemRandomize(dst_pixels_v_c, kPixels);
+  MemRandomize(dst_pixels_u_opt, kPixels);
+  MemRandomize(dst_pixels_v_opt, kPixels);
 
   MaskCpuFlags(disable_cpu_flags_);
-  SplitUVPlane(src_pixels, benchmark_width_ * 2, tmp_pixels_u, benchmark_width_,
-               tmp_pixels_v, benchmark_width_, benchmark_width_,
-               benchmark_height_);
-  MergeUVPlane(tmp_pixels_u, benchmark_width_, tmp_pixels_v, benchmark_width_,
-               dst_pixels_c, benchmark_width_ * 2, benchmark_width_,
-               benchmark_height_);
+  SplitUVPlane(src_pixels, benchmark_width_ * 2, dst_pixels_u_c,
+               benchmark_width_, dst_pixels_v_c, benchmark_width_,
+               benchmark_width_, benchmark_height_);
   MaskCpuFlags(benchmark_cpu_info_);
 
   for (int i = 0; i < benchmark_iterations_; ++i) {
-    SplitUVPlane(src_pixels, benchmark_width_ * 2, tmp_pixels_u,
-                 benchmark_width_, tmp_pixels_v, benchmark_width_,
+    SplitUVPlane(src_pixels, benchmark_width_ * 2, dst_pixels_u_opt,
+                 benchmark_width_, dst_pixels_v_opt, benchmark_width_,
                  benchmark_width_, benchmark_height_);
   }
-  MergeUVPlane(tmp_pixels_u, benchmark_width_, tmp_pixels_v, benchmark_width_,
-               dst_pixels_opt, benchmark_width_ * 2, benchmark_width_,
-               benchmark_height_);
 
-  for (int i = 0; i < kPixels * 2; ++i) {
-    EXPECT_EQ(dst_pixels_c[i], dst_pixels_opt[i]);
+  for (int i = 0; i < kPixels; ++i) {
+    EXPECT_EQ(dst_pixels_u_c[i], dst_pixels_u_opt[i]);
+    EXPECT_EQ(dst_pixels_v_c[i], dst_pixels_v_opt[i]);
   }
 
   free_aligned_buffer_page_end(src_pixels);
-  free_aligned_buffer_page_end(tmp_pixels_u);
-  free_aligned_buffer_page_end(tmp_pixels_v);
-  free_aligned_buffer_page_end(dst_pixels_opt);
-  free_aligned_buffer_page_end(dst_pixels_c);
+  free_aligned_buffer_page_end(dst_pixels_u_c);
+  free_aligned_buffer_page_end(dst_pixels_v_c);
+  free_aligned_buffer_page_end(dst_pixels_u_opt);
+  free_aligned_buffer_page_end(dst_pixels_v_opt);
+}
+
+// 16 bit channel split
+TEST_F(LibYUVPlanarTest, SplitUVPlane_16_Opt) {
+  const int kPixels = benchmark_width_ * benchmark_height_;
+  align_buffer_page_end(src_pixels, kPixels * 2 * 2);
+  align_buffer_page_end(dst_pixels_u_c, kPixels * 2);
+  align_buffer_page_end(dst_pixels_v_c, kPixels * 2);
+  align_buffer_page_end(dst_pixels_u_opt, kPixels * 2);
+  align_buffer_page_end(dst_pixels_v_opt, kPixels * 2);
+  MemRandomize(src_pixels, kPixels * 2 * 2);
+  MemRandomize(dst_pixels_u_c, kPixels * 2);
+  MemRandomize(dst_pixels_v_c, kPixels * 2);
+  MemRandomize(dst_pixels_u_opt, kPixels * 2);
+  MemRandomize(dst_pixels_v_opt, kPixels * 2);
+
+  MaskCpuFlags(disable_cpu_flags_);
+  SplitUVPlane_16((const uint16_t*)src_pixels, benchmark_width_ * 2,
+                  (uint16_t*)dst_pixels_u_c, benchmark_width_,
+                  (uint16_t*)dst_pixels_v_c, benchmark_width_, benchmark_width_,
+                  benchmark_height_, 10);
+  MaskCpuFlags(benchmark_cpu_info_);
+
+  for (int i = 0; i < benchmark_iterations_; ++i) {
+    SplitUVPlane_16((const uint16_t*)src_pixels, benchmark_width_ * 2,
+                    (uint16_t*)dst_pixels_u_opt, benchmark_width_,
+                    (uint16_t*)dst_pixels_v_opt, benchmark_width_,
+                    benchmark_width_, benchmark_height_, 10);
+  }
+
+  for (int i = 0; i < kPixels * 2; ++i) {
+    EXPECT_EQ(dst_pixels_u_c[i], dst_pixels_u_opt[i]);
+    EXPECT_EQ(dst_pixels_v_c[i], dst_pixels_v_opt[i]);
+  }
+  free_aligned_buffer_page_end(src_pixels);
+  free_aligned_buffer_page_end(dst_pixels_u_c);
+  free_aligned_buffer_page_end(dst_pixels_v_c);
+  free_aligned_buffer_page_end(dst_pixels_u_opt);
+  free_aligned_buffer_page_end(dst_pixels_v_opt);
 }
 
 TEST_F(LibYUVPlanarTest, SwapUVPlane_Opt) {
   // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 2);
   align_buffer_page_end(dst_pixels_opt, kPixels * 2);
   align_buffer_page_end(dst_pixels_c, kPixels * 2);
@@ -2681,7 +2742,7 @@ TEST_F(LibYUVPlanarTest, SwapUVPlane_Opt) {
 
 TEST_F(LibYUVPlanarTest, MergeRGBPlane_Opt) {
   // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 3);
   align_buffer_page_end(tmp_pixels_r, kPixels);
   align_buffer_page_end(tmp_pixels_g, kPixels);
@@ -2730,7 +2791,7 @@ TEST_F(LibYUVPlanarTest, MergeRGBPlane_Opt) {
 
 TEST_F(LibYUVPlanarTest, SplitRGBPlane_Opt) {
   // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 3);
   align_buffer_page_end(tmp_pixels_r, kPixels);
   align_buffer_page_end(tmp_pixels_g, kPixels);
@@ -2777,8 +2838,7 @@ TEST_F(LibYUVPlanarTest, SplitRGBPlane_Opt) {
 }
 
 TEST_F(LibYUVPlanarTest, MergeARGBPlane_Opt) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 4);
   align_buffer_page_end(tmp_pixels_r, kPixels);
   align_buffer_page_end(tmp_pixels_g, kPixels);
@@ -2832,8 +2892,7 @@ TEST_F(LibYUVPlanarTest, MergeARGBPlane_Opt) {
 }
 
 TEST_F(LibYUVPlanarTest, SplitARGBPlane_Opt) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 4);
   align_buffer_page_end(tmp_pixels_r, kPixels);
   align_buffer_page_end(tmp_pixels_g, kPixels);
@@ -2887,8 +2946,7 @@ TEST_F(LibYUVPlanarTest, SplitARGBPlane_Opt) {
 }
 
 TEST_F(LibYUVPlanarTest, MergeXRGBPlane_Opt) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 4);
   align_buffer_page_end(tmp_pixels_r, kPixels);
   align_buffer_page_end(tmp_pixels_g, kPixels);
@@ -2938,8 +2996,7 @@ TEST_F(LibYUVPlanarTest, MergeXRGBPlane_Opt) {
 }
 
 TEST_F(LibYUVPlanarTest, SplitXRGBPlane_Opt) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels, kPixels * 4);
   align_buffer_page_end(tmp_pixels_r, kPixels);
   align_buffer_page_end(tmp_pixels_g, kPixels);
@@ -2987,11 +3044,166 @@ TEST_F(LibYUVPlanarTest, SplitXRGBPlane_Opt) {
   free_aligned_buffer_page_end(dst_pixels_c);
 }
 
+// Merge 4 channels
+#define TESTQPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, W1280, N, NEG, OFF)      \
+  TEST_F(LibYUVPlanarTest, FUNC##Plane_##DEPTH##N) {                        \
+    const int kWidth = W1280;                                               \
+    const int kPixels = kWidth * benchmark_height_;                         \
+    align_buffer_page_end(src_memory_r, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(src_memory_g, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(src_memory_b, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(src_memory_a, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(dst_memory_c, kPixels * 4 * sizeof(DTYPE));       \
+    align_buffer_page_end(dst_memory_opt, kPixels * 4 * sizeof(DTYPE));     \
+    MemRandomize(src_memory_r, kPixels * sizeof(STYPE) + OFF);              \
+    MemRandomize(src_memory_g, kPixels * sizeof(STYPE) + OFF);              \
+    MemRandomize(src_memory_b, kPixels * sizeof(STYPE) + OFF);              \
+    MemRandomize(src_memory_a, kPixels * sizeof(STYPE) + OFF);              \
+    memset(dst_memory_c, 0, kPixels * 4 * sizeof(DTYPE));                   \
+    memset(dst_memory_opt, 0, kPixels * 4 * sizeof(DTYPE));                 \
+    STYPE* src_pixels_r = reinterpret_cast<STYPE*>(src_memory_r + OFF);     \
+    STYPE* src_pixels_g = reinterpret_cast<STYPE*>(src_memory_g + OFF);     \
+    STYPE* src_pixels_b = reinterpret_cast<STYPE*>(src_memory_b + OFF);     \
+    STYPE* src_pixels_a = reinterpret_cast<STYPE*>(src_memory_a + OFF);     \
+    DTYPE* dst_pixels_c = reinterpret_cast<DTYPE*>(dst_memory_c);           \
+    DTYPE* dst_pixels_opt = reinterpret_cast<DTYPE*>(dst_memory_opt);       \
+    MaskCpuFlags(disable_cpu_flags_);                                       \
+    FUNC##Plane(src_pixels_r, kWidth, src_pixels_g, kWidth, src_pixels_b,   \
+                kWidth, src_pixels_a, kWidth, dst_pixels_c, kWidth * 4,     \
+                kWidth, NEG benchmark_height_, DEPTH);                      \
+    MaskCpuFlags(benchmark_cpu_info_);                                      \
+    for (int i = 0; i < benchmark_iterations_; ++i) {                       \
+      FUNC##Plane(src_pixels_r, kWidth, src_pixels_g, kWidth, src_pixels_b, \
+                  kWidth, src_pixels_a, kWidth, dst_pixels_opt, kWidth * 4, \
+                  kWidth, NEG benchmark_height_, DEPTH);                    \
+    }                                                                       \
+    for (int i = 0; i < kPixels * 4; ++i) {                                 \
+      EXPECT_EQ(dst_pixels_c[i], dst_pixels_opt[i]);                        \
+    }                                                                       \
+    free_aligned_buffer_page_end(src_memory_r);                             \
+    free_aligned_buffer_page_end(src_memory_g);                             \
+    free_aligned_buffer_page_end(src_memory_b);                             \
+    free_aligned_buffer_page_end(src_memory_a);                             \
+    free_aligned_buffer_page_end(dst_memory_c);                             \
+    free_aligned_buffer_page_end(dst_memory_opt);                           \
+  }
+
+// Merge 3 channel RGB into 4 channel XRGB with opaque alpha
+#define TESTQPLANAROTOPI(FUNC, STYPE, DTYPE, DEPTH, W1280, N, NEG, OFF)     \
+  TEST_F(LibYUVPlanarTest, FUNC##Plane_Opaque_##DEPTH##N) {                 \
+    const int kWidth = W1280;                                               \
+    const int kPixels = kWidth * benchmark_height_;                         \
+    align_buffer_page_end(src_memory_r, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(src_memory_g, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(src_memory_b, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(dst_memory_c, kPixels * 4 * sizeof(DTYPE));       \
+    align_buffer_page_end(dst_memory_opt, kPixels * 4 * sizeof(DTYPE));     \
+    MemRandomize(src_memory_r, kPixels * sizeof(STYPE) + OFF);              \
+    MemRandomize(src_memory_g, kPixels * sizeof(STYPE) + OFF);              \
+    MemRandomize(src_memory_b, kPixels * sizeof(STYPE) + OFF);              \
+    memset(dst_memory_c, 0, kPixels * 4 * sizeof(DTYPE));                   \
+    memset(dst_memory_opt, 0, kPixels * 4 * sizeof(DTYPE));                 \
+    STYPE* src_pixels_r = reinterpret_cast<STYPE*>(src_memory_r + OFF);     \
+    STYPE* src_pixels_g = reinterpret_cast<STYPE*>(src_memory_g + OFF);     \
+    STYPE* src_pixels_b = reinterpret_cast<STYPE*>(src_memory_b + OFF);     \
+    DTYPE* dst_pixels_c = reinterpret_cast<DTYPE*>(dst_memory_c);           \
+    DTYPE* dst_pixels_opt = reinterpret_cast<DTYPE*>(dst_memory_opt);       \
+    MaskCpuFlags(disable_cpu_flags_);                                       \
+    FUNC##Plane(src_pixels_r, kWidth, src_pixels_g, kWidth, src_pixels_b,   \
+                kWidth, NULL, 0, dst_pixels_c, kWidth * 4, kWidth,          \
+                NEG benchmark_height_, DEPTH);                              \
+    MaskCpuFlags(benchmark_cpu_info_);                                      \
+    for (int i = 0; i < benchmark_iterations_; ++i) {                       \
+      FUNC##Plane(src_pixels_r, kWidth, src_pixels_g, kWidth, src_pixels_b, \
+                  kWidth, NULL, 0, dst_pixels_opt, kWidth * 4, kWidth,      \
+                  NEG benchmark_height_, DEPTH);                            \
+    }                                                                       \
+    for (int i = 0; i < kPixels * 4; ++i) {                                 \
+      EXPECT_EQ(dst_pixels_c[i], dst_pixels_opt[i]);                        \
+    }                                                                       \
+    free_aligned_buffer_page_end(src_memory_r);                             \
+    free_aligned_buffer_page_end(src_memory_g);                             \
+    free_aligned_buffer_page_end(src_memory_b);                             \
+    free_aligned_buffer_page_end(dst_memory_c);                             \
+    free_aligned_buffer_page_end(dst_memory_opt);                           \
+  }
+
+#define TESTQPLANARTOP(FUNC, STYPE, DTYPE, DEPTH)                              \
+  TESTQPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_ + 1, _Any, +, 0) \
+  TESTQPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Unaligned, +,  \
+                  1)                                                           \
+  TESTQPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Invert, -, 0)  \
+  TESTQPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Opt, +, 0)     \
+  TESTQPLANAROTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_ + 1, _Any, +,   \
+                   0)                                                          \
+  TESTQPLANAROTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Unaligned, +, \
+                   1)                                                          \
+  TESTQPLANAROTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Invert, -, 0) \
+  TESTQPLANAROTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Opt, +, 0)
+
+TESTQPLANARTOP(MergeAR64, uint16_t, uint16_t, 10)
+TESTQPLANARTOP(MergeAR64, uint16_t, uint16_t, 12)
+TESTQPLANARTOP(MergeAR64, uint16_t, uint16_t, 16)
+TESTQPLANARTOP(MergeARGB16To8, uint16_t, uint8_t, 10)
+TESTQPLANARTOP(MergeARGB16To8, uint16_t, uint8_t, 12)
+TESTQPLANARTOP(MergeARGB16To8, uint16_t, uint8_t, 16)
+
+#define TESTTPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, W1280, N, NEG, OFF)      \
+  TEST_F(LibYUVPlanarTest, FUNC##Plane_##DEPTH##N) {                        \
+    const int kWidth = W1280;                                               \
+    const int kPixels = kWidth * benchmark_height_;                         \
+    align_buffer_page_end(src_memory_r, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(src_memory_g, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(src_memory_b, kPixels * sizeof(STYPE) + OFF);     \
+    align_buffer_page_end(dst_memory_c, kPixels * 4 * sizeof(DTYPE));       \
+    align_buffer_page_end(dst_memory_opt, kPixels * 4 * sizeof(DTYPE));     \
+    MemRandomize(src_memory_r, kPixels * sizeof(STYPE) + OFF);              \
+    MemRandomize(src_memory_g, kPixels * sizeof(STYPE) + OFF);              \
+    MemRandomize(src_memory_b, kPixels * sizeof(STYPE) + OFF);              \
+    STYPE* src_pixels_r = reinterpret_cast<STYPE*>(src_memory_r + OFF);     \
+    STYPE* src_pixels_g = reinterpret_cast<STYPE*>(src_memory_g + OFF);     \
+    STYPE* src_pixels_b = reinterpret_cast<STYPE*>(src_memory_b + OFF);     \
+    DTYPE* dst_pixels_c = reinterpret_cast<DTYPE*>(dst_memory_c);           \
+    DTYPE* dst_pixels_opt = reinterpret_cast<DTYPE*>(dst_memory_opt);       \
+    memset(dst_pixels_c, 1, kPixels * 4 * sizeof(DTYPE));                   \
+    memset(dst_pixels_opt, 2, kPixels * 4 * sizeof(DTYPE));                 \
+    MaskCpuFlags(disable_cpu_flags_);                                       \
+    FUNC##Plane(src_pixels_r, kWidth, src_pixels_g, kWidth, src_pixels_b,   \
+                kWidth, dst_pixels_c, kWidth * 4, kWidth,                   \
+                NEG benchmark_height_, DEPTH);                              \
+    MaskCpuFlags(benchmark_cpu_info_);                                      \
+    for (int i = 0; i < benchmark_iterations_; ++i) {                       \
+      FUNC##Plane(src_pixels_r, kWidth, src_pixels_g, kWidth, src_pixels_b, \
+                  kWidth, dst_pixels_opt, kWidth * 4, kWidth,               \
+                  NEG benchmark_height_, DEPTH);                            \
+    }                                                                       \
+    for (int i = 0; i < kPixels * 4; ++i) {                                 \
+      EXPECT_EQ(dst_pixels_c[i], dst_pixels_opt[i]);                        \
+    }                                                                       \
+    free_aligned_buffer_page_end(src_memory_r);                             \
+    free_aligned_buffer_page_end(src_memory_g);                             \
+    free_aligned_buffer_page_end(src_memory_b);                             \
+    free_aligned_buffer_page_end(dst_memory_c);                             \
+    free_aligned_buffer_page_end(dst_memory_opt);                           \
+  }
+
+#define TESTTPLANARTOP(FUNC, STYPE, DTYPE, DEPTH)                              \
+  TESTTPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_ + 1, _Any, +, 0) \
+  TESTTPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Unaligned, +,  \
+                  1)                                                           \
+  TESTTPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Invert, -, 0)  \
+  TESTTPLANARTOPI(FUNC, STYPE, DTYPE, DEPTH, benchmark_width_, _Opt, +, 0)
+
+TESTTPLANARTOP(MergeXR30, uint16_t, uint8_t, 10)
+TESTTPLANARTOP(MergeXR30, uint16_t, uint8_t, 12)
+TESTTPLANARTOP(MergeXR30, uint16_t, uint8_t, 16)
+
 // TODO(fbarchard): improve test for platforms and cpu detect
 #ifdef HAS_MERGEUVROW_16_AVX2
 TEST_F(LibYUVPlanarTest, MergeUVRow_16_Opt) {
   // Round count up to multiple of 16
   const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+
   align_buffer_page_end(src_pixels_u, kPixels * 2);
   align_buffer_page_end(src_pixels_v, kPixels * 2);
   align_buffer_page_end(dst_pixels_uv_opt, kPixels * 2 * 2);
@@ -3035,8 +3247,9 @@ TEST_F(LibYUVPlanarTest, MergeUVRow_16_Opt) {
 // TODO(fbarchard): Improve test for more platforms.
 #ifdef HAS_MULTIPLYROW_16_AVX2
 TEST_F(LibYUVPlanarTest, MultiplyRow_16_Opt) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  // Round count up to multiple of 32
+  const int kPixels = (benchmark_width_ * benchmark_height_ + 31) & ~31;
+
   align_buffer_page_end(src_pixels_y, kPixels * 2);
   align_buffer_page_end(dst_pixels_y_opt, kPixels * 2);
   align_buffer_page_end(dst_pixels_y_c, kPixels * 2);
@@ -3072,8 +3285,7 @@ TEST_F(LibYUVPlanarTest, MultiplyRow_16_Opt) {
 #endif  // HAS_MULTIPLYROW_16_AVX2
 
 TEST_F(LibYUVPlanarTest, Convert16To8Plane) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels_y, kPixels * 2);
   align_buffer_page_end(dst_pixels_y_opt, kPixels);
   align_buffer_page_end(dst_pixels_y_c, kPixels);
@@ -3152,8 +3364,7 @@ TEST_F(LibYUVPlanarTest, Convert16To8Row_Opt) {
 #endif  // ENABLE_ROW_TESTS
 
 TEST_F(LibYUVPlanarTest, Convert8To16Plane) {
-  // Round count up to multiple of 16
-  const int kPixels = (benchmark_width_ * benchmark_height_ + 15) & ~15;
+  const int kPixels = benchmark_width_ * benchmark_height_;
   align_buffer_page_end(src_pixels_y, kPixels);
   align_buffer_page_end(dst_pixels_y_opt, kPixels * 2);
   align_buffer_page_end(dst_pixels_y_c, kPixels * 2);
diff --git a/unit_test/rotate_argb_test.cc b/unit_test/rotate_argb_test.cc
index 3208b66a..01ed69ca 100644
--- a/unit_test/rotate_argb_test.cc
+++ b/unit_test/rotate_argb_test.cc
@@ -156,29 +156,29 @@ TEST_F(LibYUVRotateTest, RotatePlane270_Opt) {
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_RotatePlane0_Odd) {
-  TestRotatePlane(benchmark_width_ - 3, benchmark_height_ - 1,
-                  benchmark_width_ - 3, benchmark_height_ - 1, kRotate0,
+  TestRotatePlane(benchmark_width_ + 1, benchmark_height_ + 1,
+                  benchmark_width_ + 1, benchmark_height_ + 1, kRotate0,
                   benchmark_iterations_, disable_cpu_flags_,
                   benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_RotatePlane90_Odd) {
-  TestRotatePlane(benchmark_width_ - 3, benchmark_height_ - 1,
-                  benchmark_height_ - 1, benchmark_width_ - 3, kRotate90,
+  TestRotatePlane(benchmark_width_ + 1, benchmark_height_ + 1,
+                  benchmark_height_ + 1, benchmark_width_ + 1, kRotate90,
                   benchmark_iterations_, disable_cpu_flags_,
                   benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_RotatePlane180_Odd) {
-  TestRotatePlane(benchmark_width_ - 3, benchmark_height_ - 1,
-                  benchmark_width_ - 3, benchmark_height_ - 1, kRotate180,
+  TestRotatePlane(benchmark_width_ + 1, benchmark_height_ + 1,
+                  benchmark_width_ + 1, benchmark_height_ + 1, kRotate180,
                   benchmark_iterations_, disable_cpu_flags_,
                   benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_RotatePlane270_Odd) {
-  TestRotatePlane(benchmark_width_ - 3, benchmark_height_ - 1,
-                  benchmark_height_ - 1, benchmark_width_ - 3, kRotate270,
+  TestRotatePlane(benchmark_width_ + 1, benchmark_height_ + 1,
+                  benchmark_height_ + 1, benchmark_width_ + 1, kRotate270,
                   benchmark_iterations_, disable_cpu_flags_,
                   benchmark_cpu_info_);
 }
diff --git a/unit_test/rotate_test.cc b/unit_test/rotate_test.cc
index 61941e63..1bab584f 100644
--- a/unit_test/rotate_test.cc
+++ b/unit_test/rotate_test.cc
@@ -108,29 +108,29 @@ TEST_F(LibYUVRotateTest, I420Rotate270_Opt) {
 // Odd width tests work but disabled because they use C code and can be
 // tested by passing an odd width command line or environment variable.
 TEST_F(LibYUVRotateTest, DISABLED_I420Rotate0_Odd) {
-  I420TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_width_ - 3, benchmark_height_ - 1, kRotate0,
+  I420TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_width_ + 1, benchmark_height_ + 1, kRotate0,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_I420Rotate90_Odd) {
-  I420TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_height_ - 1, benchmark_width_ - 3, kRotate90,
+  I420TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_height_ + 1, benchmark_width_ + 1, kRotate90,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_I420Rotate180_Odd) {
-  I420TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_width_ - 3, benchmark_height_ - 1, kRotate180,
+  I420TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_width_ + 1, benchmark_height_ + 1, kRotate180,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_I420Rotate270_Odd) {
-  I420TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_height_ - 1, benchmark_width_ - 3, kRotate270,
+  I420TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_height_ + 1, benchmark_width_ + 1, kRotate270,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
@@ -225,29 +225,29 @@ TEST_F(LibYUVRotateTest, I444Rotate270_Opt) {
 // Odd width tests work but disabled because they use C code and can be
 // tested by passing an odd width command line or environment variable.
 TEST_F(LibYUVRotateTest, DISABLED_I444Rotate0_Odd) {
-  I444TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_width_ - 3, benchmark_height_ - 1, kRotate0,
+  I444TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_width_ + 1, benchmark_height_ + 1, kRotate0,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_I444Rotate90_Odd) {
-  I444TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_height_ - 1, benchmark_width_ - 3, kRotate90,
+  I444TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_height_ + 1, benchmark_width_ + 1, kRotate90,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_I444Rotate180_Odd) {
-  I444TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_width_ - 3, benchmark_height_ - 1, kRotate180,
+  I444TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_width_ + 1, benchmark_height_ + 1, kRotate180,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_I444Rotate270_Odd) {
-  I444TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_height_ - 1, benchmark_width_ - 3, kRotate270,
+  I444TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_height_ + 1, benchmark_width_ + 1, kRotate270,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
@@ -340,29 +340,29 @@ TEST_F(LibYUVRotateTest, NV12Rotate270_Opt) {
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_NV12Rotate0_Odd) {
-  NV12TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_width_ - 3, benchmark_height_ - 1, kRotate0,
+  NV12TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_width_ + 1, benchmark_height_ + 1, kRotate0,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_NV12Rotate90_Odd) {
-  NV12TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_height_ - 1, benchmark_width_ - 3, kRotate90,
+  NV12TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_height_ + 1, benchmark_width_ + 1, kRotate90,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_NV12Rotate180_Odd) {
-  NV12TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_width_ - 3, benchmark_height_ - 1, kRotate180,
+  NV12TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_width_ + 1, benchmark_height_ + 1, kRotate180,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
 
 TEST_F(LibYUVRotateTest, DISABLED_NV12Rotate270_Odd) {
-  NV12TestRotate(benchmark_width_ - 3, benchmark_height_ - 1,
-                 benchmark_height_ - 1, benchmark_width_ - 3, kRotate270,
+  NV12TestRotate(benchmark_width_ + 1, benchmark_height_ + 1,
+                 benchmark_height_ + 1, benchmark_width_ + 1, kRotate270,
                  benchmark_iterations_, disable_cpu_flags_,
                  benchmark_cpu_info_);
 }
diff --git a/unit_test/scale_argb_test.cc b/unit_test/scale_argb_test.cc
index f98fbaa2..48ad75ea 100644
--- a/unit_test/scale_argb_test.cc
+++ b/unit_test/scale_argb_test.cc
@@ -252,13 +252,13 @@ static int ARGBClipTestFilter(int src_width,
 // Test a scale factor with all 4 filters.  Expect unfiltered to be exact, but
 // filtering is different fixed point implementations for SSSE3, Neon and C.
 #ifdef ENABLE_SLOW_TESTS
-#define TEST_FACTOR(name, nom, denom)         \
+#define TEST_FACTOR(name, nom, denom)           \
   TEST_FACTOR1(, name, None, nom, denom, 0)     \
   TEST_FACTOR1(, name, Linear, nom, denom, 3)   \
   TEST_FACTOR1(, name, Bilinear, nom, denom, 3) \
   TEST_FACTOR1(, name, Box, nom, denom, 3)
 #else
-#define TEST_FACTOR(name, nom, denom)         \
+#define TEST_FACTOR(name, nom, denom)                    \
   TEST_FACTOR1(DISABLED_, name, None, nom, denom, 0)     \
   TEST_FACTOR1(DISABLED_, name, Linear, nom, denom, 3)   \
   TEST_FACTOR1(DISABLED_, name, Bilinear, nom, denom, 3) \
@@ -290,13 +290,15 @@ TEST_FACTOR(3, 1, 3)
                               benchmark_cpu_info_);                            \
     EXPECT_LE(diff, max_diff);                                                 \
   }                                                                            \
-  TEST_F(LibYUVScaleTest, DISABLED_##name##ClipTo##width##x##height##_##filter) { \
+  TEST_F(LibYUVScaleTest,                                                      \
+         DISABLED_##name##ClipTo##width##x##height##_##filter) {               \
     int diff =                                                                 \
         ARGBClipTestFilter(benchmark_width_, benchmark_height_, width, height, \
                            kFilter##filter, benchmark_iterations_);            \
     EXPECT_LE(diff, max_diff);                                                 \
   }                                                                            \
-  TEST_F(LibYUVScaleTest, DISABLED_##name##ClipFrom##width##x##height##_##filter) { \
+  TEST_F(LibYUVScaleTest,                                                      \
+         DISABLED_##name##ClipFrom##width##x##height##_##filter) {             \
     int diff = ARGBClipTestFilter(width, height, Abs(benchmark_width_),        \
                                   Abs(benchmark_height_), kFilter##filter,     \
                                   benchmark_iterations_);                      \
@@ -305,12 +307,12 @@ TEST_FACTOR(3, 1, 3)
 
 /// Test scale to a specified size with all 4 filters.
 #ifdef ENABLE_SLOW_TESTS
-#define TEST_SCALETO(name, width, height)       \
+#define TEST_SCALETO(name, width, height)         \
   TEST_SCALETO1(, name, width, height, None, 0)   \
   TEST_SCALETO1(, name, width, height, Linear, 3) \
   TEST_SCALETO1(, name, width, height, Bilinear, 3)
 #else
-#define TEST_SCALETO(name, width, height)       \
+#define TEST_SCALETO(name, width, height)                  \
   TEST_SCALETO1(DISABLED_, name, width, height, None, 0)   \
   TEST_SCALETO1(DISABLED_, name, width, height, Linear, 3) \
   TEST_SCALETO1(DISABLED_, name, width, height, Bilinear, 3)
diff --git a/unit_test/unit_test.h b/unit_test/unit_test.h
index 87907fa1..580832ad 100644
--- a/unit_test/unit_test.h
+++ b/unit_test/unit_test.h
@@ -11,7 +11,7 @@
 #ifndef UNIT_TEST_UNIT_TEST_H_  // NOLINT
 #define UNIT_TEST_UNIT_TEST_H_
 
-#ifdef WIN32
+#ifdef _WIN32
 #include <windows.h>
 #else
 #include <sys/resource.h>
diff --git a/unit_test/video_common_test.cc b/unit_test/video_common_test.cc
index 6c6a384d..36728ea9 100644
--- a/unit_test/video_common_test.cc
+++ b/unit_test/video_common_test.cc
@@ -29,7 +29,7 @@ static bool TestValidFourCC(uint32_t fourcc, int bpp) {
       !TestValidChar((fourcc >> 24) & 0xff)) {
     return false;
   }
-  if (bpp < 0 || bpp > 32) {
+  if (bpp < 0 || bpp > 64) {
     return false;
   }
   return true;
@@ -72,6 +72,8 @@ TEST_F(LibYUVBaseTest, TestFourCC) {
   EXPECT_TRUE(TestValidFourCC(FOURCC_ABGR, FOURCC_BPP_ABGR));
   EXPECT_TRUE(TestValidFourCC(FOURCC_AR30, FOURCC_BPP_AR30));
   EXPECT_TRUE(TestValidFourCC(FOURCC_AB30, FOURCC_BPP_AB30));
+  EXPECT_TRUE(TestValidFourCC(FOURCC_AR64, FOURCC_BPP_AR64));
+  EXPECT_TRUE(TestValidFourCC(FOURCC_AB64, FOURCC_BPP_AB64));
   EXPECT_TRUE(TestValidFourCC(FOURCC_24BG, FOURCC_BPP_24BG));
   EXPECT_TRUE(TestValidFourCC(FOURCC_RAW, FOURCC_BPP_RAW));
   EXPECT_TRUE(TestValidFourCC(FOURCC_RGBA, FOURCC_BPP_RGBA));
-- 
2.36.1

